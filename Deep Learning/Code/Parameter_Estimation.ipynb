{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    return ((t_p - t_c) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_u, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(t_p, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_W = \\\n",
    "    (loss_fn(model(t_u, W + delta, b), t_c)) - \\\n",
    "    (loss_fn(model(t_u, W - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W - learning_rate * loss_rate_of_change_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, W, b + delta), t_c)) - \\\n",
    "    (loss_fn(model(t_u, W, b - delta), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    return 2 * (t_p - t_c) / t_p.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        print(\"Epoch %d, Loss %f\" % (epoch, float(loss)))\n",
    "\n",
    "        print(\"Params:\", abs(params[0] / params[1]))\n",
    "        print(\"Grad:\", grad)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = t_u * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "Params: tensor(16.6931)\n",
      "Grad: tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "Params: tensor(16.0042)\n",
      "Grad: tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "Params: tensor(18.1579)\n",
      "Grad: tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "Params: tensor(22.5372)\n",
      "Grad: tensor([-5.2218,  2.1327])\n",
      "Epoch 5, Loss 29.507153\n",
      "Params: tensor(30.8660)\n",
      "Grad: tensor([-2.3715,  2.6310])\n",
      "Epoch 6, Loss 29.392456\n",
      "Params: tensor(50.1712)\n",
      "Grad: tensor([-1.2492,  2.8241])\n",
      "Epoch 7, Loss 29.298828\n",
      "Params: tensor(137.0145)\n",
      "Grad: tensor([-0.8071,  2.8970])\n",
      "Epoch 8, Loss 29.208717\n",
      "Params: tensor(186.5102)\n",
      "Grad: tensor([-0.6325,  2.9227])\n",
      "Epoch 9, Loss 29.119415\n",
      "Params: tensor(55.5908)\n",
      "Grad: tensor([-0.5633,  2.9298])\n",
      "Epoch 10, Loss 29.030489\n",
      "Params: tensor(32.7258)\n",
      "Grad: tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "Params: tensor(23.2254)\n",
      "Grad: tensor([-0.5240,  2.9264])\n",
      "Epoch 12, Loss 28.853565\n",
      "Params: tensor(18.0237)\n",
      "Grad: tensor([-0.5190,  2.9222])\n",
      "Epoch 13, Loss 28.765553\n",
      "Params: tensor(14.7418)\n",
      "Grad: tensor([-0.5165,  2.9175])\n",
      "Epoch 14, Loss 28.677851\n",
      "Params: tensor(12.4826)\n",
      "Grad: tensor([-0.5150,  2.9126])\n",
      "Epoch 15, Loss 28.590431\n",
      "Params: tensor(10.8325)\n",
      "Grad: tensor([-0.5138,  2.9077])\n",
      "Epoch 16, Loss 28.503319\n",
      "Params: tensor(9.5745)\n",
      "Grad: tensor([-0.5129,  2.9028])\n",
      "Epoch 17, Loss 28.416498\n",
      "Params: tensor(8.5837)\n",
      "Grad: tensor([-0.5120,  2.8979])\n",
      "Epoch 18, Loss 28.329973\n",
      "Params: tensor(7.7831)\n",
      "Grad: tensor([-0.5111,  2.8930])\n",
      "Epoch 19, Loss 28.243742\n",
      "Params: tensor(7.1228)\n",
      "Grad: tensor([-0.5102,  2.8881])\n",
      "Epoch 20, Loss 28.157804\n",
      "Params: tensor(6.5688)\n",
      "Grad: tensor([-0.5093,  2.8832])\n",
      "Epoch 21, Loss 28.072151\n",
      "Params: tensor(6.0974)\n",
      "Grad: tensor([-0.5084,  2.8783])\n",
      "Epoch 22, Loss 27.986797\n",
      "Params: tensor(5.6914)\n",
      "Grad: tensor([-0.5076,  2.8734])\n",
      "Epoch 23, Loss 27.901728\n",
      "Params: tensor(5.3380)\n",
      "Grad: tensor([-0.5067,  2.8685])\n",
      "Epoch 24, Loss 27.816950\n",
      "Params: tensor(5.0278)\n",
      "Grad: tensor([-0.5059,  2.8636])\n",
      "Epoch 25, Loss 27.732464\n",
      "Params: tensor(4.7531)\n",
      "Grad: tensor([-0.5050,  2.8588])\n",
      "Epoch 26, Loss 27.648256\n",
      "Params: tensor(4.5083)\n",
      "Grad: tensor([-0.5042,  2.8539])\n",
      "Epoch 27, Loss 27.564344\n",
      "Params: tensor(4.2887)\n",
      "Grad: tensor([-0.5033,  2.8490])\n",
      "Epoch 28, Loss 27.480707\n",
      "Params: tensor(4.0906)\n",
      "Grad: tensor([-0.5024,  2.8442])\n",
      "Epoch 29, Loss 27.397362\n",
      "Params: tensor(3.9110)\n",
      "Grad: tensor([-0.5016,  2.8394])\n",
      "Epoch 30, Loss 27.314295\n",
      "Params: tensor(3.7474)\n",
      "Grad: tensor([-0.5007,  2.8346])\n",
      "Epoch 31, Loss 27.231512\n",
      "Params: tensor(3.5978)\n",
      "Grad: tensor([-0.4999,  2.8297])\n",
      "Epoch 32, Loss 27.149010\n",
      "Params: tensor(3.4605)\n",
      "Grad: tensor([-0.4990,  2.8249])\n",
      "Epoch 33, Loss 27.066790\n",
      "Params: tensor(3.3339)\n",
      "Grad: tensor([-0.4982,  2.8201])\n",
      "Epoch 34, Loss 26.984844\n",
      "Params: tensor(3.2170)\n",
      "Grad: tensor([-0.4973,  2.8153])\n",
      "Epoch 35, Loss 26.903175\n",
      "Params: tensor(3.1086)\n",
      "Grad: tensor([-0.4965,  2.8106])\n",
      "Epoch 36, Loss 26.821791\n",
      "Params: tensor(3.0078)\n",
      "Grad: tensor([-0.4957,  2.8058])\n",
      "Epoch 37, Loss 26.740679\n",
      "Params: tensor(2.9138)\n",
      "Grad: tensor([-0.4948,  2.8010])\n",
      "Epoch 38, Loss 26.659838\n",
      "Params: tensor(2.8261)\n",
      "Grad: tensor([-0.4940,  2.7963])\n",
      "Epoch 39, Loss 26.579279\n",
      "Params: tensor(2.7439)\n",
      "Grad: tensor([-0.4931,  2.7915])\n",
      "Epoch 40, Loss 26.498987\n",
      "Params: tensor(2.6668)\n",
      "Grad: tensor([-0.4923,  2.7868])\n",
      "Epoch 41, Loss 26.418974\n",
      "Params: tensor(2.5943)\n",
      "Grad: tensor([-0.4915,  2.7820])\n",
      "Epoch 42, Loss 26.339228\n",
      "Params: tensor(2.5261)\n",
      "Grad: tensor([-0.4906,  2.7773])\n",
      "Epoch 43, Loss 26.259754\n",
      "Params: tensor(2.4617)\n",
      "Grad: tensor([-0.4898,  2.7726])\n",
      "Epoch 44, Loss 26.180548\n",
      "Params: tensor(2.4008)\n",
      "Grad: tensor([-0.4890,  2.7679])\n",
      "Epoch 45, Loss 26.101616\n",
      "Params: tensor(2.3432)\n",
      "Grad: tensor([-0.4881,  2.7632])\n",
      "Epoch 46, Loss 26.022947\n",
      "Params: tensor(2.2885)\n",
      "Grad: tensor([-0.4873,  2.7585])\n",
      "Epoch 47, Loss 25.944544\n",
      "Params: tensor(2.2367)\n",
      "Grad: tensor([-0.4865,  2.7538])\n",
      "Epoch 48, Loss 25.866417\n",
      "Params: tensor(2.1874)\n",
      "Grad: tensor([-0.4856,  2.7491])\n",
      "Epoch 49, Loss 25.788549\n",
      "Params: tensor(2.1405)\n",
      "Grad: tensor([-0.4848,  2.7444])\n",
      "Epoch 50, Loss 25.710938\n",
      "Params: tensor(2.0958)\n",
      "Grad: tensor([-0.4840,  2.7398])\n",
      "Epoch 51, Loss 25.633600\n",
      "Params: tensor(2.0532)\n",
      "Grad: tensor([-0.4832,  2.7351])\n",
      "Epoch 52, Loss 25.556524\n",
      "Params: tensor(2.0125)\n",
      "Grad: tensor([-0.4823,  2.7305])\n",
      "Epoch 53, Loss 25.479700\n",
      "Params: tensor(1.9736)\n",
      "Grad: tensor([-0.4815,  2.7258])\n",
      "Epoch 54, Loss 25.403149\n",
      "Params: tensor(1.9363)\n",
      "Grad: tensor([-0.4807,  2.7212])\n",
      "Epoch 55, Loss 25.326851\n",
      "Params: tensor(1.9007)\n",
      "Grad: tensor([-0.4799,  2.7166])\n",
      "Epoch 56, Loss 25.250811\n",
      "Params: tensor(1.8665)\n",
      "Grad: tensor([-0.4791,  2.7120])\n",
      "Epoch 57, Loss 25.175035\n",
      "Params: tensor(1.8337)\n",
      "Grad: tensor([-0.4783,  2.7074])\n",
      "Epoch 58, Loss 25.099512\n",
      "Params: tensor(1.8022)\n",
      "Grad: tensor([-0.4775,  2.7028])\n",
      "Epoch 59, Loss 25.024248\n",
      "Params: tensor(1.7719)\n",
      "Grad: tensor([-0.4766,  2.6982])\n",
      "Epoch 60, Loss 24.949236\n",
      "Params: tensor(1.7428)\n",
      "Grad: tensor([-0.4758,  2.6936])\n",
      "Epoch 61, Loss 24.874483\n",
      "Params: tensor(1.7147)\n",
      "Grad: tensor([-0.4750,  2.6890])\n",
      "Epoch 62, Loss 24.799976\n",
      "Params: tensor(1.6877)\n",
      "Grad: tensor([-0.4742,  2.6845])\n",
      "Epoch 63, Loss 24.725737\n",
      "Params: tensor(1.6617)\n",
      "Grad: tensor([-0.4734,  2.6799])\n",
      "Epoch 64, Loss 24.651739\n",
      "Params: tensor(1.6366)\n",
      "Grad: tensor([-0.4726,  2.6753])\n",
      "Epoch 65, Loss 24.577986\n",
      "Params: tensor(1.6124)\n",
      "Grad: tensor([-0.4718,  2.6708])\n",
      "Epoch 66, Loss 24.504494\n",
      "Params: tensor(1.5890)\n",
      "Grad: tensor([-0.4710,  2.6663])\n",
      "Epoch 67, Loss 24.431252\n",
      "Params: tensor(1.5664)\n",
      "Grad: tensor([-0.4702,  2.6617])\n",
      "Epoch 68, Loss 24.358257\n",
      "Params: tensor(1.5445)\n",
      "Grad: tensor([-0.4694,  2.6572])\n",
      "Epoch 69, Loss 24.285505\n",
      "Params: tensor(1.5234)\n",
      "Grad: tensor([-0.4686,  2.6527])\n",
      "Epoch 70, Loss 24.212999\n",
      "Params: tensor(1.5029)\n",
      "Grad: tensor([-0.4678,  2.6482])\n",
      "Epoch 71, Loss 24.140741\n",
      "Params: tensor(1.4831)\n",
      "Grad: tensor([-0.4670,  2.6437])\n",
      "Epoch 72, Loss 24.068733\n",
      "Params: tensor(1.4639)\n",
      "Grad: tensor([-0.4662,  2.6392])\n",
      "Epoch 73, Loss 23.996971\n",
      "Params: tensor(1.4453)\n",
      "Grad: tensor([-0.4654,  2.6347])\n",
      "Epoch 74, Loss 23.925446\n",
      "Params: tensor(1.4272)\n",
      "Grad: tensor([-0.4646,  2.6302])\n",
      "Epoch 75, Loss 23.854168\n",
      "Params: tensor(1.4097)\n",
      "Grad: tensor([-0.4638,  2.6258])\n",
      "Epoch 76, Loss 23.783125\n",
      "Params: tensor(1.3927)\n",
      "Grad: tensor([-0.4631,  2.6213])\n",
      "Epoch 77, Loss 23.712328\n",
      "Params: tensor(1.3762)\n",
      "Grad: tensor([-0.4623,  2.6169])\n",
      "Epoch 78, Loss 23.641773\n",
      "Params: tensor(1.3601)\n",
      "Grad: tensor([-0.4615,  2.6124])\n",
      "Epoch 79, Loss 23.571455\n",
      "Params: tensor(1.3445)\n",
      "Grad: tensor([-0.4607,  2.6080])\n",
      "Epoch 80, Loss 23.501379\n",
      "Params: tensor(1.3293)\n",
      "Grad: tensor([-0.4599,  2.6035])\n",
      "Epoch 81, Loss 23.431538\n",
      "Params: tensor(1.3146)\n",
      "Grad: tensor([-0.4591,  2.5991])\n",
      "Epoch 82, Loss 23.361937\n",
      "Params: tensor(1.3002)\n",
      "Grad: tensor([-0.4584,  2.5947])\n",
      "Epoch 83, Loss 23.292570\n",
      "Params: tensor(1.2863)\n",
      "Grad: tensor([-0.4576,  2.5903])\n",
      "Epoch 84, Loss 23.223436\n",
      "Params: tensor(1.2726)\n",
      "Grad: tensor([-0.4568,  2.5859])\n",
      "Epoch 85, Loss 23.154541\n",
      "Params: tensor(1.2594)\n",
      "Grad: tensor([-0.4560,  2.5815])\n",
      "Epoch 86, Loss 23.085882\n",
      "Params: tensor(1.2465)\n",
      "Grad: tensor([-0.4553,  2.5771])\n",
      "Epoch 87, Loss 23.017447\n",
      "Params: tensor(1.2339)\n",
      "Grad: tensor([-0.4545,  2.5727])\n",
      "Epoch 88, Loss 22.949251\n",
      "Params: tensor(1.2216)\n",
      "Grad: tensor([-0.4537,  2.5684])\n",
      "Epoch 89, Loss 22.881283\n",
      "Params: tensor(1.2096)\n",
      "Grad: tensor([-0.4529,  2.5640])\n",
      "Epoch 90, Loss 22.813549\n",
      "Params: tensor(1.1979)\n",
      "Grad: tensor([-0.4522,  2.5597])\n",
      "Epoch 91, Loss 22.746044\n",
      "Params: tensor(1.1865)\n",
      "Grad: tensor([-0.4514,  2.5553])\n",
      "Epoch 92, Loss 22.678766\n",
      "Params: tensor(1.1754)\n",
      "Grad: tensor([-0.4506,  2.5510])\n",
      "Epoch 93, Loss 22.611717\n",
      "Params: tensor(1.1645)\n",
      "Grad: tensor([-0.4499,  2.5466])\n",
      "Epoch 94, Loss 22.544899\n",
      "Params: tensor(1.1539)\n",
      "Grad: tensor([-0.4491,  2.5423])\n",
      "Epoch 95, Loss 22.478306\n",
      "Params: tensor(1.1435)\n",
      "Grad: tensor([-0.4483,  2.5380])\n",
      "Epoch 96, Loss 22.411934\n",
      "Params: tensor(1.1334)\n",
      "Grad: tensor([-0.4476,  2.5337])\n",
      "Epoch 97, Loss 22.345793\n",
      "Params: tensor(1.1235)\n",
      "Grad: tensor([-0.4468,  2.5294])\n",
      "Epoch 98, Loss 22.279875\n",
      "Params: tensor(1.1138)\n",
      "Grad: tensor([-0.4461,  2.5251])\n",
      "Epoch 99, Loss 22.214186\n",
      "Params: tensor(1.1043)\n",
      "Grad: tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "Params: tensor(1.0950)\n",
      "Grad: tensor([-0.4446,  2.5165])\n",
      "Epoch 101, Loss 22.083464\n",
      "Params: tensor(1.0859)\n",
      "Grad: tensor([-0.4438,  2.5122])\n",
      "Epoch 102, Loss 22.018436\n",
      "Params: tensor(1.0770)\n",
      "Grad: tensor([-0.4430,  2.5080])\n",
      "Epoch 103, Loss 21.953632\n",
      "Params: tensor(1.0683)\n",
      "Grad: tensor([-0.4423,  2.5037])\n",
      "Epoch 104, Loss 21.889046\n",
      "Params: tensor(1.0598)\n",
      "Grad: tensor([-0.4415,  2.4994])\n",
      "Epoch 105, Loss 21.824677\n",
      "Params: tensor(1.0515)\n",
      "Grad: tensor([-0.4408,  2.4952])\n",
      "Epoch 106, Loss 21.760529\n",
      "Params: tensor(1.0433)\n",
      "Grad: tensor([-0.4400,  2.4910])\n",
      "Epoch 107, Loss 21.696600\n",
      "Params: tensor(1.0353)\n",
      "Grad: tensor([-0.4393,  2.4867])\n",
      "Epoch 108, Loss 21.632883\n",
      "Params: tensor(1.0275)\n",
      "Grad: tensor([-0.4385,  2.4825])\n",
      "Epoch 109, Loss 21.569389\n",
      "Params: tensor(1.0198)\n",
      "Grad: tensor([-0.4378,  2.4783])\n",
      "Epoch 110, Loss 21.506102\n",
      "Params: tensor(1.0122)\n",
      "Grad: tensor([-0.4370,  2.4741])\n",
      "Epoch 111, Loss 21.443037\n",
      "Params: tensor(1.0048)\n",
      "Grad: tensor([-0.4363,  2.4699])\n",
      "Epoch 112, Loss 21.380186\n",
      "Params: tensor(0.9976)\n",
      "Grad: tensor([-0.4356,  2.4657])\n",
      "Epoch 113, Loss 21.317549\n",
      "Params: tensor(0.9905)\n",
      "Grad: tensor([-0.4348,  2.4615])\n",
      "Epoch 114, Loss 21.255117\n",
      "Params: tensor(0.9835)\n",
      "Grad: tensor([-0.4341,  2.4573])\n",
      "Epoch 115, Loss 21.192907\n",
      "Params: tensor(0.9766)\n",
      "Grad: tensor([-0.4334,  2.4531])\n",
      "Epoch 116, Loss 21.130898\n",
      "Params: tensor(0.9699)\n",
      "Grad: tensor([-0.4326,  2.4490])\n",
      "Epoch 117, Loss 21.069105\n",
      "Params: tensor(0.9633)\n",
      "Grad: tensor([-0.4319,  2.4448])\n",
      "Epoch 118, Loss 21.007526\n",
      "Params: tensor(0.9568)\n",
      "Grad: tensor([-0.4311,  2.4407])\n",
      "Epoch 119, Loss 20.946150\n",
      "Params: tensor(0.9504)\n",
      "Grad: tensor([-0.4304,  2.4365])\n",
      "Epoch 120, Loss 20.884981\n",
      "Params: tensor(0.9442)\n",
      "Grad: tensor([-0.4297,  2.4324])\n",
      "Epoch 121, Loss 20.824024\n",
      "Params: tensor(0.9381)\n",
      "Grad: tensor([-0.4290,  2.4282])\n",
      "Epoch 122, Loss 20.763273\n",
      "Params: tensor(0.9320)\n",
      "Grad: tensor([-0.4282,  2.4241])\n",
      "Epoch 123, Loss 20.702728\n",
      "Params: tensor(0.9261)\n",
      "Grad: tensor([-0.4275,  2.4200])\n",
      "Epoch 124, Loss 20.642384\n",
      "Params: tensor(0.9203)\n",
      "Grad: tensor([-0.4268,  2.4159])\n",
      "Epoch 125, Loss 20.582249\n",
      "Params: tensor(0.9145)\n",
      "Grad: tensor([-0.4260,  2.4118])\n",
      "Epoch 126, Loss 20.522322\n",
      "Params: tensor(0.9089)\n",
      "Grad: tensor([-0.4253,  2.4077])\n",
      "Epoch 127, Loss 20.462593\n",
      "Params: tensor(0.9034)\n",
      "Grad: tensor([-0.4246,  2.4036])\n",
      "Epoch 128, Loss 20.403069\n",
      "Params: tensor(0.8979)\n",
      "Grad: tensor([-0.4239,  2.3995])\n",
      "Epoch 129, Loss 20.343742\n",
      "Params: tensor(0.8926)\n",
      "Grad: tensor([-0.4232,  2.3954])\n",
      "Epoch 130, Loss 20.284624\n",
      "Params: tensor(0.8873)\n",
      "Grad: tensor([-0.4224,  2.3914])\n",
      "Epoch 131, Loss 20.225702\n",
      "Params: tensor(0.8821)\n",
      "Grad: tensor([-0.4217,  2.3873])\n",
      "Epoch 132, Loss 20.166981\n",
      "Params: tensor(0.8770)\n",
      "Grad: tensor([-0.4210,  2.3832])\n",
      "Epoch 133, Loss 20.108461\n",
      "Params: tensor(0.8720)\n",
      "Grad: tensor([-0.4203,  2.3792])\n",
      "Epoch 134, Loss 20.050137\n",
      "Params: tensor(0.8671)\n",
      "Grad: tensor([-0.4196,  2.3752])\n",
      "Epoch 135, Loss 19.992016\n",
      "Params: tensor(0.8622)\n",
      "Grad: tensor([-0.4189,  2.3711])\n",
      "Epoch 136, Loss 19.934086\n",
      "Params: tensor(0.8574)\n",
      "Grad: tensor([-0.4182,  2.3671])\n",
      "Epoch 137, Loss 19.876352\n",
      "Params: tensor(0.8527)\n",
      "Grad: tensor([-0.4174,  2.3631])\n",
      "Epoch 138, Loss 19.818823\n",
      "Params: tensor(0.8481)\n",
      "Grad: tensor([-0.4167,  2.3591])\n",
      "Epoch 139, Loss 19.761480\n",
      "Params: tensor(0.8435)\n",
      "Grad: tensor([-0.4160,  2.3550])\n",
      "Epoch 140, Loss 19.704336\n",
      "Params: tensor(0.8390)\n",
      "Grad: tensor([-0.4153,  2.3510])\n",
      "Epoch 141, Loss 19.647385\n",
      "Params: tensor(0.8346)\n",
      "Grad: tensor([-0.4146,  2.3471])\n",
      "Epoch 142, Loss 19.590626\n",
      "Params: tensor(0.8303)\n",
      "Grad: tensor([-0.4139,  2.3431])\n",
      "Epoch 143, Loss 19.534061\n",
      "Params: tensor(0.8260)\n",
      "Grad: tensor([-0.4132,  2.3391])\n",
      "Epoch 144, Loss 19.477690\n",
      "Params: tensor(0.8217)\n",
      "Grad: tensor([-0.4125,  2.3351])\n",
      "Epoch 145, Loss 19.421507\n",
      "Params: tensor(0.8176)\n",
      "Grad: tensor([-0.4118,  2.3311])\n",
      "Epoch 146, Loss 19.365515\n",
      "Params: tensor(0.8135)\n",
      "Grad: tensor([-0.4111,  2.3272])\n",
      "Epoch 147, Loss 19.309715\n",
      "Params: tensor(0.8094)\n",
      "Grad: tensor([-0.4104,  2.3232])\n",
      "Epoch 148, Loss 19.254107\n",
      "Params: tensor(0.8054)\n",
      "Grad: tensor([-0.4097,  2.3193])\n",
      "Epoch 149, Loss 19.198685\n",
      "Params: tensor(0.8015)\n",
      "Grad: tensor([-0.4090,  2.3153])\n",
      "Epoch 150, Loss 19.143446\n",
      "Params: tensor(0.7976)\n",
      "Grad: tensor([-0.4083,  2.3114])\n",
      "Epoch 151, Loss 19.088402\n",
      "Params: tensor(0.7938)\n",
      "Grad: tensor([-0.4076,  2.3075])\n",
      "Epoch 152, Loss 19.033543\n",
      "Params: tensor(0.7900)\n",
      "Grad: tensor([-0.4069,  2.3036])\n",
      "Epoch 153, Loss 18.978868\n",
      "Params: tensor(0.7863)\n",
      "Grad: tensor([-0.4062,  2.2997])\n",
      "Epoch 154, Loss 18.924377\n",
      "Params: tensor(0.7826)\n",
      "Grad: tensor([-0.4056,  2.2957])\n",
      "Epoch 155, Loss 18.870081\n",
      "Params: tensor(0.7790)\n",
      "Grad: tensor([-0.4049,  2.2918])\n",
      "Epoch 156, Loss 18.815960\n",
      "Params: tensor(0.7754)\n",
      "Grad: tensor([-0.4042,  2.2880])\n",
      "Epoch 157, Loss 18.762022\n",
      "Params: tensor(0.7719)\n",
      "Grad: tensor([-0.4035,  2.2841])\n",
      "Epoch 158, Loss 18.708271\n",
      "Params: tensor(0.7684)\n",
      "Grad: tensor([-0.4028,  2.2802])\n",
      "Epoch 159, Loss 18.654699\n",
      "Params: tensor(0.7650)\n",
      "Grad: tensor([-0.4021,  2.2763])\n",
      "Epoch 160, Loss 18.601313\n",
      "Params: tensor(0.7616)\n",
      "Grad: tensor([-0.4014,  2.2724])\n",
      "Epoch 161, Loss 18.548109\n",
      "Params: tensor(0.7583)\n",
      "Grad: tensor([-0.4007,  2.2686])\n",
      "Epoch 162, Loss 18.495085\n",
      "Params: tensor(0.7550)\n",
      "Grad: tensor([-0.4001,  2.2647])\n",
      "Epoch 163, Loss 18.442236\n",
      "Params: tensor(0.7517)\n",
      "Grad: tensor([-0.3994,  2.2609])\n",
      "Epoch 164, Loss 18.389570\n",
      "Params: tensor(0.7485)\n",
      "Grad: tensor([-0.3987,  2.2570])\n",
      "Epoch 165, Loss 18.337080\n",
      "Params: tensor(0.7453)\n",
      "Grad: tensor([-0.3980,  2.2532])\n",
      "Epoch 166, Loss 18.284777\n",
      "Params: tensor(0.7422)\n",
      "Grad: tensor([-0.3974,  2.2494])\n",
      "Epoch 167, Loss 18.232641\n",
      "Params: tensor(0.7391)\n",
      "Grad: tensor([-0.3967,  2.2456])\n",
      "Epoch 168, Loss 18.180685\n",
      "Params: tensor(0.7360)\n",
      "Grad: tensor([-0.3960,  2.2417])\n",
      "Epoch 169, Loss 18.128906\n",
      "Params: tensor(0.7330)\n",
      "Grad: tensor([-0.3953,  2.2379])\n",
      "Epoch 170, Loss 18.077301\n",
      "Params: tensor(0.7301)\n",
      "Grad: tensor([-0.3947,  2.2341])\n",
      "Epoch 171, Loss 18.025877\n",
      "Params: tensor(0.7271)\n",
      "Grad: tensor([-0.3940,  2.2303])\n",
      "Epoch 172, Loss 17.974623\n",
      "Params: tensor(0.7242)\n",
      "Grad: tensor([-0.3933,  2.2266])\n",
      "Epoch 173, Loss 17.923546\n",
      "Params: tensor(0.7213)\n",
      "Grad: tensor([-0.3927,  2.2228])\n",
      "Epoch 174, Loss 17.872643\n",
      "Params: tensor(0.7185)\n",
      "Grad: tensor([-0.3920,  2.2190])\n",
      "Epoch 175, Loss 17.821909\n",
      "Params: tensor(0.7157)\n",
      "Grad: tensor([-0.3913,  2.2152])\n",
      "Epoch 176, Loss 17.771345\n",
      "Params: tensor(0.7129)\n",
      "Grad: tensor([-0.3907,  2.2115])\n",
      "Epoch 177, Loss 17.720955\n",
      "Params: tensor(0.7102)\n",
      "Grad: tensor([-0.3900,  2.2077])\n",
      "Epoch 178, Loss 17.670738\n",
      "Params: tensor(0.7075)\n",
      "Grad: tensor([-0.3893,  2.2040])\n",
      "Epoch 179, Loss 17.620689\n",
      "Params: tensor(0.7048)\n",
      "Grad: tensor([-0.3887,  2.2002])\n",
      "Epoch 180, Loss 17.570814\n",
      "Params: tensor(0.7022)\n",
      "Grad: tensor([-0.3880,  2.1965])\n",
      "Epoch 181, Loss 17.521103\n",
      "Params: tensor(0.6996)\n",
      "Grad: tensor([-0.3873,  2.1927])\n",
      "Epoch 182, Loss 17.471565\n",
      "Params: tensor(0.6970)\n",
      "Grad: tensor([-0.3867,  2.1890])\n",
      "Epoch 183, Loss 17.422192\n",
      "Params: tensor(0.6945)\n",
      "Grad: tensor([-0.3860,  2.1853])\n",
      "Epoch 184, Loss 17.372993\n",
      "Params: tensor(0.6919)\n",
      "Grad: tensor([-0.3854,  2.1816])\n",
      "Epoch 185, Loss 17.323954\n",
      "Params: tensor(0.6895)\n",
      "Grad: tensor([-0.3847,  2.1779])\n",
      "Epoch 186, Loss 17.275084\n",
      "Params: tensor(0.6870)\n",
      "Grad: tensor([-0.3841,  2.1742])\n",
      "Epoch 187, Loss 17.226379\n",
      "Params: tensor(0.6846)\n",
      "Grad: tensor([-0.3834,  2.1705])\n",
      "Epoch 188, Loss 17.177839\n",
      "Params: tensor(0.6822)\n",
      "Grad: tensor([-0.3828,  2.1668])\n",
      "Epoch 189, Loss 17.129463\n",
      "Params: tensor(0.6798)\n",
      "Grad: tensor([-0.3821,  2.1631])\n",
      "Epoch 190, Loss 17.081255\n",
      "Params: tensor(0.6774)\n",
      "Grad: tensor([-0.3815,  2.1594])\n",
      "Epoch 191, Loss 17.033209\n",
      "Params: tensor(0.6751)\n",
      "Grad: tensor([-0.3808,  2.1558])\n",
      "Epoch 192, Loss 16.985327\n",
      "Params: tensor(0.6728)\n",
      "Grad: tensor([-0.3802,  2.1521])\n",
      "Epoch 193, Loss 16.937605\n",
      "Params: tensor(0.6705)\n",
      "Grad: tensor([-0.3795,  2.1485])\n",
      "Epoch 194, Loss 16.890047\n",
      "Params: tensor(0.6683)\n",
      "Grad: tensor([-0.3789,  2.1448])\n",
      "Epoch 195, Loss 16.842649\n",
      "Params: tensor(0.6660)\n",
      "Grad: tensor([-0.3782,  2.1412])\n",
      "Epoch 196, Loss 16.795412\n",
      "Params: tensor(0.6638)\n",
      "Grad: tensor([-0.3776,  2.1375])\n",
      "Epoch 197, Loss 16.748339\n",
      "Params: tensor(0.6616)\n",
      "Grad: tensor([-0.3770,  2.1339])\n",
      "Epoch 198, Loss 16.701422\n",
      "Params: tensor(0.6595)\n",
      "Grad: tensor([-0.3763,  2.1303])\n",
      "Epoch 199, Loss 16.654661\n",
      "Params: tensor(0.6573)\n",
      "Grad: tensor([-0.3757,  2.1267])\n",
      "Epoch 200, Loss 16.608067\n",
      "Params: tensor(0.6552)\n",
      "Grad: tensor([-0.3750,  2.1230])\n",
      "Epoch 201, Loss 16.561623\n",
      "Params: tensor(0.6531)\n",
      "Grad: tensor([-0.3744,  2.1194])\n",
      "Epoch 202, Loss 16.515343\n",
      "Params: tensor(0.6511)\n",
      "Grad: tensor([-0.3738,  2.1158])\n",
      "Epoch 203, Loss 16.469219\n",
      "Params: tensor(0.6490)\n",
      "Grad: tensor([-0.3731,  2.1122])\n",
      "Epoch 204, Loss 16.423248\n",
      "Params: tensor(0.6470)\n",
      "Grad: tensor([-0.3725,  2.1087])\n",
      "Epoch 205, Loss 16.377434\n",
      "Params: tensor(0.6450)\n",
      "Grad: tensor([-0.3719,  2.1051])\n",
      "Epoch 206, Loss 16.331776\n",
      "Params: tensor(0.6430)\n",
      "Grad: tensor([-0.3712,  2.1015])\n",
      "Epoch 207, Loss 16.286276\n",
      "Params: tensor(0.6410)\n",
      "Grad: tensor([-0.3706,  2.0979])\n",
      "Epoch 208, Loss 16.240929\n",
      "Params: tensor(0.6391)\n",
      "Grad: tensor([-0.3700,  2.0944])\n",
      "Epoch 209, Loss 16.195732\n",
      "Params: tensor(0.6372)\n",
      "Grad: tensor([-0.3694,  2.0908])\n",
      "Epoch 210, Loss 16.150694\n",
      "Params: tensor(0.6353)\n",
      "Grad: tensor([-0.3687,  2.0873])\n",
      "Epoch 211, Loss 16.105806\n",
      "Params: tensor(0.6334)\n",
      "Grad: tensor([-0.3681,  2.0837])\n",
      "Epoch 212, Loss 16.061073\n",
      "Params: tensor(0.6315)\n",
      "Grad: tensor([-0.3675,  2.0802])\n",
      "Epoch 213, Loss 16.016487\n",
      "Params: tensor(0.6296)\n",
      "Grad: tensor([-0.3668,  2.0766])\n",
      "Epoch 214, Loss 15.972058\n",
      "Params: tensor(0.6278)\n",
      "Grad: tensor([-0.3662,  2.0731])\n",
      "Epoch 215, Loss 15.927776\n",
      "Params: tensor(0.6260)\n",
      "Grad: tensor([-0.3656,  2.0696])\n",
      "Epoch 216, Loss 15.883645\n",
      "Params: tensor(0.6242)\n",
      "Grad: tensor([-0.3650,  2.0661])\n",
      "Epoch 217, Loss 15.839664\n",
      "Params: tensor(0.6224)\n",
      "Grad: tensor([-0.3644,  2.0626])\n",
      "Epoch 218, Loss 15.795832\n",
      "Params: tensor(0.6207)\n",
      "Grad: tensor([-0.3637,  2.0591])\n",
      "Epoch 219, Loss 15.752152\n",
      "Params: tensor(0.6189)\n",
      "Grad: tensor([-0.3631,  2.0556])\n",
      "Epoch 220, Loss 15.708612\n",
      "Params: tensor(0.6172)\n",
      "Grad: tensor([-0.3625,  2.0521])\n",
      "Epoch 221, Loss 15.665226\n",
      "Params: tensor(0.6155)\n",
      "Grad: tensor([-0.3619,  2.0486])\n",
      "Epoch 222, Loss 15.621990\n",
      "Params: tensor(0.6138)\n",
      "Grad: tensor([-0.3613,  2.0451])\n",
      "Epoch 223, Loss 15.578897\n",
      "Params: tensor(0.6121)\n",
      "Grad: tensor([-0.3607,  2.0416])\n",
      "Epoch 224, Loss 15.535950\n",
      "Params: tensor(0.6104)\n",
      "Grad: tensor([-0.3601,  2.0382])\n",
      "Epoch 225, Loss 15.493150\n",
      "Params: tensor(0.6088)\n",
      "Grad: tensor([-0.3594,  2.0347])\n",
      "Epoch 226, Loss 15.450495\n",
      "Params: tensor(0.6071)\n",
      "Grad: tensor([-0.3588,  2.0312])\n",
      "Epoch 227, Loss 15.407981\n",
      "Params: tensor(0.6055)\n",
      "Grad: tensor([-0.3582,  2.0278])\n",
      "Epoch 228, Loss 15.365616\n",
      "Params: tensor(0.6039)\n",
      "Grad: tensor([-0.3576,  2.0243])\n",
      "Epoch 229, Loss 15.323396\n",
      "Params: tensor(0.6023)\n",
      "Grad: tensor([-0.3570,  2.0209])\n",
      "Epoch 230, Loss 15.281317\n",
      "Params: tensor(0.6007)\n",
      "Grad: tensor([-0.3564,  2.0175])\n",
      "Epoch 231, Loss 15.239380\n",
      "Params: tensor(0.5992)\n",
      "Grad: tensor([-0.3558,  2.0140])\n",
      "Epoch 232, Loss 15.197585\n",
      "Params: tensor(0.5976)\n",
      "Grad: tensor([-0.3552,  2.0106])\n",
      "Epoch 233, Loss 15.155932\n",
      "Params: tensor(0.5961)\n",
      "Grad: tensor([-0.3546,  2.0072])\n",
      "Epoch 234, Loss 15.114425\n",
      "Params: tensor(0.5946)\n",
      "Grad: tensor([-0.3540,  2.0038])\n",
      "Epoch 235, Loss 15.073055\n",
      "Params: tensor(0.5931)\n",
      "Grad: tensor([-0.3534,  2.0004])\n",
      "Epoch 236, Loss 15.031823\n",
      "Params: tensor(0.5916)\n",
      "Grad: tensor([-0.3528,  1.9970])\n",
      "Epoch 237, Loss 14.990734\n",
      "Params: tensor(0.5901)\n",
      "Grad: tensor([-0.3522,  1.9936])\n",
      "Epoch 238, Loss 14.949784\n",
      "Params: tensor(0.5886)\n",
      "Grad: tensor([-0.3516,  1.9902])\n",
      "Epoch 239, Loss 14.908973\n",
      "Params: tensor(0.5872)\n",
      "Grad: tensor([-0.3510,  1.9868])\n",
      "Epoch 240, Loss 14.868304\n",
      "Params: tensor(0.5857)\n",
      "Grad: tensor([-0.3504,  1.9835])\n",
      "Epoch 241, Loss 14.827767\n",
      "Params: tensor(0.5843)\n",
      "Grad: tensor([-0.3498,  1.9801])\n",
      "Epoch 242, Loss 14.787370\n",
      "Params: tensor(0.5829)\n",
      "Grad: tensor([-0.3492,  1.9767])\n",
      "Epoch 243, Loss 14.747109\n",
      "Params: tensor(0.5815)\n",
      "Grad: tensor([-0.3486,  1.9734])\n",
      "Epoch 244, Loss 14.706989\n",
      "Params: tensor(0.5801)\n",
      "Grad: tensor([-0.3480,  1.9700])\n",
      "Epoch 245, Loss 14.667002\n",
      "Params: tensor(0.5787)\n",
      "Grad: tensor([-0.3474,  1.9667])\n",
      "Epoch 246, Loss 14.627151\n",
      "Params: tensor(0.5773)\n",
      "Grad: tensor([-0.3468,  1.9633])\n",
      "Epoch 247, Loss 14.587436\n",
      "Params: tensor(0.5760)\n",
      "Grad: tensor([-0.3462,  1.9600])\n",
      "Epoch 248, Loss 14.547855\n",
      "Params: tensor(0.5746)\n",
      "Grad: tensor([-0.3456,  1.9567])\n",
      "Epoch 249, Loss 14.508409\n",
      "Params: tensor(0.5733)\n",
      "Grad: tensor([-0.3451,  1.9533])\n",
      "Epoch 250, Loss 14.469097\n",
      "Params: tensor(0.5720)\n",
      "Grad: tensor([-0.3445,  1.9500])\n",
      "Epoch 251, Loss 14.429920\n",
      "Params: tensor(0.5707)\n",
      "Grad: tensor([-0.3439,  1.9467])\n",
      "Epoch 252, Loss 14.390870\n",
      "Params: tensor(0.5694)\n",
      "Grad: tensor([-0.3433,  1.9434])\n",
      "Epoch 253, Loss 14.351956\n",
      "Params: tensor(0.5681)\n",
      "Grad: tensor([-0.3427,  1.9401])\n",
      "Epoch 254, Loss 14.313177\n",
      "Params: tensor(0.5668)\n",
      "Grad: tensor([-0.3421,  1.9368])\n",
      "Epoch 255, Loss 14.274529\n",
      "Params: tensor(0.5655)\n",
      "Grad: tensor([-0.3416,  1.9335])\n",
      "Epoch 256, Loss 14.236009\n",
      "Params: tensor(0.5643)\n",
      "Grad: tensor([-0.3410,  1.9302])\n",
      "Epoch 257, Loss 14.197620\n",
      "Params: tensor(0.5630)\n",
      "Grad: tensor([-0.3404,  1.9269])\n",
      "Epoch 258, Loss 14.159363\n",
      "Params: tensor(0.5618)\n",
      "Grad: tensor([-0.3398,  1.9237])\n",
      "Epoch 259, Loss 14.121234\n",
      "Params: tensor(0.5606)\n",
      "Grad: tensor([-0.3392,  1.9204])\n",
      "Epoch 260, Loss 14.083236\n",
      "Params: tensor(0.5593)\n",
      "Grad: tensor([-0.3387,  1.9171])\n",
      "Epoch 261, Loss 14.045367\n",
      "Params: tensor(0.5581)\n",
      "Grad: tensor([-0.3381,  1.9139])\n",
      "Epoch 262, Loss 14.007627\n",
      "Params: tensor(0.5569)\n",
      "Grad: tensor([-0.3375,  1.9106])\n",
      "Epoch 263, Loss 13.970016\n",
      "Params: tensor(0.5557)\n",
      "Grad: tensor([-0.3369,  1.9074])\n",
      "Epoch 264, Loss 13.932531\n",
      "Params: tensor(0.5546)\n",
      "Grad: tensor([-0.3364,  1.9041])\n",
      "Epoch 265, Loss 13.895172\n",
      "Params: tensor(0.5534)\n",
      "Grad: tensor([-0.3358,  1.9009])\n",
      "Epoch 266, Loss 13.857944\n",
      "Params: tensor(0.5522)\n",
      "Grad: tensor([-0.3352,  1.8977])\n",
      "Epoch 267, Loss 13.820837\n",
      "Params: tensor(0.5511)\n",
      "Grad: tensor([-0.3347,  1.8945])\n",
      "Epoch 268, Loss 13.783858\n",
      "Params: tensor(0.5499)\n",
      "Grad: tensor([-0.3341,  1.8912])\n",
      "Epoch 269, Loss 13.747006\n",
      "Params: tensor(0.5488)\n",
      "Grad: tensor([-0.3335,  1.8880])\n",
      "Epoch 270, Loss 13.710278\n",
      "Params: tensor(0.5477)\n",
      "Grad: tensor([-0.3330,  1.8848])\n",
      "Epoch 271, Loss 13.673676\n",
      "Params: tensor(0.5465)\n",
      "Grad: tensor([-0.3324,  1.8816])\n",
      "Epoch 272, Loss 13.637196\n",
      "Params: tensor(0.5454)\n",
      "Grad: tensor([-0.3318,  1.8784])\n",
      "Epoch 273, Loss 13.600842\n",
      "Params: tensor(0.5443)\n",
      "Grad: tensor([-0.3313,  1.8752])\n",
      "Epoch 274, Loss 13.564609\n",
      "Params: tensor(0.5432)\n",
      "Grad: tensor([-0.3307,  1.8720])\n",
      "Epoch 275, Loss 13.528501\n",
      "Params: tensor(0.5422)\n",
      "Grad: tensor([-0.3301,  1.8689])\n",
      "Epoch 276, Loss 13.492514\n",
      "Params: tensor(0.5411)\n",
      "Grad: tensor([-0.3296,  1.8657])\n",
      "Epoch 277, Loss 13.456651\n",
      "Params: tensor(0.5400)\n",
      "Grad: tensor([-0.3290,  1.8625])\n",
      "Epoch 278, Loss 13.420910\n",
      "Params: tensor(0.5390)\n",
      "Grad: tensor([-0.3285,  1.8594])\n",
      "Epoch 279, Loss 13.385287\n",
      "Params: tensor(0.5379)\n",
      "Grad: tensor([-0.3279,  1.8562])\n",
      "Epoch 280, Loss 13.349789\n",
      "Params: tensor(0.5369)\n",
      "Grad: tensor([-0.3274,  1.8530])\n",
      "Epoch 281, Loss 13.314407\n",
      "Params: tensor(0.5358)\n",
      "Grad: tensor([-0.3268,  1.8499])\n",
      "Epoch 282, Loss 13.279150\n",
      "Params: tensor(0.5348)\n",
      "Grad: tensor([-0.3262,  1.8468])\n",
      "Epoch 283, Loss 13.244009\n",
      "Params: tensor(0.5338)\n",
      "Grad: tensor([-0.3257,  1.8436])\n",
      "Epoch 284, Loss 13.208991\n",
      "Params: tensor(0.5328)\n",
      "Grad: tensor([-0.3251,  1.8405])\n",
      "Epoch 285, Loss 13.174088\n",
      "Params: tensor(0.5318)\n",
      "Grad: tensor([-0.3246,  1.8374])\n",
      "Epoch 286, Loss 13.139307\n",
      "Params: tensor(0.5308)\n",
      "Grad: tensor([-0.3240,  1.8342])\n",
      "Epoch 287, Loss 13.104639\n",
      "Params: tensor(0.5298)\n",
      "Grad: tensor([-0.3235,  1.8311])\n",
      "Epoch 288, Loss 13.070092\n",
      "Params: tensor(0.5288)\n",
      "Grad: tensor([-0.3229,  1.8280])\n",
      "Epoch 289, Loss 13.035664\n",
      "Params: tensor(0.5278)\n",
      "Grad: tensor([-0.3224,  1.8249])\n",
      "Epoch 290, Loss 13.001349\n",
      "Params: tensor(0.5268)\n",
      "Grad: tensor([-0.3218,  1.8218])\n",
      "Epoch 291, Loss 12.967152\n",
      "Params: tensor(0.5259)\n",
      "Grad: tensor([-0.3213,  1.8187])\n",
      "Epoch 292, Loss 12.933075\n",
      "Params: tensor(0.5249)\n",
      "Grad: tensor([-0.3207,  1.8156])\n",
      "Epoch 293, Loss 12.899109\n",
      "Params: tensor(0.5240)\n",
      "Grad: tensor([-0.3202,  1.8125])\n",
      "Epoch 294, Loss 12.865259\n",
      "Params: tensor(0.5230)\n",
      "Grad: tensor([-0.3196,  1.8095])\n",
      "Epoch 295, Loss 12.831525\n",
      "Params: tensor(0.5221)\n",
      "Grad: tensor([-0.3191,  1.8064])\n",
      "Epoch 296, Loss 12.797904\n",
      "Params: tensor(0.5212)\n",
      "Grad: tensor([-0.3186,  1.8033])\n",
      "Epoch 297, Loss 12.764399\n",
      "Params: tensor(0.5202)\n",
      "Grad: tensor([-0.3180,  1.8003])\n",
      "Epoch 298, Loss 12.731007\n",
      "Params: tensor(0.5193)\n",
      "Grad: tensor([-0.3175,  1.7972])\n",
      "Epoch 299, Loss 12.697727\n",
      "Params: tensor(0.5184)\n",
      "Grad: tensor([-0.3169,  1.7941])\n",
      "Epoch 300, Loss 12.664559\n",
      "Params: tensor(0.5175)\n",
      "Grad: tensor([-0.3164,  1.7911])\n",
      "Epoch 301, Loss 12.631507\n",
      "Params: tensor(0.5166)\n",
      "Grad: tensor([-0.3159,  1.7881])\n",
      "Epoch 302, Loss 12.598568\n",
      "Params: tensor(0.5157)\n",
      "Grad: tensor([-0.3153,  1.7850])\n",
      "Epoch 303, Loss 12.565738\n",
      "Params: tensor(0.5148)\n",
      "Grad: tensor([-0.3148,  1.7820])\n",
      "Epoch 304, Loss 12.533021\n",
      "Params: tensor(0.5140)\n",
      "Grad: tensor([-0.3143,  1.7790])\n",
      "Epoch 305, Loss 12.500413\n",
      "Params: tensor(0.5131)\n",
      "Grad: tensor([-0.3137,  1.7759])\n",
      "Epoch 306, Loss 12.467919\n",
      "Params: tensor(0.5122)\n",
      "Grad: tensor([-0.3132,  1.7729])\n",
      "Epoch 307, Loss 12.435532\n",
      "Params: tensor(0.5114)\n",
      "Grad: tensor([-0.3127,  1.7699])\n",
      "Epoch 308, Loss 12.403256\n",
      "Params: tensor(0.5105)\n",
      "Grad: tensor([-0.3121,  1.7669])\n",
      "Epoch 309, Loss 12.371090\n",
      "Params: tensor(0.5097)\n",
      "Grad: tensor([-0.3116,  1.7639])\n",
      "Epoch 310, Loss 12.339031\n",
      "Params: tensor(0.5088)\n",
      "Grad: tensor([-0.3111,  1.7609])\n",
      "Epoch 311, Loss 12.307082\n",
      "Params: tensor(0.5080)\n",
      "Grad: tensor([-0.3105,  1.7579])\n",
      "Epoch 312, Loss 12.275247\n",
      "Params: tensor(0.5072)\n",
      "Grad: tensor([-0.3100,  1.7549])\n",
      "Epoch 313, Loss 12.243509\n",
      "Params: tensor(0.5063)\n",
      "Grad: tensor([-0.3095,  1.7519])\n",
      "Epoch 314, Loss 12.211887\n",
      "Params: tensor(0.5055)\n",
      "Grad: tensor([-0.3090,  1.7490])\n",
      "Epoch 315, Loss 12.180370\n",
      "Params: tensor(0.5047)\n",
      "Grad: tensor([-0.3084,  1.7460])\n",
      "Epoch 316, Loss 12.148962\n",
      "Params: tensor(0.5039)\n",
      "Grad: tensor([-0.3079,  1.7430])\n",
      "Epoch 317, Loss 12.117657\n",
      "Params: tensor(0.5031)\n",
      "Grad: tensor([-0.3074,  1.7401])\n",
      "Epoch 318, Loss 12.086462\n",
      "Params: tensor(0.5023)\n",
      "Grad: tensor([-0.3069,  1.7371])\n",
      "Epoch 319, Loss 12.055373\n",
      "Params: tensor(0.5015)\n",
      "Grad: tensor([-0.3063,  1.7342])\n",
      "Epoch 320, Loss 12.024384\n",
      "Params: tensor(0.5007)\n",
      "Grad: tensor([-0.3058,  1.7312])\n",
      "Epoch 321, Loss 11.993508\n",
      "Params: tensor(0.4999)\n",
      "Grad: tensor([-0.3053,  1.7283])\n",
      "Epoch 322, Loss 11.962731\n",
      "Params: tensor(0.4991)\n",
      "Grad: tensor([-0.3048,  1.7253])\n",
      "Epoch 323, Loss 11.932056\n",
      "Params: tensor(0.4984)\n",
      "Grad: tensor([-0.3043,  1.7224])\n",
      "Epoch 324, Loss 11.901492\n",
      "Params: tensor(0.4976)\n",
      "Grad: tensor([-0.3037,  1.7195])\n",
      "Epoch 325, Loss 11.871029\n",
      "Params: tensor(0.4968)\n",
      "Grad: tensor([-0.3032,  1.7166])\n",
      "Epoch 326, Loss 11.840671\n",
      "Params: tensor(0.4961)\n",
      "Grad: tensor([-0.3027,  1.7136])\n",
      "Epoch 327, Loss 11.810413\n",
      "Params: tensor(0.4953)\n",
      "Grad: tensor([-0.3022,  1.7107])\n",
      "Epoch 328, Loss 11.780257\n",
      "Params: tensor(0.4946)\n",
      "Grad: tensor([-0.3017,  1.7078])\n",
      "Epoch 329, Loss 11.750208\n",
      "Params: tensor(0.4938)\n",
      "Grad: tensor([-0.3012,  1.7049])\n",
      "Epoch 330, Loss 11.720258\n",
      "Params: tensor(0.4931)\n",
      "Grad: tensor([-0.3007,  1.7020])\n",
      "Epoch 331, Loss 11.690412\n",
      "Params: tensor(0.4924)\n",
      "Grad: tensor([-0.3002,  1.6991])\n",
      "Epoch 332, Loss 11.660664\n",
      "Params: tensor(0.4916)\n",
      "Grad: tensor([-0.2996,  1.6963])\n",
      "Epoch 333, Loss 11.631015\n",
      "Params: tensor(0.4909)\n",
      "Grad: tensor([-0.2991,  1.6934])\n",
      "Epoch 334, Loss 11.601473\n",
      "Params: tensor(0.4902)\n",
      "Grad: tensor([-0.2986,  1.6905])\n",
      "Epoch 335, Loss 11.572030\n",
      "Params: tensor(0.4895)\n",
      "Grad: tensor([-0.2981,  1.6876])\n",
      "Epoch 336, Loss 11.542686\n",
      "Params: tensor(0.4887)\n",
      "Grad: tensor([-0.2976,  1.6848])\n",
      "Epoch 337, Loss 11.513440\n",
      "Params: tensor(0.4880)\n",
      "Grad: tensor([-0.2971,  1.6819])\n",
      "Epoch 338, Loss 11.484293\n",
      "Params: tensor(0.4873)\n",
      "Grad: tensor([-0.2966,  1.6790])\n",
      "Epoch 339, Loss 11.455246\n",
      "Params: tensor(0.4866)\n",
      "Grad: tensor([-0.2961,  1.6762])\n",
      "Epoch 340, Loss 11.426300\n",
      "Params: tensor(0.4859)\n",
      "Grad: tensor([-0.2956,  1.6733])\n",
      "Epoch 341, Loss 11.397448\n",
      "Params: tensor(0.4853)\n",
      "Grad: tensor([-0.2951,  1.6705])\n",
      "Epoch 342, Loss 11.368696\n",
      "Params: tensor(0.4846)\n",
      "Grad: tensor([-0.2946,  1.6677])\n",
      "Epoch 343, Loss 11.340043\n",
      "Params: tensor(0.4839)\n",
      "Grad: tensor([-0.2941,  1.6648])\n",
      "Epoch 344, Loss 11.311487\n",
      "Params: tensor(0.4832)\n",
      "Grad: tensor([-0.2936,  1.6620])\n",
      "Epoch 345, Loss 11.283028\n",
      "Params: tensor(0.4825)\n",
      "Grad: tensor([-0.2931,  1.6592])\n",
      "Epoch 346, Loss 11.254662\n",
      "Params: tensor(0.4819)\n",
      "Grad: tensor([-0.2926,  1.6564])\n",
      "Epoch 347, Loss 11.226396\n",
      "Params: tensor(0.4812)\n",
      "Grad: tensor([-0.2921,  1.6535])\n",
      "Epoch 348, Loss 11.198220\n",
      "Params: tensor(0.4805)\n",
      "Grad: tensor([-0.2916,  1.6507])\n",
      "Epoch 349, Loss 11.170150\n",
      "Params: tensor(0.4799)\n",
      "Grad: tensor([-0.2911,  1.6479])\n",
      "Epoch 350, Loss 11.142170\n",
      "Params: tensor(0.4792)\n",
      "Grad: tensor([-0.2906,  1.6451])\n",
      "Epoch 351, Loss 11.114282\n",
      "Params: tensor(0.4786)\n",
      "Grad: tensor([-0.2901,  1.6423])\n",
      "Epoch 352, Loss 11.086491\n",
      "Params: tensor(0.4779)\n",
      "Grad: tensor([-0.2896,  1.6395])\n",
      "Epoch 353, Loss 11.058797\n",
      "Params: tensor(0.4773)\n",
      "Grad: tensor([-0.2892,  1.6368])\n",
      "Epoch 354, Loss 11.031193\n",
      "Params: tensor(0.4767)\n",
      "Grad: tensor([-0.2886,  1.6340])\n",
      "Epoch 355, Loss 11.003686\n",
      "Params: tensor(0.4760)\n",
      "Grad: tensor([-0.2882,  1.6312])\n",
      "Epoch 356, Loss 10.976270\n",
      "Params: tensor(0.4754)\n",
      "Grad: tensor([-0.2877,  1.6284])\n",
      "Epoch 357, Loss 10.948948\n",
      "Params: tensor(0.4748)\n",
      "Grad: tensor([-0.2872,  1.6257])\n",
      "Epoch 358, Loss 10.921719\n",
      "Params: tensor(0.4741)\n",
      "Grad: tensor([-0.2867,  1.6229])\n",
      "Epoch 359, Loss 10.894581\n",
      "Params: tensor(0.4735)\n",
      "Grad: tensor([-0.2862,  1.6201])\n",
      "Epoch 360, Loss 10.867537\n",
      "Params: tensor(0.4729)\n",
      "Grad: tensor([-0.2857,  1.6174])\n",
      "Epoch 361, Loss 10.840583\n",
      "Params: tensor(0.4723)\n",
      "Grad: tensor([-0.2852,  1.6146])\n",
      "Epoch 362, Loss 10.813721\n",
      "Params: tensor(0.4717)\n",
      "Grad: tensor([-0.2847,  1.6119])\n",
      "Epoch 363, Loss 10.786950\n",
      "Params: tensor(0.4711)\n",
      "Grad: tensor([-0.2843,  1.6092])\n",
      "Epoch 364, Loss 10.760270\n",
      "Params: tensor(0.4705)\n",
      "Grad: tensor([-0.2838,  1.6064])\n",
      "Epoch 365, Loss 10.733681\n",
      "Params: tensor(0.4699)\n",
      "Grad: tensor([-0.2833,  1.6037])\n",
      "Epoch 366, Loss 10.707184\n",
      "Params: tensor(0.4693)\n",
      "Grad: tensor([-0.2828,  1.6010])\n",
      "Epoch 367, Loss 10.680775\n",
      "Params: tensor(0.4687)\n",
      "Grad: tensor([-0.2823,  1.5983])\n",
      "Epoch 368, Loss 10.654454\n",
      "Params: tensor(0.4681)\n",
      "Grad: tensor([-0.2819,  1.5955])\n",
      "Epoch 369, Loss 10.628225\n",
      "Params: tensor(0.4675)\n",
      "Grad: tensor([-0.2814,  1.5928])\n",
      "Epoch 370, Loss 10.602086\n",
      "Params: tensor(0.4669)\n",
      "Grad: tensor([-0.2809,  1.5901])\n",
      "Epoch 371, Loss 10.576034\n",
      "Params: tensor(0.4664)\n",
      "Grad: tensor([-0.2804,  1.5874])\n",
      "Epoch 372, Loss 10.550071\n",
      "Params: tensor(0.4658)\n",
      "Grad: tensor([-0.2799,  1.5847])\n",
      "Epoch 373, Loss 10.524194\n",
      "Params: tensor(0.4652)\n",
      "Grad: tensor([-0.2795,  1.5820])\n",
      "Epoch 374, Loss 10.498409\n",
      "Params: tensor(0.4647)\n",
      "Grad: tensor([-0.2790,  1.5794])\n",
      "Epoch 375, Loss 10.472707\n",
      "Params: tensor(0.4641)\n",
      "Grad: tensor([-0.2785,  1.5767])\n",
      "Epoch 376, Loss 10.447093\n",
      "Params: tensor(0.4635)\n",
      "Grad: tensor([-0.2780,  1.5740])\n",
      "Epoch 377, Loss 10.421569\n",
      "Params: tensor(0.4630)\n",
      "Grad: tensor([-0.2776,  1.5713])\n",
      "Epoch 378, Loss 10.396132\n",
      "Params: tensor(0.4624)\n",
      "Grad: tensor([-0.2771,  1.5686])\n",
      "Epoch 379, Loss 10.370779\n",
      "Params: tensor(0.4619)\n",
      "Grad: tensor([-0.2766,  1.5660])\n",
      "Epoch 380, Loss 10.345510\n",
      "Params: tensor(0.4613)\n",
      "Grad: tensor([-0.2762,  1.5633])\n",
      "Epoch 381, Loss 10.320328\n",
      "Params: tensor(0.4608)\n",
      "Grad: tensor([-0.2757,  1.5607])\n",
      "Epoch 382, Loss 10.295234\n",
      "Params: tensor(0.4602)\n",
      "Grad: tensor([-0.2752,  1.5580])\n",
      "Epoch 383, Loss 10.270224\n",
      "Params: tensor(0.4597)\n",
      "Grad: tensor([-0.2748,  1.5554])\n",
      "Epoch 384, Loss 10.245296\n",
      "Params: tensor(0.4591)\n",
      "Grad: tensor([-0.2743,  1.5527])\n",
      "Epoch 385, Loss 10.220457\n",
      "Params: tensor(0.4586)\n",
      "Grad: tensor([-0.2738,  1.5501])\n",
      "Epoch 386, Loss 10.195701\n",
      "Params: tensor(0.4581)\n",
      "Grad: tensor([-0.2734,  1.5475])\n",
      "Epoch 387, Loss 10.171029\n",
      "Params: tensor(0.4576)\n",
      "Grad: tensor([-0.2729,  1.5448])\n",
      "Epoch 388, Loss 10.146438\n",
      "Params: tensor(0.4570)\n",
      "Grad: tensor([-0.2724,  1.5422])\n",
      "Epoch 389, Loss 10.121935\n",
      "Params: tensor(0.4565)\n",
      "Grad: tensor([-0.2720,  1.5396])\n",
      "Epoch 390, Loss 10.097512\n",
      "Params: tensor(0.4560)\n",
      "Grad: tensor([-0.2715,  1.5370])\n",
      "Epoch 391, Loss 10.073173\n",
      "Params: tensor(0.4555)\n",
      "Grad: tensor([-0.2711,  1.5344])\n",
      "Epoch 392, Loss 10.048919\n",
      "Params: tensor(0.4550)\n",
      "Grad: tensor([-0.2706,  1.5317])\n",
      "Epoch 393, Loss 10.024743\n",
      "Params: tensor(0.4544)\n",
      "Grad: tensor([-0.2701,  1.5291])\n",
      "Epoch 394, Loss 10.000652\n",
      "Params: tensor(0.4539)\n",
      "Grad: tensor([-0.2697,  1.5265])\n",
      "Epoch 395, Loss 9.976640\n",
      "Params: tensor(0.4534)\n",
      "Grad: tensor([-0.2692,  1.5240])\n",
      "Epoch 396, Loss 9.952712\n",
      "Params: tensor(0.4529)\n",
      "Grad: tensor([-0.2688,  1.5214])\n",
      "Epoch 397, Loss 9.928862\n",
      "Params: tensor(0.4524)\n",
      "Grad: tensor([-0.2683,  1.5188])\n",
      "Epoch 398, Loss 9.905093\n",
      "Params: tensor(0.4519)\n",
      "Grad: tensor([-0.2678,  1.5162])\n",
      "Epoch 399, Loss 9.881409\n",
      "Params: tensor(0.4514)\n",
      "Grad: tensor([-0.2674,  1.5136])\n",
      "Epoch 400, Loss 9.857804\n",
      "Params: tensor(0.4509)\n",
      "Grad: tensor([-0.2669,  1.5111])\n",
      "Epoch 401, Loss 9.834277\n",
      "Params: tensor(0.4504)\n",
      "Grad: tensor([-0.2665,  1.5085])\n",
      "Epoch 402, Loss 9.810831\n",
      "Params: tensor(0.4500)\n",
      "Grad: tensor([-0.2660,  1.5059])\n",
      "Epoch 403, Loss 9.787466\n",
      "Params: tensor(0.4495)\n",
      "Grad: tensor([-0.2656,  1.5034])\n",
      "Epoch 404, Loss 9.764176\n",
      "Params: tensor(0.4490)\n",
      "Grad: tensor([-0.2651,  1.5008])\n",
      "Epoch 405, Loss 9.740973\n",
      "Params: tensor(0.4485)\n",
      "Grad: tensor([-0.2647,  1.4983])\n",
      "Epoch 406, Loss 9.717843\n",
      "Params: tensor(0.4480)\n",
      "Grad: tensor([-0.2642,  1.4957])\n",
      "Epoch 407, Loss 9.694793\n",
      "Params: tensor(0.4476)\n",
      "Grad: tensor([-0.2638,  1.4932])\n",
      "Epoch 408, Loss 9.671824\n",
      "Params: tensor(0.4471)\n",
      "Grad: tensor([-0.2633,  1.4906])\n",
      "Epoch 409, Loss 9.648926\n",
      "Params: tensor(0.4466)\n",
      "Grad: tensor([-0.2629,  1.4881])\n",
      "Epoch 410, Loss 9.626110\n",
      "Params: tensor(0.4461)\n",
      "Grad: tensor([-0.2624,  1.4856])\n",
      "Epoch 411, Loss 9.603373\n",
      "Params: tensor(0.4457)\n",
      "Grad: tensor([-0.2620,  1.4831])\n",
      "Epoch 412, Loss 9.580709\n",
      "Params: tensor(0.4452)\n",
      "Grad: tensor([-0.2615,  1.4805])\n",
      "Epoch 413, Loss 9.558125\n",
      "Params: tensor(0.4448)\n",
      "Grad: tensor([-0.2611,  1.4780])\n",
      "Epoch 414, Loss 9.535617\n",
      "Params: tensor(0.4443)\n",
      "Grad: tensor([-0.2606,  1.4755])\n",
      "Epoch 415, Loss 9.513184\n",
      "Params: tensor(0.4438)\n",
      "Grad: tensor([-0.2602,  1.4730])\n",
      "Epoch 416, Loss 9.490829\n",
      "Params: tensor(0.4434)\n",
      "Grad: tensor([-0.2598,  1.4705])\n",
      "Epoch 417, Loss 9.468551\n",
      "Params: tensor(0.4429)\n",
      "Grad: tensor([-0.2593,  1.4680])\n",
      "Epoch 418, Loss 9.446347\n",
      "Params: tensor(0.4425)\n",
      "Grad: tensor([-0.2589,  1.4655])\n",
      "Epoch 419, Loss 9.424216\n",
      "Params: tensor(0.4420)\n",
      "Grad: tensor([-0.2584,  1.4630])\n",
      "Epoch 420, Loss 9.402164\n",
      "Params: tensor(0.4416)\n",
      "Grad: tensor([-0.2580,  1.4605])\n",
      "Epoch 421, Loss 9.380184\n",
      "Params: tensor(0.4412)\n",
      "Grad: tensor([-0.2576,  1.4581])\n",
      "Epoch 422, Loss 9.358282\n",
      "Params: tensor(0.4407)\n",
      "Grad: tensor([-0.2571,  1.4556])\n",
      "Epoch 423, Loss 9.336448\n",
      "Params: tensor(0.4403)\n",
      "Grad: tensor([-0.2567,  1.4531])\n",
      "Epoch 424, Loss 9.314695\n",
      "Params: tensor(0.4398)\n",
      "Grad: tensor([-0.2563,  1.4506])\n",
      "Epoch 425, Loss 9.293012\n",
      "Params: tensor(0.4394)\n",
      "Grad: tensor([-0.2558,  1.4482])\n",
      "Epoch 426, Loss 9.271403\n",
      "Params: tensor(0.4390)\n",
      "Grad: tensor([-0.2554,  1.4457])\n",
      "Epoch 427, Loss 9.249871\n",
      "Params: tensor(0.4385)\n",
      "Grad: tensor([-0.2550,  1.4433])\n",
      "Epoch 428, Loss 9.228410\n",
      "Params: tensor(0.4381)\n",
      "Grad: tensor([-0.2545,  1.4408])\n",
      "Epoch 429, Loss 9.207022\n",
      "Params: tensor(0.4377)\n",
      "Grad: tensor([-0.2541,  1.4384])\n",
      "Epoch 430, Loss 9.185704\n",
      "Params: tensor(0.4373)\n",
      "Grad: tensor([-0.2537,  1.4359])\n",
      "Epoch 431, Loss 9.164462\n",
      "Params: tensor(0.4369)\n",
      "Grad: tensor([-0.2532,  1.4335])\n",
      "Epoch 432, Loss 9.143289\n",
      "Params: tensor(0.4364)\n",
      "Grad: tensor([-0.2528,  1.4310])\n",
      "Epoch 433, Loss 9.122189\n",
      "Params: tensor(0.4360)\n",
      "Grad: tensor([-0.2524,  1.4286])\n",
      "Epoch 434, Loss 9.101160\n",
      "Params: tensor(0.4356)\n",
      "Grad: tensor([-0.2519,  1.4262])\n",
      "Epoch 435, Loss 9.080204\n",
      "Params: tensor(0.4352)\n",
      "Grad: tensor([-0.2515,  1.4238])\n",
      "Epoch 436, Loss 9.059318\n",
      "Params: tensor(0.4348)\n",
      "Grad: tensor([-0.2511,  1.4213])\n",
      "Epoch 437, Loss 9.038502\n",
      "Params: tensor(0.4344)\n",
      "Grad: tensor([-0.2507,  1.4189])\n",
      "Epoch 438, Loss 9.017757\n",
      "Params: tensor(0.4340)\n",
      "Grad: tensor([-0.2502,  1.4165])\n",
      "Epoch 439, Loss 8.997084\n",
      "Params: tensor(0.4336)\n",
      "Grad: tensor([-0.2498,  1.4141])\n",
      "Epoch 440, Loss 8.976479\n",
      "Params: tensor(0.4332)\n",
      "Grad: tensor([-0.2494,  1.4117])\n",
      "Epoch 441, Loss 8.955944\n",
      "Params: tensor(0.4328)\n",
      "Grad: tensor([-0.2489,  1.4093])\n",
      "Epoch 442, Loss 8.935480\n",
      "Params: tensor(0.4324)\n",
      "Grad: tensor([-0.2485,  1.4069])\n",
      "Epoch 443, Loss 8.915089\n",
      "Params: tensor(0.4320)\n",
      "Grad: tensor([-0.2481,  1.4045])\n",
      "Epoch 444, Loss 8.894762\n",
      "Params: tensor(0.4316)\n",
      "Grad: tensor([-0.2477,  1.4021])\n",
      "Epoch 445, Loss 8.874508\n",
      "Params: tensor(0.4312)\n",
      "Grad: tensor([-0.2473,  1.3998])\n",
      "Epoch 446, Loss 8.854318\n",
      "Params: tensor(0.4308)\n",
      "Grad: tensor([-0.2468,  1.3974])\n",
      "Epoch 447, Loss 8.834197\n",
      "Params: tensor(0.4304)\n",
      "Grad: tensor([-0.2464,  1.3950])\n",
      "Epoch 448, Loss 8.814149\n",
      "Params: tensor(0.4300)\n",
      "Grad: tensor([-0.2460,  1.3926])\n",
      "Epoch 449, Loss 8.794162\n",
      "Params: tensor(0.4296)\n",
      "Grad: tensor([-0.2456,  1.3903])\n",
      "Epoch 450, Loss 8.774253\n",
      "Params: tensor(0.4292)\n",
      "Grad: tensor([-0.2452,  1.3879])\n",
      "Epoch 451, Loss 8.754405\n",
      "Params: tensor(0.4289)\n",
      "Grad: tensor([-0.2448,  1.3856])\n",
      "Epoch 452, Loss 8.734623\n",
      "Params: tensor(0.4285)\n",
      "Grad: tensor([-0.2443,  1.3832])\n",
      "Epoch 453, Loss 8.714911\n",
      "Params: tensor(0.4281)\n",
      "Grad: tensor([-0.2439,  1.3808])\n",
      "Epoch 454, Loss 8.695266\n",
      "Params: tensor(0.4277)\n",
      "Grad: tensor([-0.2435,  1.3785])\n",
      "Epoch 455, Loss 8.675688\n",
      "Params: tensor(0.4273)\n",
      "Grad: tensor([-0.2431,  1.3762])\n",
      "Epoch 456, Loss 8.656173\n",
      "Params: tensor(0.4270)\n",
      "Grad: tensor([-0.2427,  1.3738])\n",
      "Epoch 457, Loss 8.636729\n",
      "Params: tensor(0.4266)\n",
      "Grad: tensor([-0.2423,  1.3715])\n",
      "Epoch 458, Loss 8.617347\n",
      "Params: tensor(0.4262)\n",
      "Grad: tensor([-0.2419,  1.3692])\n",
      "Epoch 459, Loss 8.598029\n",
      "Params: tensor(0.4259)\n",
      "Grad: tensor([-0.2414,  1.3668])\n",
      "Epoch 460, Loss 8.578781\n",
      "Params: tensor(0.4255)\n",
      "Grad: tensor([-0.2410,  1.3645])\n",
      "Epoch 461, Loss 8.559597\n",
      "Params: tensor(0.4251)\n",
      "Grad: tensor([-0.2406,  1.3622])\n",
      "Epoch 462, Loss 8.540479\n",
      "Params: tensor(0.4248)\n",
      "Grad: tensor([-0.2402,  1.3599])\n",
      "Epoch 463, Loss 8.521426\n",
      "Params: tensor(0.4244)\n",
      "Grad: tensor([-0.2398,  1.3576])\n",
      "Epoch 464, Loss 8.502437\n",
      "Params: tensor(0.4240)\n",
      "Grad: tensor([-0.2394,  1.3553])\n",
      "Epoch 465, Loss 8.483517\n",
      "Params: tensor(0.4237)\n",
      "Grad: tensor([-0.2390,  1.3530])\n",
      "Epoch 466, Loss 8.464652\n",
      "Params: tensor(0.4233)\n",
      "Grad: tensor([-0.2386,  1.3507])\n",
      "Epoch 467, Loss 8.445858\n",
      "Params: tensor(0.4230)\n",
      "Grad: tensor([-0.2382,  1.3484])\n",
      "Epoch 468, Loss 8.427128\n",
      "Params: tensor(0.4226)\n",
      "Grad: tensor([-0.2378,  1.3461])\n",
      "Epoch 469, Loss 8.408454\n",
      "Params: tensor(0.4223)\n",
      "Grad: tensor([-0.2374,  1.3438])\n",
      "Epoch 470, Loss 8.389848\n",
      "Params: tensor(0.4219)\n",
      "Grad: tensor([-0.2370,  1.3415])\n",
      "Epoch 471, Loss 8.371305\n",
      "Params: tensor(0.4216)\n",
      "Grad: tensor([-0.2366,  1.3392])\n",
      "Epoch 472, Loss 8.352828\n",
      "Params: tensor(0.4212)\n",
      "Grad: tensor([-0.2362,  1.3370])\n",
      "Epoch 473, Loss 8.334409\n",
      "Params: tensor(0.4209)\n",
      "Grad: tensor([-0.2358,  1.3347])\n",
      "Epoch 474, Loss 8.316054\n",
      "Params: tensor(0.4205)\n",
      "Grad: tensor([-0.2354,  1.3324])\n",
      "Epoch 475, Loss 8.297764\n",
      "Params: tensor(0.4202)\n",
      "Grad: tensor([-0.2350,  1.3301])\n",
      "Epoch 476, Loss 8.279534\n",
      "Params: tensor(0.4199)\n",
      "Grad: tensor([-0.2346,  1.3279])\n",
      "Epoch 477, Loss 8.261369\n",
      "Params: tensor(0.4195)\n",
      "Grad: tensor([-0.2342,  1.3256])\n",
      "Epoch 478, Loss 8.243259\n",
      "Params: tensor(0.4192)\n",
      "Grad: tensor([-0.2338,  1.3234])\n",
      "Epoch 479, Loss 8.225213\n",
      "Params: tensor(0.4188)\n",
      "Grad: tensor([-0.2334,  1.3211])\n",
      "Epoch 480, Loss 8.207231\n",
      "Params: tensor(0.4185)\n",
      "Grad: tensor([-0.2330,  1.3189])\n",
      "Epoch 481, Loss 8.189310\n",
      "Params: tensor(0.4182)\n",
      "Grad: tensor([-0.2326,  1.3166])\n",
      "Epoch 482, Loss 8.171452\n",
      "Params: tensor(0.4178)\n",
      "Grad: tensor([-0.2322,  1.3144])\n",
      "Epoch 483, Loss 8.153647\n",
      "Params: tensor(0.4175)\n",
      "Grad: tensor([-0.2318,  1.3122])\n",
      "Epoch 484, Loss 8.135906\n",
      "Params: tensor(0.4172)\n",
      "Grad: tensor([-0.2314,  1.3100])\n",
      "Epoch 485, Loss 8.118226\n",
      "Params: tensor(0.4169)\n",
      "Grad: tensor([-0.2310,  1.3077])\n",
      "Epoch 486, Loss 8.100607\n",
      "Params: tensor(0.4165)\n",
      "Grad: tensor([-0.2306,  1.3055])\n",
      "Epoch 487, Loss 8.083045\n",
      "Params: tensor(0.4162)\n",
      "Grad: tensor([-0.2302,  1.3033])\n",
      "Epoch 488, Loss 8.065548\n",
      "Params: tensor(0.4159)\n",
      "Grad: tensor([-0.2298,  1.3011])\n",
      "Epoch 489, Loss 8.048104\n",
      "Params: tensor(0.4156)\n",
      "Grad: tensor([-0.2295,  1.2989])\n",
      "Epoch 490, Loss 8.030724\n",
      "Params: tensor(0.4153)\n",
      "Grad: tensor([-0.2291,  1.2967])\n",
      "Epoch 491, Loss 8.013401\n",
      "Params: tensor(0.4149)\n",
      "Grad: tensor([-0.2287,  1.2945])\n",
      "Epoch 492, Loss 7.996137\n",
      "Params: tensor(0.4146)\n",
      "Grad: tensor([-0.2283,  1.2923])\n",
      "Epoch 493, Loss 7.978930\n",
      "Params: tensor(0.4143)\n",
      "Grad: tensor([-0.2279,  1.2901])\n",
      "Epoch 494, Loss 7.961783\n",
      "Params: tensor(0.4140)\n",
      "Grad: tensor([-0.2275,  1.2879])\n",
      "Epoch 495, Loss 7.944690\n",
      "Params: tensor(0.4137)\n",
      "Grad: tensor([-0.2271,  1.2857])\n",
      "Epoch 496, Loss 7.927663\n",
      "Params: tensor(0.4134)\n",
      "Grad: tensor([-0.2267,  1.2835])\n",
      "Epoch 497, Loss 7.910690\n",
      "Params: tensor(0.4131)\n",
      "Grad: tensor([-0.2263,  1.2813])\n",
      "Epoch 498, Loss 7.893775\n",
      "Params: tensor(0.4127)\n",
      "Grad: tensor([-0.2260,  1.2791])\n",
      "Epoch 499, Loss 7.876915\n",
      "Params: tensor(0.4124)\n",
      "Grad: tensor([-0.2256,  1.2770])\n",
      "Epoch 500, Loss 7.860115\n",
      "Params: tensor(0.4121)\n",
      "Grad: tensor([-0.2252,  1.2748])\n",
      "Epoch 501, Loss 7.843369\n",
      "Params: tensor(0.4118)\n",
      "Grad: tensor([-0.2248,  1.2726])\n",
      "Epoch 502, Loss 7.826683\n",
      "Params: tensor(0.4115)\n",
      "Grad: tensor([-0.2244,  1.2705])\n",
      "Epoch 503, Loss 7.810053\n",
      "Params: tensor(0.4112)\n",
      "Grad: tensor([-0.2241,  1.2683])\n",
      "Epoch 504, Loss 7.793481\n",
      "Params: tensor(0.4109)\n",
      "Grad: tensor([-0.2237,  1.2662])\n",
      "Epoch 505, Loss 7.776962\n",
      "Params: tensor(0.4106)\n",
      "Grad: tensor([-0.2233,  1.2640])\n",
      "Epoch 506, Loss 7.760498\n",
      "Params: tensor(0.4103)\n",
      "Grad: tensor([-0.2229,  1.2619])\n",
      "Epoch 507, Loss 7.744092\n",
      "Params: tensor(0.4100)\n",
      "Grad: tensor([-0.2225,  1.2597])\n",
      "Epoch 508, Loss 7.727745\n",
      "Params: tensor(0.4097)\n",
      "Grad: tensor([-0.2222,  1.2576])\n",
      "Epoch 509, Loss 7.711449\n",
      "Params: tensor(0.4094)\n",
      "Grad: tensor([-0.2218,  1.2554])\n",
      "Epoch 510, Loss 7.695211\n",
      "Params: tensor(0.4091)\n",
      "Grad: tensor([-0.2214,  1.2533])\n",
      "Epoch 511, Loss 7.679024\n",
      "Params: tensor(0.4088)\n",
      "Grad: tensor([-0.2210,  1.2512])\n",
      "Epoch 512, Loss 7.662896\n",
      "Params: tensor(0.4086)\n",
      "Grad: tensor([-0.2207,  1.2490])\n",
      "Epoch 513, Loss 7.646820\n",
      "Params: tensor(0.4083)\n",
      "Grad: tensor([-0.2203,  1.2469])\n",
      "Epoch 514, Loss 7.630803\n",
      "Params: tensor(0.4080)\n",
      "Grad: tensor([-0.2199,  1.2448])\n",
      "Epoch 515, Loss 7.614836\n",
      "Params: tensor(0.4077)\n",
      "Grad: tensor([-0.2195,  1.2427])\n",
      "Epoch 516, Loss 7.598925\n",
      "Params: tensor(0.4074)\n",
      "Grad: tensor([-0.2192,  1.2406])\n",
      "Epoch 517, Loss 7.583069\n",
      "Params: tensor(0.4071)\n",
      "Grad: tensor([-0.2188,  1.2385])\n",
      "Epoch 518, Loss 7.567265\n",
      "Params: tensor(0.4068)\n",
      "Grad: tensor([-0.2184,  1.2364])\n",
      "Epoch 519, Loss 7.551515\n",
      "Params: tensor(0.4065)\n",
      "Grad: tensor([-0.2180,  1.2343])\n",
      "Epoch 520, Loss 7.535818\n",
      "Params: tensor(0.4063)\n",
      "Grad: tensor([-0.2177,  1.2322])\n",
      "Epoch 521, Loss 7.520176\n",
      "Params: tensor(0.4060)\n",
      "Grad: tensor([-0.2173,  1.2301])\n",
      "Epoch 522, Loss 7.504587\n",
      "Params: tensor(0.4057)\n",
      "Grad: tensor([-0.2169,  1.2280])\n",
      "Epoch 523, Loss 7.489048\n",
      "Params: tensor(0.4054)\n",
      "Grad: tensor([-0.2165,  1.2259])\n",
      "Epoch 524, Loss 7.473566\n",
      "Params: tensor(0.4052)\n",
      "Grad: tensor([-0.2162,  1.2238])\n",
      "Epoch 525, Loss 7.458135\n",
      "Params: tensor(0.4049)\n",
      "Grad: tensor([-0.2158,  1.2217])\n",
      "Epoch 526, Loss 7.442750\n",
      "Params: tensor(0.4046)\n",
      "Grad: tensor([-0.2155,  1.2197])\n",
      "Epoch 527, Loss 7.427427\n",
      "Params: tensor(0.4043)\n",
      "Grad: tensor([-0.2151,  1.2176])\n",
      "Epoch 528, Loss 7.412152\n",
      "Params: tensor(0.4041)\n",
      "Grad: tensor([-0.2147,  1.2155])\n",
      "Epoch 529, Loss 7.396928\n",
      "Params: tensor(0.4038)\n",
      "Grad: tensor([-0.2144,  1.2135])\n",
      "Epoch 530, Loss 7.381757\n",
      "Params: tensor(0.4035)\n",
      "Grad: tensor([-0.2140,  1.2114])\n",
      "Epoch 531, Loss 7.366637\n",
      "Params: tensor(0.4032)\n",
      "Grad: tensor([-0.2136,  1.2093])\n",
      "Epoch 532, Loss 7.351567\n",
      "Params: tensor(0.4030)\n",
      "Grad: tensor([-0.2133,  1.2073])\n",
      "Epoch 533, Loss 7.336549\n",
      "Params: tensor(0.4027)\n",
      "Grad: tensor([-0.2129,  1.2052])\n",
      "Epoch 534, Loss 7.321584\n",
      "Params: tensor(0.4024)\n",
      "Grad: tensor([-0.2125,  1.2032])\n",
      "Epoch 535, Loss 7.306671\n",
      "Params: tensor(0.4022)\n",
      "Grad: tensor([-0.2122,  1.2012])\n",
      "Epoch 536, Loss 7.291804\n",
      "Params: tensor(0.4019)\n",
      "Grad: tensor([-0.2118,  1.1991])\n",
      "Epoch 537, Loss 7.276989\n",
      "Params: tensor(0.4017)\n",
      "Grad: tensor([-0.2115,  1.1971])\n",
      "Epoch 538, Loss 7.262227\n",
      "Params: tensor(0.4014)\n",
      "Grad: tensor([-0.2111,  1.1950])\n",
      "Epoch 539, Loss 7.247512\n",
      "Params: tensor(0.4011)\n",
      "Grad: tensor([-0.2108,  1.1930])\n",
      "Epoch 540, Loss 7.232845\n",
      "Params: tensor(0.4009)\n",
      "Grad: tensor([-0.2104,  1.1910])\n",
      "Epoch 541, Loss 7.218231\n",
      "Params: tensor(0.4006)\n",
      "Grad: tensor([-0.2100,  1.1890])\n",
      "Epoch 542, Loss 7.203665\n",
      "Params: tensor(0.4004)\n",
      "Grad: tensor([-0.2097,  1.1869])\n",
      "Epoch 543, Loss 7.189151\n",
      "Params: tensor(0.4001)\n",
      "Grad: tensor([-0.2093,  1.1849])\n",
      "Epoch 544, Loss 7.174683\n",
      "Params: tensor(0.3998)\n",
      "Grad: tensor([-0.2090,  1.1829])\n",
      "Epoch 545, Loss 7.160266\n",
      "Params: tensor(0.3996)\n",
      "Grad: tensor([-0.2086,  1.1809])\n",
      "Epoch 546, Loss 7.145897\n",
      "Params: tensor(0.3993)\n",
      "Grad: tensor([-0.2083,  1.1789])\n",
      "Epoch 547, Loss 7.131581\n",
      "Params: tensor(0.3991)\n",
      "Grad: tensor([-0.2079,  1.1769])\n",
      "Epoch 548, Loss 7.117305\n",
      "Params: tensor(0.3988)\n",
      "Grad: tensor([-0.2075,  1.1749])\n",
      "Epoch 549, Loss 7.103083\n",
      "Params: tensor(0.3986)\n",
      "Grad: tensor([-0.2072,  1.1729])\n",
      "Epoch 550, Loss 7.088911\n",
      "Params: tensor(0.3983)\n",
      "Grad: tensor([-0.2068,  1.1709])\n",
      "Epoch 551, Loss 7.074785\n",
      "Params: tensor(0.3981)\n",
      "Grad: tensor([-0.2065,  1.1689])\n",
      "Epoch 552, Loss 7.060707\n",
      "Params: tensor(0.3978)\n",
      "Grad: tensor([-0.2062,  1.1669])\n",
      "Epoch 553, Loss 7.046676\n",
      "Params: tensor(0.3976)\n",
      "Grad: tensor([-0.2058,  1.1649])\n",
      "Epoch 554, Loss 7.032695\n",
      "Params: tensor(0.3973)\n",
      "Grad: tensor([-0.2054,  1.1630])\n",
      "Epoch 555, Loss 7.018755\n",
      "Params: tensor(0.3971)\n",
      "Grad: tensor([-0.2051,  1.1610])\n",
      "Epoch 556, Loss 7.004870\n",
      "Params: tensor(0.3969)\n",
      "Grad: tensor([-0.2047,  1.1590])\n",
      "Epoch 557, Loss 6.991028\n",
      "Params: tensor(0.3966)\n",
      "Grad: tensor([-0.2044,  1.1571])\n",
      "Epoch 558, Loss 6.977232\n",
      "Params: tensor(0.3964)\n",
      "Grad: tensor([-0.2041,  1.1551])\n",
      "Epoch 559, Loss 6.963488\n",
      "Params: tensor(0.3961)\n",
      "Grad: tensor([-0.2037,  1.1531])\n",
      "Epoch 560, Loss 6.949787\n",
      "Params: tensor(0.3959)\n",
      "Grad: tensor([-0.2034,  1.1512])\n",
      "Epoch 561, Loss 6.936135\n",
      "Params: tensor(0.3957)\n",
      "Grad: tensor([-0.2030,  1.1492])\n",
      "Epoch 562, Loss 6.922528\n",
      "Params: tensor(0.3954)\n",
      "Grad: tensor([-0.2027,  1.1473])\n",
      "Epoch 563, Loss 6.908967\n",
      "Params: tensor(0.3952)\n",
      "Grad: tensor([-0.2023,  1.1453])\n",
      "Epoch 564, Loss 6.895452\n",
      "Params: tensor(0.3949)\n",
      "Grad: tensor([-0.2020,  1.1434])\n",
      "Epoch 565, Loss 6.881980\n",
      "Params: tensor(0.3947)\n",
      "Grad: tensor([-0.2016,  1.1414])\n",
      "Epoch 566, Loss 6.868559\n",
      "Params: tensor(0.3945)\n",
      "Grad: tensor([-0.2013,  1.1395])\n",
      "Epoch 567, Loss 6.855180\n",
      "Params: tensor(0.3942)\n",
      "Grad: tensor([-0.2010,  1.1375])\n",
      "Epoch 568, Loss 6.841848\n",
      "Params: tensor(0.3940)\n",
      "Grad: tensor([-0.2006,  1.1356])\n",
      "Epoch 569, Loss 6.828561\n",
      "Params: tensor(0.3938)\n",
      "Grad: tensor([-0.2003,  1.1337])\n",
      "Epoch 570, Loss 6.815319\n",
      "Params: tensor(0.3935)\n",
      "Grad: tensor([-0.1999,  1.1318])\n",
      "Epoch 571, Loss 6.802118\n",
      "Params: tensor(0.3933)\n",
      "Grad: tensor([-0.1996,  1.1298])\n",
      "Epoch 572, Loss 6.788968\n",
      "Params: tensor(0.3931)\n",
      "Grad: tensor([-0.1993,  1.1279])\n",
      "Epoch 573, Loss 6.775864\n",
      "Params: tensor(0.3929)\n",
      "Grad: tensor([-0.1989,  1.1260])\n",
      "Epoch 574, Loss 6.762797\n",
      "Params: tensor(0.3926)\n",
      "Grad: tensor([-0.1986,  1.1241])\n",
      "Epoch 575, Loss 6.749779\n",
      "Params: tensor(0.3924)\n",
      "Grad: tensor([-0.1982,  1.1222])\n",
      "Epoch 576, Loss 6.736804\n",
      "Params: tensor(0.3922)\n",
      "Grad: tensor([-0.1979,  1.1203])\n",
      "Epoch 577, Loss 6.723876\n",
      "Params: tensor(0.3920)\n",
      "Grad: tensor([-0.1976,  1.1184])\n",
      "Epoch 578, Loss 6.710987\n",
      "Params: tensor(0.3917)\n",
      "Grad: tensor([-0.1972,  1.1165])\n",
      "Epoch 579, Loss 6.698142\n",
      "Params: tensor(0.3915)\n",
      "Grad: tensor([-0.1969,  1.1146])\n",
      "Epoch 580, Loss 6.685345\n",
      "Params: tensor(0.3913)\n",
      "Grad: tensor([-0.1966,  1.1127])\n",
      "Epoch 581, Loss 6.672589\n",
      "Params: tensor(0.3911)\n",
      "Grad: tensor([-0.1962,  1.1108])\n",
      "Epoch 582, Loss 6.659873\n",
      "Params: tensor(0.3908)\n",
      "Grad: tensor([-0.1959,  1.1089])\n",
      "Epoch 583, Loss 6.647207\n",
      "Params: tensor(0.3906)\n",
      "Grad: tensor([-0.1956,  1.1070])\n",
      "Epoch 584, Loss 6.634578\n",
      "Params: tensor(0.3904)\n",
      "Grad: tensor([-0.1952,  1.1051])\n",
      "Epoch 585, Loss 6.621994\n",
      "Params: tensor(0.3902)\n",
      "Grad: tensor([-0.1949,  1.1033])\n",
      "Epoch 586, Loss 6.609454\n",
      "Params: tensor(0.3900)\n",
      "Grad: tensor([-0.1946,  1.1014])\n",
      "Epoch 587, Loss 6.596953\n",
      "Params: tensor(0.3898)\n",
      "Grad: tensor([-0.1942,  1.0995])\n",
      "Epoch 588, Loss 6.584499\n",
      "Params: tensor(0.3895)\n",
      "Grad: tensor([-0.1939,  1.0976])\n",
      "Epoch 589, Loss 6.572087\n",
      "Params: tensor(0.3893)\n",
      "Grad: tensor([-0.1936,  1.0958])\n",
      "Epoch 590, Loss 6.559712\n",
      "Params: tensor(0.3891)\n",
      "Grad: tensor([-0.1932,  1.0939])\n",
      "Epoch 591, Loss 6.547384\n",
      "Params: tensor(0.3889)\n",
      "Grad: tensor([-0.1929,  1.0921])\n",
      "Epoch 592, Loss 6.535097\n",
      "Params: tensor(0.3887)\n",
      "Grad: tensor([-0.1926,  1.0902])\n",
      "Epoch 593, Loss 6.522851\n",
      "Params: tensor(0.3885)\n",
      "Grad: tensor([-0.1923,  1.0884])\n",
      "Epoch 594, Loss 6.510646\n",
      "Params: tensor(0.3883)\n",
      "Grad: tensor([-0.1919,  1.0865])\n",
      "Epoch 595, Loss 6.498482\n",
      "Params: tensor(0.3881)\n",
      "Grad: tensor([-0.1916,  1.0847])\n",
      "Epoch 596, Loss 6.486361\n",
      "Params: tensor(0.3878)\n",
      "Grad: tensor([-0.1913,  1.0828])\n",
      "Epoch 597, Loss 6.474282\n",
      "Params: tensor(0.3876)\n",
      "Grad: tensor([-0.1910,  1.0810])\n",
      "Epoch 598, Loss 6.462241\n",
      "Params: tensor(0.3874)\n",
      "Grad: tensor([-0.1906,  1.0791])\n",
      "Epoch 599, Loss 6.450243\n",
      "Params: tensor(0.3872)\n",
      "Grad: tensor([-0.1903,  1.0773])\n",
      "Epoch 600, Loss 6.438284\n",
      "Params: tensor(0.3870)\n",
      "Grad: tensor([-0.1900,  1.0755])\n",
      "Epoch 601, Loss 6.426368\n",
      "Params: tensor(0.3868)\n",
      "Grad: tensor([-0.1897,  1.0737])\n",
      "Epoch 602, Loss 6.414490\n",
      "Params: tensor(0.3866)\n",
      "Grad: tensor([-0.1893,  1.0718])\n",
      "Epoch 603, Loss 6.402653\n",
      "Params: tensor(0.3864)\n",
      "Grad: tensor([-0.1890,  1.0700])\n",
      "Epoch 604, Loss 6.390859\n",
      "Params: tensor(0.3862)\n",
      "Grad: tensor([-0.1887,  1.0682])\n",
      "Epoch 605, Loss 6.379103\n",
      "Params: tensor(0.3860)\n",
      "Grad: tensor([-0.1884,  1.0664])\n",
      "Epoch 606, Loss 6.367385\n",
      "Params: tensor(0.3858)\n",
      "Grad: tensor([-0.1880,  1.0646])\n",
      "Epoch 607, Loss 6.355706\n",
      "Params: tensor(0.3856)\n",
      "Grad: tensor([-0.1877,  1.0628])\n",
      "Epoch 608, Loss 6.344070\n",
      "Params: tensor(0.3854)\n",
      "Grad: tensor([-0.1874,  1.0609])\n",
      "Epoch 609, Loss 6.332472\n",
      "Params: tensor(0.3852)\n",
      "Grad: tensor([-0.1871,  1.0591])\n",
      "Epoch 610, Loss 6.320912\n",
      "Params: tensor(0.3850)\n",
      "Grad: tensor([-0.1868,  1.0573])\n",
      "Epoch 611, Loss 6.309395\n",
      "Params: tensor(0.3848)\n",
      "Grad: tensor([-0.1865,  1.0555])\n",
      "Epoch 612, Loss 6.297915\n",
      "Params: tensor(0.3846)\n",
      "Grad: tensor([-0.1861,  1.0538])\n",
      "Epoch 613, Loss 6.286473\n",
      "Params: tensor(0.3844)\n",
      "Grad: tensor([-0.1858,  1.0520])\n",
      "Epoch 614, Loss 6.275074\n",
      "Params: tensor(0.3842)\n",
      "Grad: tensor([-0.1855,  1.0502])\n",
      "Epoch 615, Loss 6.263708\n",
      "Params: tensor(0.3840)\n",
      "Grad: tensor([-0.1852,  1.0484])\n",
      "Epoch 616, Loss 6.252382\n",
      "Params: tensor(0.3838)\n",
      "Grad: tensor([-0.1849,  1.0466])\n",
      "Epoch 617, Loss 6.241098\n",
      "Params: tensor(0.3836)\n",
      "Grad: tensor([-0.1846,  1.0448])\n",
      "Epoch 618, Loss 6.229849\n",
      "Params: tensor(0.3834)\n",
      "Grad: tensor([-0.1843,  1.0431])\n",
      "Epoch 619, Loss 6.218639\n",
      "Params: tensor(0.3832)\n",
      "Grad: tensor([-0.1840,  1.0413])\n",
      "Epoch 620, Loss 6.207470\n",
      "Params: tensor(0.3830)\n",
      "Grad: tensor([-0.1836,  1.0395])\n",
      "Epoch 621, Loss 6.196334\n",
      "Params: tensor(0.3829)\n",
      "Grad: tensor([-0.1833,  1.0378])\n",
      "Epoch 622, Loss 6.185240\n",
      "Params: tensor(0.3827)\n",
      "Grad: tensor([-0.1830,  1.0360])\n",
      "Epoch 623, Loss 6.174181\n",
      "Params: tensor(0.3825)\n",
      "Grad: tensor([-0.1827,  1.0342])\n",
      "Epoch 624, Loss 6.163159\n",
      "Params: tensor(0.3823)\n",
      "Grad: tensor([-0.1824,  1.0325])\n",
      "Epoch 625, Loss 6.152177\n",
      "Params: tensor(0.3821)\n",
      "Grad: tensor([-0.1821,  1.0307])\n",
      "Epoch 626, Loss 6.141230\n",
      "Params: tensor(0.3819)\n",
      "Grad: tensor([-0.1818,  1.0290])\n",
      "Epoch 627, Loss 6.130322\n",
      "Params: tensor(0.3817)\n",
      "Grad: tensor([-0.1815,  1.0272])\n",
      "Epoch 628, Loss 6.119448\n",
      "Params: tensor(0.3815)\n",
      "Grad: tensor([-0.1811,  1.0255])\n",
      "Epoch 629, Loss 6.108614\n",
      "Params: tensor(0.3814)\n",
      "Grad: tensor([-0.1808,  1.0237])\n",
      "Epoch 630, Loss 6.097815\n",
      "Params: tensor(0.3812)\n",
      "Grad: tensor([-0.1805,  1.0220])\n",
      "Epoch 631, Loss 6.087054\n",
      "Params: tensor(0.3810)\n",
      "Grad: tensor([-0.1802,  1.0203])\n",
      "Epoch 632, Loss 6.076329\n",
      "Params: tensor(0.3808)\n",
      "Grad: tensor([-0.1799,  1.0185])\n",
      "Epoch 633, Loss 6.065644\n",
      "Params: tensor(0.3806)\n",
      "Grad: tensor([-0.1796,  1.0168])\n",
      "Epoch 634, Loss 6.054988\n",
      "Params: tensor(0.3804)\n",
      "Grad: tensor([-0.1793,  1.0151])\n",
      "Epoch 635, Loss 6.044372\n",
      "Params: tensor(0.3803)\n",
      "Grad: tensor([-0.1790,  1.0133])\n",
      "Epoch 636, Loss 6.033794\n",
      "Params: tensor(0.3801)\n",
      "Grad: tensor([-0.1787,  1.0116])\n",
      "Epoch 637, Loss 6.023247\n",
      "Params: tensor(0.3799)\n",
      "Grad: tensor([-0.1784,  1.0099])\n",
      "Epoch 638, Loss 6.012738\n",
      "Params: tensor(0.3797)\n",
      "Grad: tensor([-0.1781,  1.0082])\n",
      "Epoch 639, Loss 6.002264\n",
      "Params: tensor(0.3795)\n",
      "Grad: tensor([-0.1778,  1.0065])\n",
      "Epoch 640, Loss 5.991828\n",
      "Params: tensor(0.3794)\n",
      "Grad: tensor([-0.1775,  1.0048])\n",
      "Epoch 641, Loss 5.981425\n",
      "Params: tensor(0.3792)\n",
      "Grad: tensor([-0.1772,  1.0031])\n",
      "Epoch 642, Loss 5.971058\n",
      "Params: tensor(0.3790)\n",
      "Grad: tensor([-0.1769,  1.0014])\n",
      "Epoch 643, Loss 5.960727\n",
      "Params: tensor(0.3788)\n",
      "Grad: tensor([-0.1766,  0.9997])\n",
      "Epoch 644, Loss 5.950432\n",
      "Params: tensor(0.3786)\n",
      "Grad: tensor([-0.1763,  0.9980])\n",
      "Epoch 645, Loss 5.940171\n",
      "Params: tensor(0.3785)\n",
      "Grad: tensor([-0.1760,  0.9963])\n",
      "Epoch 646, Loss 5.929944\n",
      "Params: tensor(0.3783)\n",
      "Grad: tensor([-0.1757,  0.9946])\n",
      "Epoch 647, Loss 5.919752\n",
      "Params: tensor(0.3781)\n",
      "Grad: tensor([-0.1754,  0.9929])\n",
      "Epoch 648, Loss 5.909596\n",
      "Params: tensor(0.3779)\n",
      "Grad: tensor([-0.1751,  0.9912])\n",
      "Epoch 649, Loss 5.899472\n",
      "Params: tensor(0.3778)\n",
      "Grad: tensor([-0.1748,  0.9895])\n",
      "Epoch 650, Loss 5.889383\n",
      "Params: tensor(0.3776)\n",
      "Grad: tensor([-0.1745,  0.9878])\n",
      "Epoch 651, Loss 5.879326\n",
      "Params: tensor(0.3774)\n",
      "Grad: tensor([-0.1742,  0.9862])\n",
      "Epoch 652, Loss 5.869310\n",
      "Params: tensor(0.3773)\n",
      "Grad: tensor([-0.1739,  0.9845])\n",
      "Epoch 653, Loss 5.859322\n",
      "Params: tensor(0.3771)\n",
      "Grad: tensor([-0.1736,  0.9828])\n",
      "Epoch 654, Loss 5.849374\n",
      "Params: tensor(0.3769)\n",
      "Grad: tensor([-0.1733,  0.9811])\n",
      "Epoch 655, Loss 5.839453\n",
      "Params: tensor(0.3767)\n",
      "Grad: tensor([-0.1730,  0.9795])\n",
      "Epoch 656, Loss 5.829570\n",
      "Params: tensor(0.3766)\n",
      "Grad: tensor([-0.1727,  0.9778])\n",
      "Epoch 657, Loss 5.819718\n",
      "Params: tensor(0.3764)\n",
      "Grad: tensor([-0.1724,  0.9761])\n",
      "Epoch 658, Loss 5.809901\n",
      "Params: tensor(0.3762)\n",
      "Grad: tensor([-0.1722,  0.9745])\n",
      "Epoch 659, Loss 5.800116\n",
      "Params: tensor(0.3761)\n",
      "Grad: tensor([-0.1719,  0.9728])\n",
      "Epoch 660, Loss 5.790367\n",
      "Params: tensor(0.3759)\n",
      "Grad: tensor([-0.1716,  0.9712])\n",
      "Epoch 661, Loss 5.780646\n",
      "Params: tensor(0.3757)\n",
      "Grad: tensor([-0.1713,  0.9695])\n",
      "Epoch 662, Loss 5.770962\n",
      "Params: tensor(0.3756)\n",
      "Grad: tensor([-0.1710,  0.9679])\n",
      "Epoch 663, Loss 5.761312\n",
      "Params: tensor(0.3754)\n",
      "Grad: tensor([-0.1707,  0.9662])\n",
      "Epoch 664, Loss 5.751694\n",
      "Params: tensor(0.3752)\n",
      "Grad: tensor([-0.1704,  0.9646])\n",
      "Epoch 665, Loss 5.742105\n",
      "Params: tensor(0.3751)\n",
      "Grad: tensor([-0.1701,  0.9630])\n",
      "Epoch 666, Loss 5.732550\n",
      "Params: tensor(0.3749)\n",
      "Grad: tensor([-0.1698,  0.9613])\n",
      "Epoch 667, Loss 5.723031\n",
      "Params: tensor(0.3747)\n",
      "Grad: tensor([-0.1695,  0.9597])\n",
      "Epoch 668, Loss 5.713540\n",
      "Params: tensor(0.3746)\n",
      "Grad: tensor([-0.1692,  0.9581])\n",
      "Epoch 669, Loss 5.704083\n",
      "Params: tensor(0.3744)\n",
      "Grad: tensor([-0.1690,  0.9564])\n",
      "Epoch 670, Loss 5.694659\n",
      "Params: tensor(0.3743)\n",
      "Grad: tensor([-0.1687,  0.9548])\n",
      "Epoch 671, Loss 5.685266\n",
      "Params: tensor(0.3741)\n",
      "Grad: tensor([-0.1684,  0.9532])\n",
      "Epoch 672, Loss 5.675904\n",
      "Params: tensor(0.3739)\n",
      "Grad: tensor([-0.1681,  0.9516])\n",
      "Epoch 673, Loss 5.666573\n",
      "Params: tensor(0.3738)\n",
      "Grad: tensor([-0.1678,  0.9499])\n",
      "Epoch 674, Loss 5.657277\n",
      "Params: tensor(0.3736)\n",
      "Grad: tensor([-0.1675,  0.9483])\n",
      "Epoch 675, Loss 5.648010\n",
      "Params: tensor(0.3735)\n",
      "Grad: tensor([-0.1673,  0.9467])\n",
      "Epoch 676, Loss 5.638776\n",
      "Params: tensor(0.3733)\n",
      "Grad: tensor([-0.1670,  0.9451])\n",
      "Epoch 677, Loss 5.629574\n",
      "Params: tensor(0.3731)\n",
      "Grad: tensor([-0.1667,  0.9435])\n",
      "Epoch 678, Loss 5.620402\n",
      "Params: tensor(0.3730)\n",
      "Grad: tensor([-0.1664,  0.9419])\n",
      "Epoch 679, Loss 5.611260\n",
      "Params: tensor(0.3728)\n",
      "Grad: tensor([-0.1661,  0.9403])\n",
      "Epoch 680, Loss 5.602149\n",
      "Params: tensor(0.3727)\n",
      "Grad: tensor([-0.1658,  0.9387])\n",
      "Epoch 681, Loss 5.593071\n",
      "Params: tensor(0.3725)\n",
      "Grad: tensor([-0.1656,  0.9371])\n",
      "Epoch 682, Loss 5.584022\n",
      "Params: tensor(0.3724)\n",
      "Grad: tensor([-0.1653,  0.9355])\n",
      "Epoch 683, Loss 5.575005\n",
      "Params: tensor(0.3722)\n",
      "Grad: tensor([-0.1650,  0.9339])\n",
      "Epoch 684, Loss 5.566019\n",
      "Params: tensor(0.3721)\n",
      "Grad: tensor([-0.1647,  0.9323])\n",
      "Epoch 685, Loss 5.557063\n",
      "Params: tensor(0.3719)\n",
      "Grad: tensor([-0.1644,  0.9308])\n",
      "Epoch 686, Loss 5.548136\n",
      "Params: tensor(0.3717)\n",
      "Grad: tensor([-0.1641,  0.9292])\n",
      "Epoch 687, Loss 5.539241\n",
      "Params: tensor(0.3716)\n",
      "Grad: tensor([-0.1639,  0.9276])\n",
      "Epoch 688, Loss 5.530376\n",
      "Params: tensor(0.3714)\n",
      "Grad: tensor([-0.1636,  0.9260])\n",
      "Epoch 689, Loss 5.521540\n",
      "Params: tensor(0.3713)\n",
      "Grad: tensor([-0.1633,  0.9245])\n",
      "Epoch 690, Loss 5.512734\n",
      "Params: tensor(0.3711)\n",
      "Grad: tensor([-0.1630,  0.9229])\n",
      "Epoch 691, Loss 5.503958\n",
      "Params: tensor(0.3710)\n",
      "Grad: tensor([-0.1628,  0.9213])\n",
      "Epoch 692, Loss 5.495212\n",
      "Params: tensor(0.3708)\n",
      "Grad: tensor([-0.1625,  0.9197])\n",
      "Epoch 693, Loss 5.486496\n",
      "Params: tensor(0.3707)\n",
      "Grad: tensor([-0.1622,  0.9182])\n",
      "Epoch 694, Loss 5.477808\n",
      "Params: tensor(0.3705)\n",
      "Grad: tensor([-0.1619,  0.9166])\n",
      "Epoch 695, Loss 5.469152\n",
      "Params: tensor(0.3704)\n",
      "Grad: tensor([-0.1617,  0.9151])\n",
      "Epoch 696, Loss 5.460525\n",
      "Params: tensor(0.3702)\n",
      "Grad: tensor([-0.1614,  0.9135])\n",
      "Epoch 697, Loss 5.451928\n",
      "Params: tensor(0.3701)\n",
      "Grad: tensor([-0.1611,  0.9120])\n",
      "Epoch 698, Loss 5.443358\n",
      "Params: tensor(0.3699)\n",
      "Grad: tensor([-0.1608,  0.9104])\n",
      "Epoch 699, Loss 5.434819\n",
      "Params: tensor(0.3698)\n",
      "Grad: tensor([-0.1605,  0.9089])\n",
      "Epoch 700, Loss 5.426309\n",
      "Params: tensor(0.3697)\n",
      "Grad: tensor([-0.1603,  0.9073])\n",
      "Epoch 701, Loss 5.417827\n",
      "Params: tensor(0.3695)\n",
      "Grad: tensor([-0.1600,  0.9058])\n",
      "Epoch 702, Loss 5.409372\n",
      "Params: tensor(0.3694)\n",
      "Grad: tensor([-0.1597,  0.9042])\n",
      "Epoch 703, Loss 5.400949\n",
      "Params: tensor(0.3692)\n",
      "Grad: tensor([-0.1595,  0.9027])\n",
      "Epoch 704, Loss 5.392550\n",
      "Params: tensor(0.3691)\n",
      "Grad: tensor([-0.1592,  0.9012])\n",
      "Epoch 705, Loss 5.384184\n",
      "Params: tensor(0.3689)\n",
      "Grad: tensor([-0.1589,  0.8996])\n",
      "Epoch 706, Loss 5.375846\n",
      "Params: tensor(0.3688)\n",
      "Grad: tensor([-0.1586,  0.8981])\n",
      "Epoch 707, Loss 5.367537\n",
      "Params: tensor(0.3686)\n",
      "Grad: tensor([-0.1584,  0.8966])\n",
      "Epoch 708, Loss 5.359253\n",
      "Params: tensor(0.3685)\n",
      "Grad: tensor([-0.1581,  0.8951])\n",
      "Epoch 709, Loss 5.350999\n",
      "Params: tensor(0.3684)\n",
      "Grad: tensor([-0.1578,  0.8935])\n",
      "Epoch 710, Loss 5.342772\n",
      "Params: tensor(0.3682)\n",
      "Grad: tensor([-0.1576,  0.8920])\n",
      "Epoch 711, Loss 5.334575\n",
      "Params: tensor(0.3681)\n",
      "Grad: tensor([-0.1573,  0.8905])\n",
      "Epoch 712, Loss 5.326402\n",
      "Params: tensor(0.3679)\n",
      "Grad: tensor([-0.1570,  0.8890])\n",
      "Epoch 713, Loss 5.318260\n",
      "Params: tensor(0.3678)\n",
      "Grad: tensor([-0.1568,  0.8875])\n",
      "Epoch 714, Loss 5.310144\n",
      "Params: tensor(0.3677)\n",
      "Grad: tensor([-0.1565,  0.8860])\n",
      "Epoch 715, Loss 5.302055\n",
      "Params: tensor(0.3675)\n",
      "Grad: tensor([-0.1562,  0.8845])\n",
      "Epoch 716, Loss 5.293994\n",
      "Params: tensor(0.3674)\n",
      "Grad: tensor([-0.1560,  0.8830])\n",
      "Epoch 717, Loss 5.285964\n",
      "Params: tensor(0.3672)\n",
      "Grad: tensor([-0.1557,  0.8815])\n",
      "Epoch 718, Loss 5.277958\n",
      "Params: tensor(0.3671)\n",
      "Grad: tensor([-0.1555,  0.8800])\n",
      "Epoch 719, Loss 5.269979\n",
      "Params: tensor(0.3670)\n",
      "Grad: tensor([-0.1552,  0.8785])\n",
      "Epoch 720, Loss 5.262027\n",
      "Params: tensor(0.3668)\n",
      "Grad: tensor([-0.1549,  0.8770])\n",
      "Epoch 721, Loss 5.254103\n",
      "Params: tensor(0.3667)\n",
      "Grad: tensor([-0.1547,  0.8755])\n",
      "Epoch 722, Loss 5.246205\n",
      "Params: tensor(0.3666)\n",
      "Grad: tensor([-0.1544,  0.8740])\n",
      "Epoch 723, Loss 5.238335\n",
      "Params: tensor(0.3664)\n",
      "Grad: tensor([-0.1541,  0.8725])\n",
      "Epoch 724, Loss 5.230492\n",
      "Params: tensor(0.3663)\n",
      "Grad: tensor([-0.1539,  0.8710])\n",
      "Epoch 725, Loss 5.222674\n",
      "Params: tensor(0.3661)\n",
      "Grad: tensor([-0.1536,  0.8696])\n",
      "Epoch 726, Loss 5.214881\n",
      "Params: tensor(0.3660)\n",
      "Grad: tensor([-0.1533,  0.8681])\n",
      "Epoch 727, Loss 5.207120\n",
      "Params: tensor(0.3659)\n",
      "Grad: tensor([-0.1531,  0.8666])\n",
      "Epoch 728, Loss 5.199381\n",
      "Params: tensor(0.3657)\n",
      "Grad: tensor([-0.1528,  0.8651])\n",
      "Epoch 729, Loss 5.191670\n",
      "Params: tensor(0.3656)\n",
      "Grad: tensor([-0.1526,  0.8637])\n",
      "Epoch 730, Loss 5.183985\n",
      "Params: tensor(0.3655)\n",
      "Grad: tensor([-0.1523,  0.8622])\n",
      "Epoch 731, Loss 5.176324\n",
      "Params: tensor(0.3653)\n",
      "Grad: tensor([-0.1520,  0.8607])\n",
      "Epoch 732, Loss 5.168688\n",
      "Params: tensor(0.3652)\n",
      "Grad: tensor([-0.1518,  0.8593])\n",
      "Epoch 733, Loss 5.161084\n",
      "Params: tensor(0.3651)\n",
      "Grad: tensor([-0.1515,  0.8578])\n",
      "Epoch 734, Loss 5.153500\n",
      "Params: tensor(0.3649)\n",
      "Grad: tensor([-0.1513,  0.8564])\n",
      "Epoch 735, Loss 5.145944\n",
      "Params: tensor(0.3648)\n",
      "Grad: tensor([-0.1510,  0.8549])\n",
      "Epoch 736, Loss 5.138413\n",
      "Params: tensor(0.3647)\n",
      "Grad: tensor([-0.1508,  0.8535])\n",
      "Epoch 737, Loss 5.130910\n",
      "Params: tensor(0.3646)\n",
      "Grad: tensor([-0.1505,  0.8520])\n",
      "Epoch 738, Loss 5.123428\n",
      "Params: tensor(0.3644)\n",
      "Grad: tensor([-0.1502,  0.8506])\n",
      "Epoch 739, Loss 5.115978\n",
      "Params: tensor(0.3643)\n",
      "Grad: tensor([-0.1500,  0.8491])\n",
      "Epoch 740, Loss 5.108547\n",
      "Params: tensor(0.3642)\n",
      "Grad: tensor([-0.1497,  0.8477])\n",
      "Epoch 741, Loss 5.101143\n",
      "Params: tensor(0.3640)\n",
      "Grad: tensor([-0.1495,  0.8462])\n",
      "Epoch 742, Loss 5.093765\n",
      "Params: tensor(0.3639)\n",
      "Grad: tensor([-0.1492,  0.8448])\n",
      "Epoch 743, Loss 5.086414\n",
      "Params: tensor(0.3638)\n",
      "Grad: tensor([-0.1490,  0.8434])\n",
      "Epoch 744, Loss 5.079086\n",
      "Params: tensor(0.3637)\n",
      "Grad: tensor([-0.1487,  0.8419])\n",
      "Epoch 745, Loss 5.071781\n",
      "Params: tensor(0.3635)\n",
      "Grad: tensor([-0.1485,  0.8405])\n",
      "Epoch 746, Loss 5.064505\n",
      "Params: tensor(0.3634)\n",
      "Grad: tensor([-0.1482,  0.8391])\n",
      "Epoch 747, Loss 5.057247\n",
      "Params: tensor(0.3633)\n",
      "Grad: tensor([-0.1480,  0.8376])\n",
      "Epoch 748, Loss 5.050021\n",
      "Params: tensor(0.3631)\n",
      "Grad: tensor([-0.1477,  0.8362])\n",
      "Epoch 749, Loss 5.042817\n",
      "Params: tensor(0.3630)\n",
      "Grad: tensor([-0.1475,  0.8348])\n",
      "Epoch 750, Loss 5.035636\n",
      "Params: tensor(0.3629)\n",
      "Grad: tensor([-0.1472,  0.8334])\n",
      "Epoch 751, Loss 5.028476\n",
      "Params: tensor(0.3628)\n",
      "Grad: tensor([-0.1470,  0.8320])\n",
      "Epoch 752, Loss 5.021346\n",
      "Params: tensor(0.3626)\n",
      "Grad: tensor([-0.1467,  0.8305])\n",
      "Epoch 753, Loss 5.014240\n",
      "Params: tensor(0.3625)\n",
      "Grad: tensor([-0.1465,  0.8291])\n",
      "Epoch 754, Loss 5.007157\n",
      "Params: tensor(0.3624)\n",
      "Grad: tensor([-0.1462,  0.8277])\n",
      "Epoch 755, Loss 5.000099\n",
      "Params: tensor(0.3623)\n",
      "Grad: tensor([-0.1460,  0.8263])\n",
      "Epoch 756, Loss 4.993064\n",
      "Params: tensor(0.3622)\n",
      "Grad: tensor([-0.1457,  0.8249])\n",
      "Epoch 757, Loss 4.986051\n",
      "Params: tensor(0.3620)\n",
      "Grad: tensor([-0.1455,  0.8235])\n",
      "Epoch 758, Loss 4.979064\n",
      "Params: tensor(0.3619)\n",
      "Grad: tensor([-0.1452,  0.8221])\n",
      "Epoch 759, Loss 4.972100\n",
      "Params: tensor(0.3618)\n",
      "Grad: tensor([-0.1450,  0.8207])\n",
      "Epoch 760, Loss 4.965159\n",
      "Params: tensor(0.3617)\n",
      "Grad: tensor([-0.1447,  0.8193])\n",
      "Epoch 761, Loss 4.958245\n",
      "Params: tensor(0.3615)\n",
      "Grad: tensor([-0.1445,  0.8179])\n",
      "Epoch 762, Loss 4.951351\n",
      "Params: tensor(0.3614)\n",
      "Grad: tensor([-0.1443,  0.8165])\n",
      "Epoch 763, Loss 4.944479\n",
      "Params: tensor(0.3613)\n",
      "Grad: tensor([-0.1440,  0.8152])\n",
      "Epoch 764, Loss 4.937633\n",
      "Params: tensor(0.3612)\n",
      "Grad: tensor([-0.1438,  0.8138])\n",
      "Epoch 765, Loss 4.930812\n",
      "Params: tensor(0.3611)\n",
      "Grad: tensor([-0.1435,  0.8124])\n",
      "Epoch 766, Loss 4.924009\n",
      "Params: tensor(0.3609)\n",
      "Grad: tensor([-0.1433,  0.8110])\n",
      "Epoch 767, Loss 4.917234\n",
      "Params: tensor(0.3608)\n",
      "Grad: tensor([-0.1430,  0.8096])\n",
      "Epoch 768, Loss 4.910480\n",
      "Params: tensor(0.3607)\n",
      "Grad: tensor([-0.1428,  0.8083])\n",
      "Epoch 769, Loss 4.903749\n",
      "Params: tensor(0.3606)\n",
      "Grad: tensor([-0.1426,  0.8069])\n",
      "Epoch 770, Loss 4.897040\n",
      "Params: tensor(0.3605)\n",
      "Grad: tensor([-0.1423,  0.8055])\n",
      "Epoch 771, Loss 4.890356\n",
      "Params: tensor(0.3604)\n",
      "Grad: tensor([-0.1420,  0.8042])\n",
      "Epoch 772, Loss 4.883692\n",
      "Params: tensor(0.3602)\n",
      "Grad: tensor([-0.1418,  0.8028])\n",
      "Epoch 773, Loss 4.877052\n",
      "Params: tensor(0.3601)\n",
      "Grad: tensor([-0.1416,  0.8014])\n",
      "Epoch 774, Loss 4.870436\n",
      "Params: tensor(0.3600)\n",
      "Grad: tensor([-0.1413,  0.8001])\n",
      "Epoch 775, Loss 4.863839\n",
      "Params: tensor(0.3599)\n",
      "Grad: tensor([-0.1411,  0.7987])\n",
      "Epoch 776, Loss 4.857268\n",
      "Params: tensor(0.3598)\n",
      "Grad: tensor([-0.1408,  0.7973])\n",
      "Epoch 777, Loss 4.850718\n",
      "Params: tensor(0.3597)\n",
      "Grad: tensor([-0.1406,  0.7960])\n",
      "Epoch 778, Loss 4.844189\n",
      "Params: tensor(0.3595)\n",
      "Grad: tensor([-0.1404,  0.7946])\n",
      "Epoch 779, Loss 4.837683\n",
      "Params: tensor(0.3594)\n",
      "Grad: tensor([-0.1401,  0.7933])\n",
      "Epoch 780, Loss 4.831196\n",
      "Params: tensor(0.3593)\n",
      "Grad: tensor([-0.1399,  0.7919])\n",
      "Epoch 781, Loss 4.824737\n",
      "Params: tensor(0.3592)\n",
      "Grad: tensor([-0.1397,  0.7906])\n",
      "Epoch 782, Loss 4.818298\n",
      "Params: tensor(0.3591)\n",
      "Grad: tensor([-0.1394,  0.7893])\n",
      "Epoch 783, Loss 4.811879\n",
      "Params: tensor(0.3590)\n",
      "Grad: tensor([-0.1392,  0.7879])\n",
      "Epoch 784, Loss 4.805481\n",
      "Params: tensor(0.3589)\n",
      "Grad: tensor([-0.1389,  0.7866])\n",
      "Epoch 785, Loss 4.799106\n",
      "Params: tensor(0.3587)\n",
      "Grad: tensor([-0.1387,  0.7852])\n",
      "Epoch 786, Loss 4.792755\n",
      "Params: tensor(0.3586)\n",
      "Grad: tensor([-0.1385,  0.7839])\n",
      "Epoch 787, Loss 4.786422\n",
      "Params: tensor(0.3585)\n",
      "Grad: tensor([-0.1383,  0.7826])\n",
      "Epoch 788, Loss 4.780112\n",
      "Params: tensor(0.3584)\n",
      "Grad: tensor([-0.1380,  0.7812])\n",
      "Epoch 789, Loss 4.773824\n",
      "Params: tensor(0.3583)\n",
      "Grad: tensor([-0.1378,  0.7799])\n",
      "Epoch 790, Loss 4.767558\n",
      "Params: tensor(0.3582)\n",
      "Grad: tensor([-0.1375,  0.7786])\n",
      "Epoch 791, Loss 4.761312\n",
      "Params: tensor(0.3581)\n",
      "Grad: tensor([-0.1373,  0.7773])\n",
      "Epoch 792, Loss 4.755087\n",
      "Params: tensor(0.3580)\n",
      "Grad: tensor([-0.1371,  0.7759])\n",
      "Epoch 793, Loss 4.748885\n",
      "Params: tensor(0.3579)\n",
      "Grad: tensor([-0.1368,  0.7746])\n",
      "Epoch 794, Loss 4.742700\n",
      "Params: tensor(0.3577)\n",
      "Grad: tensor([-0.1366,  0.7733])\n",
      "Epoch 795, Loss 4.736537\n",
      "Params: tensor(0.3576)\n",
      "Grad: tensor([-0.1364,  0.7720])\n",
      "Epoch 796, Loss 4.730397\n",
      "Params: tensor(0.3575)\n",
      "Grad: tensor([-0.1361,  0.7707])\n",
      "Epoch 797, Loss 4.724279\n",
      "Params: tensor(0.3574)\n",
      "Grad: tensor([-0.1359,  0.7694])\n",
      "Epoch 798, Loss 4.718181\n",
      "Params: tensor(0.3573)\n",
      "Grad: tensor([-0.1357,  0.7681])\n",
      "Epoch 799, Loss 4.712101\n",
      "Params: tensor(0.3572)\n",
      "Grad: tensor([-0.1354,  0.7668])\n",
      "Epoch 800, Loss 4.706046\n",
      "Params: tensor(0.3571)\n",
      "Grad: tensor([-0.1352,  0.7655])\n",
      "Epoch 801, Loss 4.700009\n",
      "Params: tensor(0.3570)\n",
      "Grad: tensor([-0.1350,  0.7642])\n",
      "Epoch 802, Loss 4.693990\n",
      "Params: tensor(0.3569)\n",
      "Grad: tensor([-0.1347,  0.7629])\n",
      "Epoch 803, Loss 4.687995\n",
      "Params: tensor(0.3568)\n",
      "Grad: tensor([-0.1345,  0.7616])\n",
      "Epoch 804, Loss 4.682020\n",
      "Params: tensor(0.3567)\n",
      "Grad: tensor([-0.1343,  0.7603])\n",
      "Epoch 805, Loss 4.676063\n",
      "Params: tensor(0.3566)\n",
      "Grad: tensor([-0.1341,  0.7590])\n",
      "Epoch 806, Loss 4.670130\n",
      "Params: tensor(0.3565)\n",
      "Grad: tensor([-0.1338,  0.7577])\n",
      "Epoch 807, Loss 4.664214\n",
      "Params: tensor(0.3563)\n",
      "Grad: tensor([-0.1336,  0.7564])\n",
      "Epoch 808, Loss 4.658319\n",
      "Params: tensor(0.3562)\n",
      "Grad: tensor([-0.1334,  0.7551])\n",
      "Epoch 809, Loss 4.652445\n",
      "Params: tensor(0.3561)\n",
      "Grad: tensor([-0.1332,  0.7538])\n",
      "Epoch 810, Loss 4.646592\n",
      "Params: tensor(0.3560)\n",
      "Grad: tensor([-0.1330,  0.7526])\n",
      "Epoch 811, Loss 4.640754\n",
      "Params: tensor(0.3559)\n",
      "Grad: tensor([-0.1327,  0.7513])\n",
      "Epoch 812, Loss 4.634938\n",
      "Params: tensor(0.3558)\n",
      "Grad: tensor([-0.1325,  0.7500])\n",
      "Epoch 813, Loss 4.629142\n",
      "Params: tensor(0.3557)\n",
      "Grad: tensor([-0.1323,  0.7487])\n",
      "Epoch 814, Loss 4.623367\n",
      "Params: tensor(0.3556)\n",
      "Grad: tensor([-0.1320,  0.7475])\n",
      "Epoch 815, Loss 4.617611\n",
      "Params: tensor(0.3555)\n",
      "Grad: tensor([-0.1318,  0.7462])\n",
      "Epoch 816, Loss 4.611872\n",
      "Params: tensor(0.3554)\n",
      "Grad: tensor([-0.1316,  0.7449])\n",
      "Epoch 817, Loss 4.606156\n",
      "Params: tensor(0.3553)\n",
      "Grad: tensor([-0.1314,  0.7437])\n",
      "Epoch 818, Loss 4.600458\n",
      "Params: tensor(0.3552)\n",
      "Grad: tensor([-0.1311,  0.7424])\n",
      "Epoch 819, Loss 4.594780\n",
      "Params: tensor(0.3551)\n",
      "Grad: tensor([-0.1309,  0.7411])\n",
      "Epoch 820, Loss 4.589119\n",
      "Params: tensor(0.3550)\n",
      "Grad: tensor([-0.1307,  0.7399])\n",
      "Epoch 821, Loss 4.583479\n",
      "Params: tensor(0.3549)\n",
      "Grad: tensor([-0.1305,  0.7386])\n",
      "Epoch 822, Loss 4.577857\n",
      "Params: tensor(0.3548)\n",
      "Grad: tensor([-0.1303,  0.7374])\n",
      "Epoch 823, Loss 4.572256\n",
      "Params: tensor(0.3547)\n",
      "Grad: tensor([-0.1300,  0.7361])\n",
      "Epoch 824, Loss 4.566675\n",
      "Params: tensor(0.3546)\n",
      "Grad: tensor([-0.1298,  0.7349])\n",
      "Epoch 825, Loss 4.561108\n",
      "Params: tensor(0.3545)\n",
      "Grad: tensor([-0.1296,  0.7336])\n",
      "Epoch 826, Loss 4.555565\n",
      "Params: tensor(0.3544)\n",
      "Grad: tensor([-0.1294,  0.7324])\n",
      "Epoch 827, Loss 4.550039\n",
      "Params: tensor(0.3543)\n",
      "Grad: tensor([-0.1292,  0.7311])\n",
      "Epoch 828, Loss 4.544534\n",
      "Params: tensor(0.3542)\n",
      "Grad: tensor([-0.1289,  0.7299])\n",
      "Epoch 829, Loss 4.539044\n",
      "Params: tensor(0.3541)\n",
      "Grad: tensor([-0.1287,  0.7286])\n",
      "Epoch 830, Loss 4.533575\n",
      "Params: tensor(0.3540)\n",
      "Grad: tensor([-0.1285,  0.7274])\n",
      "Epoch 831, Loss 4.528122\n",
      "Params: tensor(0.3539)\n",
      "Grad: tensor([-0.1283,  0.7262])\n",
      "Epoch 832, Loss 4.522691\n",
      "Params: tensor(0.3538)\n",
      "Grad: tensor([-0.1280,  0.7249])\n",
      "Epoch 833, Loss 4.517276\n",
      "Params: tensor(0.3537)\n",
      "Grad: tensor([-0.1278,  0.7237])\n",
      "Epoch 834, Loss 4.511879\n",
      "Params: tensor(0.3536)\n",
      "Grad: tensor([-0.1276,  0.7225])\n",
      "Epoch 835, Loss 4.506505\n",
      "Params: tensor(0.3535)\n",
      "Grad: tensor([-0.1274,  0.7212])\n",
      "Epoch 836, Loss 4.501141\n",
      "Params: tensor(0.3534)\n",
      "Grad: tensor([-0.1272,  0.7200])\n",
      "Epoch 837, Loss 4.495801\n",
      "Params: tensor(0.3533)\n",
      "Grad: tensor([-0.1270,  0.7188])\n",
      "Epoch 838, Loss 4.490475\n",
      "Params: tensor(0.3532)\n",
      "Grad: tensor([-0.1268,  0.7176])\n",
      "Epoch 839, Loss 4.485169\n",
      "Params: tensor(0.3531)\n",
      "Grad: tensor([-0.1266,  0.7163])\n",
      "Epoch 840, Loss 4.479884\n",
      "Params: tensor(0.3530)\n",
      "Grad: tensor([-0.1263,  0.7151])\n",
      "Epoch 841, Loss 4.474613\n",
      "Params: tensor(0.3529)\n",
      "Grad: tensor([-0.1261,  0.7139])\n",
      "Epoch 842, Loss 4.469364\n",
      "Params: tensor(0.3528)\n",
      "Grad: tensor([-0.1259,  0.7127])\n",
      "Epoch 843, Loss 4.464130\n",
      "Params: tensor(0.3527)\n",
      "Grad: tensor([-0.1257,  0.7115])\n",
      "Epoch 844, Loss 4.458913\n",
      "Params: tensor(0.3526)\n",
      "Grad: tensor([-0.1255,  0.7103])\n",
      "Epoch 845, Loss 4.453716\n",
      "Params: tensor(0.3525)\n",
      "Grad: tensor([-0.1253,  0.7091])\n",
      "Epoch 846, Loss 4.448535\n",
      "Params: tensor(0.3524)\n",
      "Grad: tensor([-0.1250,  0.7079])\n",
      "Epoch 847, Loss 4.443372\n",
      "Params: tensor(0.3524)\n",
      "Grad: tensor([-0.1249,  0.7067])\n",
      "Epoch 848, Loss 4.438226\n",
      "Params: tensor(0.3523)\n",
      "Grad: tensor([-0.1246,  0.7055])\n",
      "Epoch 849, Loss 4.433099\n",
      "Params: tensor(0.3522)\n",
      "Grad: tensor([-0.1244,  0.7043])\n",
      "Epoch 850, Loss 4.427990\n",
      "Params: tensor(0.3521)\n",
      "Grad: tensor([-0.1242,  0.7031])\n",
      "Epoch 851, Loss 4.422897\n",
      "Params: tensor(0.3520)\n",
      "Grad: tensor([-0.1240,  0.7019])\n",
      "Epoch 852, Loss 4.417819\n",
      "Params: tensor(0.3519)\n",
      "Grad: tensor([-0.1238,  0.7007])\n",
      "Epoch 853, Loss 4.412762\n",
      "Params: tensor(0.3518)\n",
      "Grad: tensor([-0.1236,  0.6995])\n",
      "Epoch 854, Loss 4.407721\n",
      "Params: tensor(0.3517)\n",
      "Grad: tensor([-0.1234,  0.6983])\n",
      "Epoch 855, Loss 4.402698\n",
      "Params: tensor(0.3516)\n",
      "Grad: tensor([-0.1232,  0.6971])\n",
      "Epoch 856, Loss 4.397688\n",
      "Params: tensor(0.3515)\n",
      "Grad: tensor([-0.1229,  0.6959])\n",
      "Epoch 857, Loss 4.392697\n",
      "Params: tensor(0.3514)\n",
      "Grad: tensor([-0.1227,  0.6948])\n",
      "Epoch 858, Loss 4.387725\n",
      "Params: tensor(0.3513)\n",
      "Grad: tensor([-0.1225,  0.6936])\n",
      "Epoch 859, Loss 4.382770\n",
      "Params: tensor(0.3512)\n",
      "Grad: tensor([-0.1223,  0.6924])\n",
      "Epoch 860, Loss 4.377828\n",
      "Params: tensor(0.3511)\n",
      "Grad: tensor([-0.1221,  0.6912])\n",
      "Epoch 861, Loss 4.372905\n",
      "Params: tensor(0.3511)\n",
      "Grad: tensor([-0.1219,  0.6901])\n",
      "Epoch 862, Loss 4.368000\n",
      "Params: tensor(0.3510)\n",
      "Grad: tensor([-0.1217,  0.6889])\n",
      "Epoch 863, Loss 4.363111\n",
      "Params: tensor(0.3509)\n",
      "Grad: tensor([-0.1215,  0.6877])\n",
      "Epoch 864, Loss 4.358238\n",
      "Params: tensor(0.3508)\n",
      "Grad: tensor([-0.1213,  0.6865])\n",
      "Epoch 865, Loss 4.353383\n",
      "Params: tensor(0.3507)\n",
      "Grad: tensor([-0.1211,  0.6854])\n",
      "Epoch 866, Loss 4.348542\n",
      "Params: tensor(0.3506)\n",
      "Grad: tensor([-0.1209,  0.6842])\n",
      "Epoch 867, Loss 4.343716\n",
      "Params: tensor(0.3505)\n",
      "Grad: tensor([-0.1207,  0.6830])\n",
      "Epoch 868, Loss 4.338911\n",
      "Params: tensor(0.3504)\n",
      "Grad: tensor([-0.1205,  0.6819])\n",
      "Epoch 869, Loss 4.334120\n",
      "Params: tensor(0.3503)\n",
      "Grad: tensor([-0.1203,  0.6807])\n",
      "Epoch 870, Loss 4.329345\n",
      "Params: tensor(0.3502)\n",
      "Grad: tensor([-0.1201,  0.6796])\n",
      "Epoch 871, Loss 4.324588\n",
      "Params: tensor(0.3502)\n",
      "Grad: tensor([-0.1198,  0.6784])\n",
      "Epoch 872, Loss 4.319846\n",
      "Params: tensor(0.3501)\n",
      "Grad: tensor([-0.1196,  0.6773])\n",
      "Epoch 873, Loss 4.315117\n",
      "Params: tensor(0.3500)\n",
      "Grad: tensor([-0.1195,  0.6761])\n",
      "Epoch 874, Loss 4.310409\n",
      "Params: tensor(0.3499)\n",
      "Grad: tensor([-0.1192,  0.6750])\n",
      "Epoch 875, Loss 4.305714\n",
      "Params: tensor(0.3498)\n",
      "Grad: tensor([-0.1190,  0.6738])\n",
      "Epoch 876, Loss 4.301036\n",
      "Params: tensor(0.3497)\n",
      "Grad: tensor([-0.1188,  0.6727])\n",
      "Epoch 877, Loss 4.296376\n",
      "Params: tensor(0.3496)\n",
      "Grad: tensor([-0.1186,  0.6715])\n",
      "Epoch 878, Loss 4.291727\n",
      "Params: tensor(0.3495)\n",
      "Grad: tensor([-0.1184,  0.6704])\n",
      "Epoch 879, Loss 4.287098\n",
      "Params: tensor(0.3495)\n",
      "Grad: tensor([-0.1182,  0.6693])\n",
      "Epoch 880, Loss 4.282482\n",
      "Params: tensor(0.3494)\n",
      "Grad: tensor([-0.1180,  0.6681])\n",
      "Epoch 881, Loss 4.277882\n",
      "Params: tensor(0.3493)\n",
      "Grad: tensor([-0.1178,  0.6670])\n",
      "Epoch 882, Loss 4.273299\n",
      "Params: tensor(0.3492)\n",
      "Grad: tensor([-0.1176,  0.6658])\n",
      "Epoch 883, Loss 4.268732\n",
      "Params: tensor(0.3491)\n",
      "Grad: tensor([-0.1174,  0.6647])\n",
      "Epoch 884, Loss 4.264178\n",
      "Params: tensor(0.3490)\n",
      "Grad: tensor([-0.1172,  0.6636])\n",
      "Epoch 885, Loss 4.259643\n",
      "Params: tensor(0.3489)\n",
      "Grad: tensor([-0.1170,  0.6625])\n",
      "Epoch 886, Loss 4.255120\n",
      "Params: tensor(0.3489)\n",
      "Grad: tensor([-0.1168,  0.6613])\n",
      "Epoch 887, Loss 4.250614\n",
      "Params: tensor(0.3488)\n",
      "Grad: tensor([-0.1166,  0.6602])\n",
      "Epoch 888, Loss 4.246124\n",
      "Params: tensor(0.3487)\n",
      "Grad: tensor([-0.1164,  0.6591])\n",
      "Epoch 889, Loss 4.241648\n",
      "Params: tensor(0.3486)\n",
      "Grad: tensor([-0.1162,  0.6580])\n",
      "Epoch 890, Loss 4.237185\n",
      "Params: tensor(0.3485)\n",
      "Grad: tensor([-0.1160,  0.6569])\n",
      "Epoch 891, Loss 4.232740\n",
      "Params: tensor(0.3484)\n",
      "Grad: tensor([-0.1158,  0.6557])\n",
      "Epoch 892, Loss 4.228308\n",
      "Params: tensor(0.3484)\n",
      "Grad: tensor([-0.1157,  0.6546])\n",
      "Epoch 893, Loss 4.223895\n",
      "Params: tensor(0.3483)\n",
      "Grad: tensor([-0.1154,  0.6535])\n",
      "Epoch 894, Loss 4.219494\n",
      "Params: tensor(0.3482)\n",
      "Grad: tensor([-0.1153,  0.6524])\n",
      "Epoch 895, Loss 4.215109\n",
      "Params: tensor(0.3481)\n",
      "Grad: tensor([-0.1151,  0.6513])\n",
      "Epoch 896, Loss 4.210737\n",
      "Params: tensor(0.3480)\n",
      "Grad: tensor([-0.1148,  0.6502])\n",
      "Epoch 897, Loss 4.206383\n",
      "Params: tensor(0.3479)\n",
      "Grad: tensor([-0.1147,  0.6491])\n",
      "Epoch 898, Loss 4.202043\n",
      "Params: tensor(0.3479)\n",
      "Grad: tensor([-0.1145,  0.6480])\n",
      "Epoch 899, Loss 4.197715\n",
      "Params: tensor(0.3478)\n",
      "Grad: tensor([-0.1143,  0.6469])\n",
      "Epoch 900, Loss 4.193405\n",
      "Params: tensor(0.3477)\n",
      "Grad: tensor([-0.1141,  0.6458])\n",
      "Epoch 901, Loss 4.189108\n",
      "Params: tensor(0.3476)\n",
      "Grad: tensor([-0.1139,  0.6447])\n",
      "Epoch 902, Loss 4.184825\n",
      "Params: tensor(0.3475)\n",
      "Grad: tensor([-0.1137,  0.6436])\n",
      "Epoch 903, Loss 4.180559\n",
      "Params: tensor(0.3475)\n",
      "Grad: tensor([-0.1135,  0.6425])\n",
      "Epoch 904, Loss 4.176305\n",
      "Params: tensor(0.3474)\n",
      "Grad: tensor([-0.1133,  0.6414])\n",
      "Epoch 905, Loss 4.172065\n",
      "Params: tensor(0.3473)\n",
      "Grad: tensor([-0.1131,  0.6403])\n",
      "Epoch 906, Loss 4.167842\n",
      "Params: tensor(0.3472)\n",
      "Grad: tensor([-0.1129,  0.6392])\n",
      "Epoch 907, Loss 4.163630\n",
      "Params: tensor(0.3471)\n",
      "Grad: tensor([-0.1127,  0.6381])\n",
      "Epoch 908, Loss 4.159436\n",
      "Params: tensor(0.3471)\n",
      "Grad: tensor([-0.1125,  0.6371])\n",
      "Epoch 909, Loss 4.155253\n",
      "Params: tensor(0.3470)\n",
      "Grad: tensor([-0.1124,  0.6360])\n",
      "Epoch 910, Loss 4.151086\n",
      "Params: tensor(0.3469)\n",
      "Grad: tensor([-0.1122,  0.6349])\n",
      "Epoch 911, Loss 4.146934\n",
      "Params: tensor(0.3468)\n",
      "Grad: tensor([-0.1120,  0.6338])\n",
      "Epoch 912, Loss 4.142794\n",
      "Params: tensor(0.3467)\n",
      "Grad: tensor([-0.1118,  0.6327])\n",
      "Epoch 913, Loss 4.138669\n",
      "Params: tensor(0.3467)\n",
      "Grad: tensor([-0.1116,  0.6317])\n",
      "Epoch 914, Loss 4.134559\n",
      "Params: tensor(0.3466)\n",
      "Grad: tensor([-0.1114,  0.6306])\n",
      "Epoch 915, Loss 4.130465\n",
      "Params: tensor(0.3465)\n",
      "Grad: tensor([-0.1112,  0.6295])\n",
      "Epoch 916, Loss 4.126378\n",
      "Params: tensor(0.3464)\n",
      "Grad: tensor([-0.1110,  0.6284])\n",
      "Epoch 917, Loss 4.122310\n",
      "Params: tensor(0.3463)\n",
      "Grad: tensor([-0.1108,  0.6274])\n",
      "Epoch 918, Loss 4.118253\n",
      "Params: tensor(0.3463)\n",
      "Grad: tensor([-0.1107,  0.6263])\n",
      "Epoch 919, Loss 4.114213\n",
      "Params: tensor(0.3462)\n",
      "Grad: tensor([-0.1104,  0.6253])\n",
      "Epoch 920, Loss 4.110184\n",
      "Params: tensor(0.3461)\n",
      "Grad: tensor([-0.1103,  0.6242])\n",
      "Epoch 921, Loss 4.106170\n",
      "Params: tensor(0.3460)\n",
      "Grad: tensor([-0.1101,  0.6231])\n",
      "Epoch 922, Loss 4.102171\n",
      "Params: tensor(0.3460)\n",
      "Grad: tensor([-0.1099,  0.6221])\n",
      "Epoch 923, Loss 4.098181\n",
      "Params: tensor(0.3459)\n",
      "Grad: tensor([-0.1097,  0.6210])\n",
      "Epoch 924, Loss 4.094209\n",
      "Params: tensor(0.3458)\n",
      "Grad: tensor([-0.1095,  0.6200])\n",
      "Epoch 925, Loss 4.090250\n",
      "Params: tensor(0.3457)\n",
      "Grad: tensor([-0.1093,  0.6189])\n",
      "Epoch 926, Loss 4.086300\n",
      "Params: tensor(0.3456)\n",
      "Grad: tensor([-0.1091,  0.6179])\n",
      "Epoch 927, Loss 4.082366\n",
      "Params: tensor(0.3456)\n",
      "Grad: tensor([-0.1090,  0.6168])\n",
      "Epoch 928, Loss 4.078448\n",
      "Params: tensor(0.3455)\n",
      "Grad: tensor([-0.1088,  0.6158])\n",
      "Epoch 929, Loss 4.074540\n",
      "Params: tensor(0.3454)\n",
      "Grad: tensor([-0.1086,  0.6147])\n",
      "Epoch 930, Loss 4.070650\n",
      "Params: tensor(0.3453)\n",
      "Grad: tensor([-0.1084,  0.6137])\n",
      "Epoch 931, Loss 4.066769\n",
      "Params: tensor(0.3453)\n",
      "Grad: tensor([-0.1082,  0.6126])\n",
      "Epoch 932, Loss 4.062900\n",
      "Params: tensor(0.3452)\n",
      "Grad: tensor([-0.1080,  0.6116])\n",
      "Epoch 933, Loss 4.059047\n",
      "Params: tensor(0.3451)\n",
      "Grad: tensor([-0.1079,  0.6105])\n",
      "Epoch 934, Loss 4.055204\n",
      "Params: tensor(0.3450)\n",
      "Grad: tensor([-0.1077,  0.6095])\n",
      "Epoch 935, Loss 4.051378\n",
      "Params: tensor(0.3450)\n",
      "Grad: tensor([-0.1075,  0.6085])\n",
      "Epoch 936, Loss 4.047564\n",
      "Params: tensor(0.3449)\n",
      "Grad: tensor([-0.1073,  0.6074])\n",
      "Epoch 937, Loss 4.043762\n",
      "Params: tensor(0.3448)\n",
      "Grad: tensor([-0.1071,  0.6064])\n",
      "Epoch 938, Loss 4.039972\n",
      "Params: tensor(0.3447)\n",
      "Grad: tensor([-0.1069,  0.6054])\n",
      "Epoch 939, Loss 4.036197\n",
      "Params: tensor(0.3447)\n",
      "Grad: tensor([-0.1068,  0.6043])\n",
      "Epoch 940, Loss 4.032433\n",
      "Params: tensor(0.3446)\n",
      "Grad: tensor([-0.1066,  0.6033])\n",
      "Epoch 941, Loss 4.028685\n",
      "Params: tensor(0.3445)\n",
      "Grad: tensor([-0.1064,  0.6023])\n",
      "Epoch 942, Loss 4.024947\n",
      "Params: tensor(0.3444)\n",
      "Grad: tensor([-0.1062,  0.6013])\n",
      "Epoch 943, Loss 4.021221\n",
      "Params: tensor(0.3444)\n",
      "Grad: tensor([-0.1060,  0.6003])\n",
      "Epoch 944, Loss 4.017508\n",
      "Params: tensor(0.3443)\n",
      "Grad: tensor([-0.1058,  0.5992])\n",
      "Epoch 945, Loss 4.013809\n",
      "Params: tensor(0.3442)\n",
      "Grad: tensor([-0.1057,  0.5982])\n",
      "Epoch 946, Loss 4.010123\n",
      "Params: tensor(0.3442)\n",
      "Grad: tensor([-0.1055,  0.5972])\n",
      "Epoch 947, Loss 4.006446\n",
      "Params: tensor(0.3441)\n",
      "Grad: tensor([-0.1053,  0.5962])\n",
      "Epoch 948, Loss 4.002786\n",
      "Params: tensor(0.3440)\n",
      "Grad: tensor([-0.1051,  0.5952])\n",
      "Epoch 949, Loss 3.999135\n",
      "Params: tensor(0.3439)\n",
      "Grad: tensor([-0.1050,  0.5942])\n",
      "Epoch 950, Loss 3.995498\n",
      "Params: tensor(0.3439)\n",
      "Grad: tensor([-0.1048,  0.5931])\n",
      "Epoch 951, Loss 3.991874\n",
      "Params: tensor(0.3438)\n",
      "Grad: tensor([-0.1046,  0.5921])\n",
      "Epoch 952, Loss 3.988261\n",
      "Params: tensor(0.3437)\n",
      "Grad: tensor([-0.1044,  0.5911])\n",
      "Epoch 953, Loss 3.984660\n",
      "Params: tensor(0.3437)\n",
      "Grad: tensor([-0.1042,  0.5901])\n",
      "Epoch 954, Loss 3.981073\n",
      "Params: tensor(0.3436)\n",
      "Grad: tensor([-0.1041,  0.5891])\n",
      "Epoch 955, Loss 3.977496\n",
      "Params: tensor(0.3435)\n",
      "Grad: tensor([-0.1039,  0.5881])\n",
      "Epoch 956, Loss 3.973931\n",
      "Params: tensor(0.3434)\n",
      "Grad: tensor([-0.1037,  0.5871])\n",
      "Epoch 957, Loss 3.970381\n",
      "Params: tensor(0.3434)\n",
      "Grad: tensor([-0.1035,  0.5861])\n",
      "Epoch 958, Loss 3.966841\n",
      "Params: tensor(0.3433)\n",
      "Grad: tensor([-0.1034,  0.5851])\n",
      "Epoch 959, Loss 3.963313\n",
      "Params: tensor(0.3432)\n",
      "Grad: tensor([-0.1032,  0.5841])\n",
      "Epoch 960, Loss 3.959796\n",
      "Params: tensor(0.3432)\n",
      "Grad: tensor([-0.1030,  0.5831])\n",
      "Epoch 961, Loss 3.956295\n",
      "Params: tensor(0.3431)\n",
      "Grad: tensor([-0.1028,  0.5822])\n",
      "Epoch 962, Loss 3.952801\n",
      "Params: tensor(0.3430)\n",
      "Grad: tensor([-0.1026,  0.5812])\n",
      "Epoch 963, Loss 3.949323\n",
      "Params: tensor(0.3430)\n",
      "Grad: tensor([-0.1025,  0.5802])\n",
      "Epoch 964, Loss 3.945855\n",
      "Params: tensor(0.3429)\n",
      "Grad: tensor([-0.1023,  0.5792])\n",
      "Epoch 965, Loss 3.942398\n",
      "Params: tensor(0.3428)\n",
      "Grad: tensor([-0.1021,  0.5782])\n",
      "Epoch 966, Loss 3.938954\n",
      "Params: tensor(0.3427)\n",
      "Grad: tensor([-0.1020,  0.5772])\n",
      "Epoch 967, Loss 3.935520\n",
      "Params: tensor(0.3427)\n",
      "Grad: tensor([-0.1018,  0.5762])\n",
      "Epoch 968, Loss 3.932096\n",
      "Params: tensor(0.3426)\n",
      "Grad: tensor([-0.1016,  0.5753])\n",
      "Epoch 969, Loss 3.928688\n",
      "Params: tensor(0.3425)\n",
      "Grad: tensor([-0.1015,  0.5743])\n",
      "Epoch 970, Loss 3.925292\n",
      "Params: tensor(0.3425)\n",
      "Grad: tensor([-0.1013,  0.5733])\n",
      "Epoch 971, Loss 3.921906\n",
      "Params: tensor(0.3424)\n",
      "Grad: tensor([-0.1011,  0.5723])\n",
      "Epoch 972, Loss 3.918527\n",
      "Params: tensor(0.3423)\n",
      "Grad: tensor([-0.1009,  0.5714])\n",
      "Epoch 973, Loss 3.915166\n",
      "Params: tensor(0.3423)\n",
      "Grad: tensor([-0.1008,  0.5704])\n",
      "Epoch 974, Loss 3.911815\n",
      "Params: tensor(0.3422)\n",
      "Grad: tensor([-0.1006,  0.5694])\n",
      "Epoch 975, Loss 3.908474\n",
      "Params: tensor(0.3421)\n",
      "Grad: tensor([-0.1004,  0.5685])\n",
      "Epoch 976, Loss 3.905143\n",
      "Params: tensor(0.3421)\n",
      "Grad: tensor([-0.1003,  0.5675])\n",
      "Epoch 977, Loss 3.901825\n",
      "Params: tensor(0.3420)\n",
      "Grad: tensor([-0.1001,  0.5665])\n",
      "Epoch 978, Loss 3.898517\n",
      "Params: tensor(0.3419)\n",
      "Grad: tensor([-0.0999,  0.5656])\n",
      "Epoch 979, Loss 3.895222\n",
      "Params: tensor(0.3419)\n",
      "Grad: tensor([-0.0997,  0.5646])\n",
      "Epoch 980, Loss 3.891935\n",
      "Params: tensor(0.3418)\n",
      "Grad: tensor([-0.0996,  0.5637])\n",
      "Epoch 981, Loss 3.888664\n",
      "Params: tensor(0.3417)\n",
      "Grad: tensor([-0.0994,  0.5627])\n",
      "Epoch 982, Loss 3.885401\n",
      "Params: tensor(0.3417)\n",
      "Grad: tensor([-0.0992,  0.5617])\n",
      "Epoch 983, Loss 3.882150\n",
      "Params: tensor(0.3416)\n",
      "Grad: tensor([-0.0991,  0.5608])\n",
      "Epoch 984, Loss 3.878910\n",
      "Params: tensor(0.3415)\n",
      "Grad: tensor([-0.0989,  0.5598])\n",
      "Epoch 985, Loss 3.875680\n",
      "Params: tensor(0.3415)\n",
      "Grad: tensor([-0.0987,  0.5589])\n",
      "Epoch 986, Loss 3.872463\n",
      "Params: tensor(0.3414)\n",
      "Grad: tensor([-0.0986,  0.5579])\n",
      "Epoch 987, Loss 3.869256\n",
      "Params: tensor(0.3413)\n",
      "Grad: tensor([-0.0984,  0.5570])\n",
      "Epoch 988, Loss 3.866060\n",
      "Params: tensor(0.3413)\n",
      "Grad: tensor([-0.0982,  0.5560])\n",
      "Epoch 989, Loss 3.862872\n",
      "Params: tensor(0.3412)\n",
      "Grad: tensor([-0.0981,  0.5551])\n",
      "Epoch 990, Loss 3.859699\n",
      "Params: tensor(0.3411)\n",
      "Grad: tensor([-0.0979,  0.5541])\n",
      "Epoch 991, Loss 3.856535\n",
      "Params: tensor(0.3411)\n",
      "Grad: tensor([-0.0978,  0.5532])\n",
      "Epoch 992, Loss 3.853381\n",
      "Params: tensor(0.3410)\n",
      "Grad: tensor([-0.0976,  0.5523])\n",
      "Epoch 993, Loss 3.850237\n",
      "Params: tensor(0.3409)\n",
      "Grad: tensor([-0.0974,  0.5513])\n",
      "Epoch 994, Loss 3.847109\n",
      "Params: tensor(0.3409)\n",
      "Grad: tensor([-0.0973,  0.5504])\n",
      "Epoch 995, Loss 3.843984\n",
      "Params: tensor(0.3408)\n",
      "Grad: tensor([-0.0971,  0.5495])\n",
      "Epoch 996, Loss 3.840876\n",
      "Params: tensor(0.3408)\n",
      "Grad: tensor([-0.0969,  0.5485])\n",
      "Epoch 997, Loss 3.837775\n",
      "Params: tensor(0.3407)\n",
      "Grad: tensor([-0.0967,  0.5476])\n",
      "Epoch 998, Loss 3.834686\n",
      "Params: tensor(0.3406)\n",
      "Grad: tensor([-0.0966,  0.5467])\n",
      "Epoch 999, Loss 3.831606\n",
      "Params: tensor(0.3406)\n",
      "Grad: tensor([-0.0964,  0.5457])\n",
      "Epoch 1000, Loss 3.828538\n",
      "Params: tensor(0.3405)\n",
      "Grad: tensor([-0.0962,  0.5448])\n",
      "Epoch 1001, Loss 3.825483\n",
      "Params: tensor(0.3404)\n",
      "Grad: tensor([-0.0961,  0.5439])\n",
      "Epoch 1002, Loss 3.822433\n",
      "Params: tensor(0.3404)\n",
      "Grad: tensor([-0.0959,  0.5430])\n",
      "Epoch 1003, Loss 3.819398\n",
      "Params: tensor(0.3403)\n",
      "Grad: tensor([-0.0957,  0.5420])\n",
      "Epoch 1004, Loss 3.816369\n",
      "Params: tensor(0.3402)\n",
      "Grad: tensor([-0.0956,  0.5411])\n",
      "Epoch 1005, Loss 3.813350\n",
      "Params: tensor(0.3402)\n",
      "Grad: tensor([-0.0954,  0.5402])\n",
      "Epoch 1006, Loss 3.810344\n",
      "Params: tensor(0.3401)\n",
      "Grad: tensor([-0.0953,  0.5393])\n",
      "Epoch 1007, Loss 3.807348\n",
      "Params: tensor(0.3401)\n",
      "Grad: tensor([-0.0951,  0.5384])\n",
      "Epoch 1008, Loss 3.804360\n",
      "Params: tensor(0.3400)\n",
      "Grad: tensor([-0.0949,  0.5375])\n",
      "Epoch 1009, Loss 3.801384\n",
      "Params: tensor(0.3399)\n",
      "Grad: tensor([-0.0948,  0.5365])\n",
      "Epoch 1010, Loss 3.798421\n",
      "Params: tensor(0.3399)\n",
      "Grad: tensor([-0.0946,  0.5356])\n",
      "Epoch 1011, Loss 3.795465\n",
      "Params: tensor(0.3398)\n",
      "Grad: tensor([-0.0945,  0.5347])\n",
      "Epoch 1012, Loss 3.792518\n",
      "Params: tensor(0.3398)\n",
      "Grad: tensor([-0.0943,  0.5338])\n",
      "Epoch 1013, Loss 3.789584\n",
      "Params: tensor(0.3397)\n",
      "Grad: tensor([-0.0942,  0.5329])\n",
      "Epoch 1014, Loss 3.786658\n",
      "Params: tensor(0.3396)\n",
      "Grad: tensor([-0.0940,  0.5320])\n",
      "Epoch 1015, Loss 3.783740\n",
      "Params: tensor(0.3396)\n",
      "Grad: tensor([-0.0938,  0.5311])\n",
      "Epoch 1016, Loss 3.780832\n",
      "Params: tensor(0.3395)\n",
      "Grad: tensor([-0.0937,  0.5302])\n",
      "Epoch 1017, Loss 3.777939\n",
      "Params: tensor(0.3394)\n",
      "Grad: tensor([-0.0935,  0.5293])\n",
      "Epoch 1018, Loss 3.775053\n",
      "Params: tensor(0.3394)\n",
      "Grad: tensor([-0.0933,  0.5284])\n",
      "Epoch 1019, Loss 3.772173\n",
      "Params: tensor(0.3393)\n",
      "Grad: tensor([-0.0932,  0.5275])\n",
      "Epoch 1020, Loss 3.769311\n",
      "Params: tensor(0.3393)\n",
      "Grad: tensor([-0.0930,  0.5266])\n",
      "Epoch 1021, Loss 3.766450\n",
      "Params: tensor(0.3392)\n",
      "Grad: tensor([-0.0929,  0.5257])\n",
      "Epoch 1022, Loss 3.763602\n",
      "Params: tensor(0.3391)\n",
      "Grad: tensor([-0.0927,  0.5248])\n",
      "Epoch 1023, Loss 3.760766\n",
      "Params: tensor(0.3391)\n",
      "Grad: tensor([-0.0926,  0.5239])\n",
      "Epoch 1024, Loss 3.757936\n",
      "Params: tensor(0.3390)\n",
      "Grad: tensor([-0.0924,  0.5230])\n",
      "Epoch 1025, Loss 3.755118\n",
      "Params: tensor(0.3390)\n",
      "Grad: tensor([-0.0922,  0.5221])\n",
      "Epoch 1026, Loss 3.752309\n",
      "Params: tensor(0.3389)\n",
      "Grad: tensor([-0.0921,  0.5213])\n",
      "Epoch 1027, Loss 3.749511\n",
      "Params: tensor(0.3388)\n",
      "Grad: tensor([-0.0919,  0.5204])\n",
      "Epoch 1028, Loss 3.746722\n",
      "Params: tensor(0.3388)\n",
      "Grad: tensor([-0.0918,  0.5195])\n",
      "Epoch 1029, Loss 3.743940\n",
      "Params: tensor(0.3387)\n",
      "Grad: tensor([-0.0916,  0.5186])\n",
      "Epoch 1030, Loss 3.741169\n",
      "Params: tensor(0.3387)\n",
      "Grad: tensor([-0.0915,  0.5177])\n",
      "Epoch 1031, Loss 3.738407\n",
      "Params: tensor(0.3386)\n",
      "Grad: tensor([-0.0913,  0.5168])\n",
      "Epoch 1032, Loss 3.735656\n",
      "Params: tensor(0.3386)\n",
      "Grad: tensor([-0.0912,  0.5160])\n",
      "Epoch 1033, Loss 3.732914\n",
      "Params: tensor(0.3385)\n",
      "Grad: tensor([-0.0910,  0.5151])\n",
      "Epoch 1034, Loss 3.730181\n",
      "Params: tensor(0.3384)\n",
      "Grad: tensor([-0.0908,  0.5142])\n",
      "Epoch 1035, Loss 3.727456\n",
      "Params: tensor(0.3384)\n",
      "Grad: tensor([-0.0907,  0.5133])\n",
      "Epoch 1036, Loss 3.724740\n",
      "Params: tensor(0.3383)\n",
      "Grad: tensor([-0.0905,  0.5125])\n",
      "Epoch 1037, Loss 3.722034\n",
      "Params: tensor(0.3383)\n",
      "Grad: tensor([-0.0904,  0.5116])\n",
      "Epoch 1038, Loss 3.719337\n",
      "Params: tensor(0.3382)\n",
      "Grad: tensor([-0.0902,  0.5107])\n",
      "Epoch 1039, Loss 3.716651\n",
      "Params: tensor(0.3381)\n",
      "Grad: tensor([-0.0901,  0.5099])\n",
      "Epoch 1040, Loss 3.713972\n",
      "Params: tensor(0.3381)\n",
      "Grad: tensor([-0.0899,  0.5090])\n",
      "Epoch 1041, Loss 3.711302\n",
      "Params: tensor(0.3380)\n",
      "Grad: tensor([-0.0898,  0.5081])\n",
      "Epoch 1042, Loss 3.708644\n",
      "Params: tensor(0.3380)\n",
      "Grad: tensor([-0.0896,  0.5073])\n",
      "Epoch 1043, Loss 3.705991\n",
      "Params: tensor(0.3379)\n",
      "Grad: tensor([-0.0895,  0.5064])\n",
      "Epoch 1044, Loss 3.703351\n",
      "Params: tensor(0.3379)\n",
      "Grad: tensor([-0.0893,  0.5055])\n",
      "Epoch 1045, Loss 3.700716\n",
      "Params: tensor(0.3378)\n",
      "Grad: tensor([-0.0892,  0.5047])\n",
      "Epoch 1046, Loss 3.698091\n",
      "Params: tensor(0.3378)\n",
      "Grad: tensor([-0.0890,  0.5038])\n",
      "Epoch 1047, Loss 3.695476\n",
      "Params: tensor(0.3377)\n",
      "Grad: tensor([-0.0888,  0.5030])\n",
      "Epoch 1048, Loss 3.692869\n",
      "Params: tensor(0.3376)\n",
      "Grad: tensor([-0.0887,  0.5021])\n",
      "Epoch 1049, Loss 3.690273\n",
      "Params: tensor(0.3376)\n",
      "Grad: tensor([-0.0886,  0.5013])\n",
      "Epoch 1050, Loss 3.687683\n",
      "Params: tensor(0.3375)\n",
      "Grad: tensor([-0.0884,  0.5004])\n",
      "Epoch 1051, Loss 3.685104\n",
      "Params: tensor(0.3375)\n",
      "Grad: tensor([-0.0882,  0.4996])\n",
      "Epoch 1052, Loss 3.682532\n",
      "Params: tensor(0.3374)\n",
      "Grad: tensor([-0.0881,  0.4987])\n",
      "Epoch 1053, Loss 3.679969\n",
      "Params: tensor(0.3374)\n",
      "Grad: tensor([-0.0879,  0.4979])\n",
      "Epoch 1054, Loss 3.677417\n",
      "Params: tensor(0.3373)\n",
      "Grad: tensor([-0.0878,  0.4970])\n",
      "Epoch 1055, Loss 3.674871\n",
      "Params: tensor(0.3372)\n",
      "Grad: tensor([-0.0877,  0.4962])\n",
      "Epoch 1056, Loss 3.672335\n",
      "Params: tensor(0.3372)\n",
      "Grad: tensor([-0.0875,  0.4953])\n",
      "Epoch 1057, Loss 3.669804\n",
      "Params: tensor(0.3371)\n",
      "Grad: tensor([-0.0873,  0.4945])\n",
      "Epoch 1058, Loss 3.667287\n",
      "Params: tensor(0.3371)\n",
      "Grad: tensor([-0.0872,  0.4936])\n",
      "Epoch 1059, Loss 3.664775\n",
      "Params: tensor(0.3370)\n",
      "Grad: tensor([-0.0870,  0.4928])\n",
      "Epoch 1060, Loss 3.662273\n",
      "Params: tensor(0.3370)\n",
      "Grad: tensor([-0.0869,  0.4920])\n",
      "Epoch 1061, Loss 3.659778\n",
      "Params: tensor(0.3369)\n",
      "Grad: tensor([-0.0868,  0.4911])\n",
      "Epoch 1062, Loss 3.657295\n",
      "Params: tensor(0.3369)\n",
      "Grad: tensor([-0.0866,  0.4903])\n",
      "Epoch 1063, Loss 3.654816\n",
      "Params: tensor(0.3368)\n",
      "Grad: tensor([-0.0865,  0.4895])\n",
      "Epoch 1064, Loss 3.652349\n",
      "Params: tensor(0.3368)\n",
      "Grad: tensor([-0.0863,  0.4886])\n",
      "Epoch 1065, Loss 3.649889\n",
      "Params: tensor(0.3367)\n",
      "Grad: tensor([-0.0862,  0.4878])\n",
      "Epoch 1066, Loss 3.647437\n",
      "Params: tensor(0.3366)\n",
      "Grad: tensor([-0.0860,  0.4870])\n",
      "Epoch 1067, Loss 3.644991\n",
      "Params: tensor(0.3366)\n",
      "Grad: tensor([-0.0859,  0.4862])\n",
      "Epoch 1068, Loss 3.642559\n",
      "Params: tensor(0.3365)\n",
      "Grad: tensor([-0.0857,  0.4853])\n",
      "Epoch 1069, Loss 3.640132\n",
      "Params: tensor(0.3365)\n",
      "Grad: tensor([-0.0856,  0.4845])\n",
      "Epoch 1070, Loss 3.637711\n",
      "Params: tensor(0.3364)\n",
      "Grad: tensor([-0.0854,  0.4837])\n",
      "Epoch 1071, Loss 3.635302\n",
      "Params: tensor(0.3364)\n",
      "Grad: tensor([-0.0853,  0.4829])\n",
      "Epoch 1072, Loss 3.632902\n",
      "Params: tensor(0.3363)\n",
      "Grad: tensor([-0.0851,  0.4820])\n",
      "Epoch 1073, Loss 3.630508\n",
      "Params: tensor(0.3363)\n",
      "Grad: tensor([-0.0850,  0.4812])\n",
      "Epoch 1074, Loss 3.628119\n",
      "Params: tensor(0.3362)\n",
      "Grad: tensor([-0.0849,  0.4804])\n",
      "Epoch 1075, Loss 3.625741\n",
      "Params: tensor(0.3362)\n",
      "Grad: tensor([-0.0847,  0.4796])\n",
      "Epoch 1076, Loss 3.623374\n",
      "Params: tensor(0.3361)\n",
      "Grad: tensor([-0.0846,  0.4788])\n",
      "Epoch 1077, Loss 3.621010\n",
      "Params: tensor(0.3361)\n",
      "Grad: tensor([-0.0844,  0.4780])\n",
      "Epoch 1078, Loss 3.618659\n",
      "Params: tensor(0.3360)\n",
      "Grad: tensor([-0.0843,  0.4771])\n",
      "Epoch 1079, Loss 3.616311\n",
      "Params: tensor(0.3360)\n",
      "Grad: tensor([-0.0841,  0.4763])\n",
      "Epoch 1080, Loss 3.613973\n",
      "Params: tensor(0.3359)\n",
      "Grad: tensor([-0.0840,  0.4755])\n",
      "Epoch 1081, Loss 3.611643\n",
      "Params: tensor(0.3359)\n",
      "Grad: tensor([-0.0839,  0.4747])\n",
      "Epoch 1082, Loss 3.609321\n",
      "Params: tensor(0.3358)\n",
      "Grad: tensor([-0.0837,  0.4739])\n",
      "Epoch 1083, Loss 3.607008\n",
      "Params: tensor(0.3357)\n",
      "Grad: tensor([-0.0836,  0.4731])\n",
      "Epoch 1084, Loss 3.604701\n",
      "Params: tensor(0.3357)\n",
      "Grad: tensor([-0.0834,  0.4723])\n",
      "Epoch 1085, Loss 3.602403\n",
      "Params: tensor(0.3356)\n",
      "Grad: tensor([-0.0833,  0.4715])\n",
      "Epoch 1086, Loss 3.600114\n",
      "Params: tensor(0.3356)\n",
      "Grad: tensor([-0.0832,  0.4707])\n",
      "Epoch 1087, Loss 3.597831\n",
      "Params: tensor(0.3355)\n",
      "Grad: tensor([-0.0830,  0.4699])\n",
      "Epoch 1088, Loss 3.595553\n",
      "Params: tensor(0.3355)\n",
      "Grad: tensor([-0.0829,  0.4691])\n",
      "Epoch 1089, Loss 3.593287\n",
      "Params: tensor(0.3354)\n",
      "Grad: tensor([-0.0827,  0.4683])\n",
      "Epoch 1090, Loss 3.591030\n",
      "Params: tensor(0.3354)\n",
      "Grad: tensor([-0.0826,  0.4675])\n",
      "Epoch 1091, Loss 3.588776\n",
      "Params: tensor(0.3353)\n",
      "Grad: tensor([-0.0824,  0.4667])\n",
      "Epoch 1092, Loss 3.586534\n",
      "Params: tensor(0.3353)\n",
      "Grad: tensor([-0.0823,  0.4659])\n",
      "Epoch 1093, Loss 3.584294\n",
      "Params: tensor(0.3352)\n",
      "Grad: tensor([-0.0822,  0.4651])\n",
      "Epoch 1094, Loss 3.582067\n",
      "Params: tensor(0.3352)\n",
      "Grad: tensor([-0.0820,  0.4643])\n",
      "Epoch 1095, Loss 3.579845\n",
      "Params: tensor(0.3351)\n",
      "Grad: tensor([-0.0819,  0.4636])\n",
      "Epoch 1096, Loss 3.577631\n",
      "Params: tensor(0.3351)\n",
      "Grad: tensor([-0.0818,  0.4628])\n",
      "Epoch 1097, Loss 3.575424\n",
      "Params: tensor(0.3350)\n",
      "Grad: tensor([-0.0816,  0.4620])\n",
      "Epoch 1098, Loss 3.573225\n",
      "Params: tensor(0.3350)\n",
      "Grad: tensor([-0.0815,  0.4612])\n",
      "Epoch 1099, Loss 3.571035\n",
      "Params: tensor(0.3349)\n",
      "Grad: tensor([-0.0813,  0.4604])\n",
      "Epoch 1100, Loss 3.568848\n",
      "Params: tensor(0.3349)\n",
      "Grad: tensor([-0.0812,  0.4596])\n",
      "Epoch 1101, Loss 3.566673\n",
      "Params: tensor(0.3348)\n",
      "Grad: tensor([-0.0810,  0.4588])\n",
      "Epoch 1102, Loss 3.564506\n",
      "Params: tensor(0.3348)\n",
      "Grad: tensor([-0.0809,  0.4581])\n",
      "Epoch 1103, Loss 3.562341\n",
      "Params: tensor(0.3347)\n",
      "Grad: tensor([-0.0808,  0.4573])\n",
      "Epoch 1104, Loss 3.560185\n",
      "Params: tensor(0.3347)\n",
      "Grad: tensor([-0.0806,  0.4565])\n",
      "Epoch 1105, Loss 3.558040\n",
      "Params: tensor(0.3346)\n",
      "Grad: tensor([-0.0805,  0.4557])\n",
      "Epoch 1106, Loss 3.555901\n",
      "Params: tensor(0.3346)\n",
      "Grad: tensor([-0.0804,  0.4550])\n",
      "Epoch 1107, Loss 3.553767\n",
      "Params: tensor(0.3345)\n",
      "Grad: tensor([-0.0802,  0.4542])\n",
      "Epoch 1108, Loss 3.551641\n",
      "Params: tensor(0.3345)\n",
      "Grad: tensor([-0.0801,  0.4534])\n",
      "Epoch 1109, Loss 3.549524\n",
      "Params: tensor(0.3344)\n",
      "Grad: tensor([-0.0799,  0.4527])\n",
      "Epoch 1110, Loss 3.547411\n",
      "Params: tensor(0.3344)\n",
      "Grad: tensor([-0.0798,  0.4519])\n",
      "Epoch 1111, Loss 3.545309\n",
      "Params: tensor(0.3343)\n",
      "Grad: tensor([-0.0797,  0.4511])\n",
      "Epoch 1112, Loss 3.543211\n",
      "Params: tensor(0.3343)\n",
      "Grad: tensor([-0.0796,  0.4503])\n",
      "Epoch 1113, Loss 3.541124\n",
      "Params: tensor(0.3342)\n",
      "Grad: tensor([-0.0794,  0.4496])\n",
      "Epoch 1114, Loss 3.539041\n",
      "Params: tensor(0.3342)\n",
      "Grad: tensor([-0.0793,  0.4488])\n",
      "Epoch 1115, Loss 3.536967\n",
      "Params: tensor(0.3342)\n",
      "Grad: tensor([-0.0791,  0.4481])\n",
      "Epoch 1116, Loss 3.534896\n",
      "Params: tensor(0.3341)\n",
      "Grad: tensor([-0.0790,  0.4473])\n",
      "Epoch 1117, Loss 3.532835\n",
      "Params: tensor(0.3341)\n",
      "Grad: tensor([-0.0789,  0.4465])\n",
      "Epoch 1118, Loss 3.530781\n",
      "Params: tensor(0.3340)\n",
      "Grad: tensor([-0.0787,  0.4458])\n",
      "Epoch 1119, Loss 3.528734\n",
      "Params: tensor(0.3340)\n",
      "Grad: tensor([-0.0786,  0.4450])\n",
      "Epoch 1120, Loss 3.526694\n",
      "Params: tensor(0.3339)\n",
      "Grad: tensor([-0.0785,  0.4443])\n",
      "Epoch 1121, Loss 3.524662\n",
      "Params: tensor(0.3339)\n",
      "Grad: tensor([-0.0784,  0.4435])\n",
      "Epoch 1122, Loss 3.522633\n",
      "Params: tensor(0.3338)\n",
      "Grad: tensor([-0.0782,  0.4428])\n",
      "Epoch 1123, Loss 3.520614\n",
      "Params: tensor(0.3338)\n",
      "Grad: tensor([-0.0781,  0.4420])\n",
      "Epoch 1124, Loss 3.518601\n",
      "Params: tensor(0.3337)\n",
      "Grad: tensor([-0.0779,  0.4413])\n",
      "Epoch 1125, Loss 3.516594\n",
      "Params: tensor(0.3337)\n",
      "Grad: tensor([-0.0778,  0.4405])\n",
      "Epoch 1126, Loss 3.514594\n",
      "Params: tensor(0.3336)\n",
      "Grad: tensor([-0.0777,  0.4398])\n",
      "Epoch 1127, Loss 3.512602\n",
      "Params: tensor(0.3336)\n",
      "Grad: tensor([-0.0775,  0.4390])\n",
      "Epoch 1128, Loss 3.510619\n",
      "Params: tensor(0.3335)\n",
      "Grad: tensor([-0.0774,  0.4383])\n",
      "Epoch 1129, Loss 3.508637\n",
      "Params: tensor(0.3335)\n",
      "Grad: tensor([-0.0773,  0.4375])\n",
      "Epoch 1130, Loss 3.506665\n",
      "Params: tensor(0.3334)\n",
      "Grad: tensor([-0.0772,  0.4368])\n",
      "Epoch 1131, Loss 3.504699\n",
      "Params: tensor(0.3334)\n",
      "Grad: tensor([-0.0770,  0.4360])\n",
      "Epoch 1132, Loss 3.502741\n",
      "Params: tensor(0.3334)\n",
      "Grad: tensor([-0.0769,  0.4353])\n",
      "Epoch 1133, Loss 3.500789\n",
      "Params: tensor(0.3333)\n",
      "Grad: tensor([-0.0767,  0.4346])\n",
      "Epoch 1134, Loss 3.498843\n",
      "Params: tensor(0.3333)\n",
      "Grad: tensor([-0.0766,  0.4338])\n",
      "Epoch 1135, Loss 3.496905\n",
      "Params: tensor(0.3332)\n",
      "Grad: tensor([-0.0765,  0.4331])\n",
      "Epoch 1136, Loss 3.494972\n",
      "Params: tensor(0.3332)\n",
      "Grad: tensor([-0.0764,  0.4323])\n",
      "Epoch 1137, Loss 3.493046\n",
      "Params: tensor(0.3331)\n",
      "Grad: tensor([-0.0763,  0.4316])\n",
      "Epoch 1138, Loss 3.491127\n",
      "Params: tensor(0.3331)\n",
      "Grad: tensor([-0.0761,  0.4309])\n",
      "Epoch 1139, Loss 3.489214\n",
      "Params: tensor(0.3330)\n",
      "Grad: tensor([-0.0760,  0.4301])\n",
      "Epoch 1140, Loss 3.487308\n",
      "Params: tensor(0.3330)\n",
      "Grad: tensor([-0.0759,  0.4294])\n",
      "Epoch 1141, Loss 3.485410\n",
      "Params: tensor(0.3329)\n",
      "Grad: tensor([-0.0757,  0.4287])\n",
      "Epoch 1142, Loss 3.483515\n",
      "Params: tensor(0.3329)\n",
      "Grad: tensor([-0.0756,  0.4280])\n",
      "Epoch 1143, Loss 3.481627\n",
      "Params: tensor(0.3328)\n",
      "Grad: tensor([-0.0755,  0.4272])\n",
      "Epoch 1144, Loss 3.479746\n",
      "Params: tensor(0.3328)\n",
      "Grad: tensor([-0.0753,  0.4265])\n",
      "Epoch 1145, Loss 3.477872\n",
      "Params: tensor(0.3328)\n",
      "Grad: tensor([-0.0752,  0.4258])\n",
      "Epoch 1146, Loss 3.476005\n",
      "Params: tensor(0.3327)\n",
      "Grad: tensor([-0.0751,  0.4250])\n",
      "Epoch 1147, Loss 3.474143\n",
      "Params: tensor(0.3327)\n",
      "Grad: tensor([-0.0750,  0.4243])\n",
      "Epoch 1148, Loss 3.472288\n",
      "Params: tensor(0.3326)\n",
      "Grad: tensor([-0.0748,  0.4236])\n",
      "Epoch 1149, Loss 3.470441\n",
      "Params: tensor(0.3326)\n",
      "Grad: tensor([-0.0747,  0.4229])\n",
      "Epoch 1150, Loss 3.468597\n",
      "Params: tensor(0.3325)\n",
      "Grad: tensor([-0.0746,  0.4222])\n",
      "Epoch 1151, Loss 3.466762\n",
      "Params: tensor(0.3325)\n",
      "Grad: tensor([-0.0745,  0.4215])\n",
      "Epoch 1152, Loss 3.464930\n",
      "Params: tensor(0.3324)\n",
      "Grad: tensor([-0.0743,  0.4207])\n",
      "Epoch 1153, Loss 3.463105\n",
      "Params: tensor(0.3324)\n",
      "Grad: tensor([-0.0742,  0.4200])\n",
      "Epoch 1154, Loss 3.461290\n",
      "Params: tensor(0.3324)\n",
      "Grad: tensor([-0.0741,  0.4193])\n",
      "Epoch 1155, Loss 3.459477\n",
      "Params: tensor(0.3323)\n",
      "Grad: tensor([-0.0739,  0.4186])\n",
      "Epoch 1156, Loss 3.457671\n",
      "Params: tensor(0.3323)\n",
      "Grad: tensor([-0.0738,  0.4179])\n",
      "Epoch 1157, Loss 3.455873\n",
      "Params: tensor(0.3322)\n",
      "Grad: tensor([-0.0737,  0.4172])\n",
      "Epoch 1158, Loss 3.454080\n",
      "Params: tensor(0.3322)\n",
      "Grad: tensor([-0.0736,  0.4165])\n",
      "Epoch 1159, Loss 3.452293\n",
      "Params: tensor(0.3321)\n",
      "Grad: tensor([-0.0734,  0.4158])\n",
      "Epoch 1160, Loss 3.450513\n",
      "Params: tensor(0.3321)\n",
      "Grad: tensor([-0.0733,  0.4151])\n",
      "Epoch 1161, Loss 3.448736\n",
      "Params: tensor(0.3321)\n",
      "Grad: tensor([-0.0732,  0.4143])\n",
      "Epoch 1162, Loss 3.446968\n",
      "Params: tensor(0.3320)\n",
      "Grad: tensor([-0.0731,  0.4136])\n",
      "Epoch 1163, Loss 3.445203\n",
      "Params: tensor(0.3320)\n",
      "Grad: tensor([-0.0730,  0.4129])\n",
      "Epoch 1164, Loss 3.443449\n",
      "Params: tensor(0.3319)\n",
      "Grad: tensor([-0.0728,  0.4122])\n",
      "Epoch 1165, Loss 3.441697\n",
      "Params: tensor(0.3319)\n",
      "Grad: tensor([-0.0727,  0.4115])\n",
      "Epoch 1166, Loss 3.439952\n",
      "Params: tensor(0.3318)\n",
      "Grad: tensor([-0.0726,  0.4108])\n",
      "Epoch 1167, Loss 3.438210\n",
      "Params: tensor(0.3318)\n",
      "Grad: tensor([-0.0725,  0.4101])\n",
      "Epoch 1168, Loss 3.436479\n",
      "Params: tensor(0.3318)\n",
      "Grad: tensor([-0.0723,  0.4094])\n",
      "Epoch 1169, Loss 3.434753\n",
      "Params: tensor(0.3317)\n",
      "Grad: tensor([-0.0722,  0.4087])\n",
      "Epoch 1170, Loss 3.433030\n",
      "Params: tensor(0.3317)\n",
      "Grad: tensor([-0.0721,  0.4081])\n",
      "Epoch 1171, Loss 3.431314\n",
      "Params: tensor(0.3316)\n",
      "Grad: tensor([-0.0720,  0.4074])\n",
      "Epoch 1172, Loss 3.429607\n",
      "Params: tensor(0.3316)\n",
      "Grad: tensor([-0.0719,  0.4067])\n",
      "Epoch 1173, Loss 3.427903\n",
      "Params: tensor(0.3315)\n",
      "Grad: tensor([-0.0717,  0.4060])\n",
      "Epoch 1174, Loss 3.426204\n",
      "Params: tensor(0.3315)\n",
      "Grad: tensor([-0.0716,  0.4053])\n",
      "Epoch 1175, Loss 3.424510\n",
      "Params: tensor(0.3315)\n",
      "Grad: tensor([-0.0715,  0.4046])\n",
      "Epoch 1176, Loss 3.422823\n",
      "Params: tensor(0.3314)\n",
      "Grad: tensor([-0.0714,  0.4039])\n",
      "Epoch 1177, Loss 3.421144\n",
      "Params: tensor(0.3314)\n",
      "Grad: tensor([-0.0712,  0.4032])\n",
      "Epoch 1178, Loss 3.419468\n",
      "Params: tensor(0.3313)\n",
      "Grad: tensor([-0.0711,  0.4025])\n",
      "Epoch 1179, Loss 3.417798\n",
      "Params: tensor(0.3313)\n",
      "Grad: tensor([-0.0710,  0.4019])\n",
      "Epoch 1180, Loss 3.416134\n",
      "Params: tensor(0.3312)\n",
      "Grad: tensor([-0.0709,  0.4012])\n",
      "Epoch 1181, Loss 3.414476\n",
      "Params: tensor(0.3312)\n",
      "Grad: tensor([-0.0708,  0.4005])\n",
      "Epoch 1182, Loss 3.412824\n",
      "Params: tensor(0.3312)\n",
      "Grad: tensor([-0.0706,  0.3998])\n",
      "Epoch 1183, Loss 3.411176\n",
      "Params: tensor(0.3311)\n",
      "Grad: tensor([-0.0705,  0.3991])\n",
      "Epoch 1184, Loss 3.409534\n",
      "Params: tensor(0.3311)\n",
      "Grad: tensor([-0.0704,  0.3985])\n",
      "Epoch 1185, Loss 3.407900\n",
      "Params: tensor(0.3310)\n",
      "Grad: tensor([-0.0703,  0.3978])\n",
      "Epoch 1186, Loss 3.406271\n",
      "Params: tensor(0.3310)\n",
      "Grad: tensor([-0.0701,  0.3971])\n",
      "Epoch 1187, Loss 3.404645\n",
      "Params: tensor(0.3310)\n",
      "Grad: tensor([-0.0700,  0.3964])\n",
      "Epoch 1188, Loss 3.403024\n",
      "Params: tensor(0.3309)\n",
      "Grad: tensor([-0.0699,  0.3958])\n",
      "Epoch 1189, Loss 3.401413\n",
      "Params: tensor(0.3309)\n",
      "Grad: tensor([-0.0698,  0.3951])\n",
      "Epoch 1190, Loss 3.399802\n",
      "Params: tensor(0.3308)\n",
      "Grad: tensor([-0.0697,  0.3944])\n",
      "Epoch 1191, Loss 3.398200\n",
      "Params: tensor(0.3308)\n",
      "Grad: tensor([-0.0696,  0.3937])\n",
      "Epoch 1192, Loss 3.396602\n",
      "Params: tensor(0.3308)\n",
      "Grad: tensor([-0.0694,  0.3931])\n",
      "Epoch 1193, Loss 3.395011\n",
      "Params: tensor(0.3307)\n",
      "Grad: tensor([-0.0693,  0.3924])\n",
      "Epoch 1194, Loss 3.393425\n",
      "Params: tensor(0.3307)\n",
      "Grad: tensor([-0.0692,  0.3917])\n",
      "Epoch 1195, Loss 3.391844\n",
      "Params: tensor(0.3306)\n",
      "Grad: tensor([-0.0691,  0.3911])\n",
      "Epoch 1196, Loss 3.390266\n",
      "Params: tensor(0.3306)\n",
      "Grad: tensor([-0.0690,  0.3904])\n",
      "Epoch 1197, Loss 3.388697\n",
      "Params: tensor(0.3306)\n",
      "Grad: tensor([-0.0689,  0.3897])\n",
      "Epoch 1198, Loss 3.387131\n",
      "Params: tensor(0.3305)\n",
      "Grad: tensor([-0.0687,  0.3891])\n",
      "Epoch 1199, Loss 3.385571\n",
      "Params: tensor(0.3305)\n",
      "Grad: tensor([-0.0686,  0.3884])\n",
      "Epoch 1200, Loss 3.384018\n",
      "Params: tensor(0.3304)\n",
      "Grad: tensor([-0.0685,  0.3878])\n",
      "Epoch 1201, Loss 3.382467\n",
      "Params: tensor(0.3304)\n",
      "Grad: tensor([-0.0684,  0.3871])\n",
      "Epoch 1202, Loss 3.380925\n",
      "Params: tensor(0.3304)\n",
      "Grad: tensor([-0.0683,  0.3864])\n",
      "Epoch 1203, Loss 3.379385\n",
      "Params: tensor(0.3303)\n",
      "Grad: tensor([-0.0681,  0.3858])\n",
      "Epoch 1204, Loss 3.377851\n",
      "Params: tensor(0.3303)\n",
      "Grad: tensor([-0.0680,  0.3851])\n",
      "Epoch 1205, Loss 3.376323\n",
      "Params: tensor(0.3302)\n",
      "Grad: tensor([-0.0679,  0.3845])\n",
      "Epoch 1206, Loss 3.374800\n",
      "Params: tensor(0.3302)\n",
      "Grad: tensor([-0.0678,  0.3838])\n",
      "Epoch 1207, Loss 3.373284\n",
      "Params: tensor(0.3302)\n",
      "Grad: tensor([-0.0677,  0.3832])\n",
      "Epoch 1208, Loss 3.371769\n",
      "Params: tensor(0.3301)\n",
      "Grad: tensor([-0.0676,  0.3825])\n",
      "Epoch 1209, Loss 3.370261\n",
      "Params: tensor(0.3301)\n",
      "Grad: tensor([-0.0675,  0.3819])\n",
      "Epoch 1210, Loss 3.368760\n",
      "Params: tensor(0.3300)\n",
      "Grad: tensor([-0.0673,  0.3812])\n",
      "Epoch 1211, Loss 3.367262\n",
      "Params: tensor(0.3300)\n",
      "Grad: tensor([-0.0672,  0.3806])\n",
      "Epoch 1212, Loss 3.365771\n",
      "Params: tensor(0.3300)\n",
      "Grad: tensor([-0.0671,  0.3799])\n",
      "Epoch 1213, Loss 3.364282\n",
      "Params: tensor(0.3299)\n",
      "Grad: tensor([-0.0670,  0.3793])\n",
      "Epoch 1214, Loss 3.362800\n",
      "Params: tensor(0.3299)\n",
      "Grad: tensor([-0.0669,  0.3786])\n",
      "Epoch 1215, Loss 3.361324\n",
      "Params: tensor(0.3299)\n",
      "Grad: tensor([-0.0668,  0.3780])\n",
      "Epoch 1216, Loss 3.359850\n",
      "Params: tensor(0.3298)\n",
      "Grad: tensor([-0.0667,  0.3774])\n",
      "Epoch 1217, Loss 3.358384\n",
      "Params: tensor(0.3298)\n",
      "Grad: tensor([-0.0665,  0.3767])\n",
      "Epoch 1218, Loss 3.356921\n",
      "Params: tensor(0.3297)\n",
      "Grad: tensor([-0.0664,  0.3761])\n",
      "Epoch 1219, Loss 3.355464\n",
      "Params: tensor(0.3297)\n",
      "Grad: tensor([-0.0663,  0.3754])\n",
      "Epoch 1220, Loss 3.354012\n",
      "Params: tensor(0.3297)\n",
      "Grad: tensor([-0.0662,  0.3748])\n",
      "Epoch 1221, Loss 3.352564\n",
      "Params: tensor(0.3296)\n",
      "Grad: tensor([-0.0661,  0.3742])\n",
      "Epoch 1222, Loss 3.351122\n",
      "Params: tensor(0.3296)\n",
      "Grad: tensor([-0.0660,  0.3735])\n",
      "Epoch 1223, Loss 3.349685\n",
      "Params: tensor(0.3295)\n",
      "Grad: tensor([-0.0659,  0.3729])\n",
      "Epoch 1224, Loss 3.348251\n",
      "Params: tensor(0.3295)\n",
      "Grad: tensor([-0.0657,  0.3723])\n",
      "Epoch 1225, Loss 3.346824\n",
      "Params: tensor(0.3295)\n",
      "Grad: tensor([-0.0656,  0.3716])\n",
      "Epoch 1226, Loss 3.345403\n",
      "Params: tensor(0.3294)\n",
      "Grad: tensor([-0.0655,  0.3710])\n",
      "Epoch 1227, Loss 3.343982\n",
      "Params: tensor(0.3294)\n",
      "Grad: tensor([-0.0654,  0.3704])\n",
      "Epoch 1228, Loss 3.342571\n",
      "Params: tensor(0.3294)\n",
      "Grad: tensor([-0.0653,  0.3697])\n",
      "Epoch 1229, Loss 3.341160\n",
      "Params: tensor(0.3293)\n",
      "Grad: tensor([-0.0652,  0.3691])\n",
      "Epoch 1230, Loss 3.339758\n",
      "Params: tensor(0.3293)\n",
      "Grad: tensor([-0.0651,  0.3685])\n",
      "Epoch 1231, Loss 3.338359\n",
      "Params: tensor(0.3292)\n",
      "Grad: tensor([-0.0650,  0.3679])\n",
      "Epoch 1232, Loss 3.336965\n",
      "Params: tensor(0.3292)\n",
      "Grad: tensor([-0.0649,  0.3672])\n",
      "Epoch 1233, Loss 3.335577\n",
      "Params: tensor(0.3292)\n",
      "Grad: tensor([-0.0648,  0.3666])\n",
      "Epoch 1234, Loss 3.334192\n",
      "Params: tensor(0.3291)\n",
      "Grad: tensor([-0.0646,  0.3660])\n",
      "Epoch 1235, Loss 3.332811\n",
      "Params: tensor(0.3291)\n",
      "Grad: tensor([-0.0645,  0.3654])\n",
      "Epoch 1236, Loss 3.331436\n",
      "Params: tensor(0.3291)\n",
      "Grad: tensor([-0.0644,  0.3647])\n",
      "Epoch 1237, Loss 3.330065\n",
      "Params: tensor(0.3290)\n",
      "Grad: tensor([-0.0643,  0.3641])\n",
      "Epoch 1238, Loss 3.328699\n",
      "Params: tensor(0.3290)\n",
      "Grad: tensor([-0.0642,  0.3635])\n",
      "Epoch 1239, Loss 3.327339\n",
      "Params: tensor(0.3290)\n",
      "Grad: tensor([-0.0641,  0.3629])\n",
      "Epoch 1240, Loss 3.325980\n",
      "Params: tensor(0.3289)\n",
      "Grad: tensor([-0.0640,  0.3623])\n",
      "Epoch 1241, Loss 3.324628\n",
      "Params: tensor(0.3289)\n",
      "Grad: tensor([-0.0639,  0.3617])\n",
      "Epoch 1242, Loss 3.323279\n",
      "Params: tensor(0.3288)\n",
      "Grad: tensor([-0.0638,  0.3610])\n",
      "Epoch 1243, Loss 3.321935\n",
      "Params: tensor(0.3288)\n",
      "Grad: tensor([-0.0637,  0.3604])\n",
      "Epoch 1244, Loss 3.320600\n",
      "Params: tensor(0.3288)\n",
      "Grad: tensor([-0.0636,  0.3598])\n",
      "Epoch 1245, Loss 3.319264\n",
      "Params: tensor(0.3287)\n",
      "Grad: tensor([-0.0635,  0.3592])\n",
      "Epoch 1246, Loss 3.317935\n",
      "Params: tensor(0.3287)\n",
      "Grad: tensor([-0.0633,  0.3586])\n",
      "Epoch 1247, Loss 3.316611\n",
      "Params: tensor(0.3287)\n",
      "Grad: tensor([-0.0633,  0.3580])\n",
      "Epoch 1248, Loss 3.315289\n",
      "Params: tensor(0.3286)\n",
      "Grad: tensor([-0.0631,  0.3574])\n",
      "Epoch 1249, Loss 3.313973\n",
      "Params: tensor(0.3286)\n",
      "Grad: tensor([-0.0630,  0.3568])\n",
      "Epoch 1250, Loss 3.312663\n",
      "Params: tensor(0.3286)\n",
      "Grad: tensor([-0.0629,  0.3562])\n",
      "Epoch 1251, Loss 3.311353\n",
      "Params: tensor(0.3285)\n",
      "Grad: tensor([-0.0628,  0.3556])\n",
      "Epoch 1252, Loss 3.310053\n",
      "Params: tensor(0.3285)\n",
      "Grad: tensor([-0.0627,  0.3550])\n",
      "Epoch 1253, Loss 3.308756\n",
      "Params: tensor(0.3285)\n",
      "Grad: tensor([-0.0626,  0.3543])\n",
      "Epoch 1254, Loss 3.307463\n",
      "Params: tensor(0.3284)\n",
      "Grad: tensor([-0.0625,  0.3537])\n",
      "Epoch 1255, Loss 3.306170\n",
      "Params: tensor(0.3284)\n",
      "Grad: tensor([-0.0624,  0.3531])\n",
      "Epoch 1256, Loss 3.304887\n",
      "Params: tensor(0.3283)\n",
      "Grad: tensor([-0.0623,  0.3525])\n",
      "Epoch 1257, Loss 3.303605\n",
      "Params: tensor(0.3283)\n",
      "Grad: tensor([-0.0622,  0.3519])\n",
      "Epoch 1258, Loss 3.302329\n",
      "Params: tensor(0.3283)\n",
      "Grad: tensor([-0.0620,  0.3514])\n",
      "Epoch 1259, Loss 3.301057\n",
      "Params: tensor(0.3282)\n",
      "Grad: tensor([-0.0620,  0.3508])\n",
      "Epoch 1260, Loss 3.299791\n",
      "Params: tensor(0.3282)\n",
      "Grad: tensor([-0.0619,  0.3502])\n",
      "Epoch 1261, Loss 3.298528\n",
      "Params: tensor(0.3282)\n",
      "Grad: tensor([-0.0618,  0.3496])\n",
      "Epoch 1262, Loss 3.297267\n",
      "Params: tensor(0.3281)\n",
      "Grad: tensor([-0.0616,  0.3490])\n",
      "Epoch 1263, Loss 3.296014\n",
      "Params: tensor(0.3281)\n",
      "Grad: tensor([-0.0615,  0.3484])\n",
      "Epoch 1264, Loss 3.294762\n",
      "Params: tensor(0.3281)\n",
      "Grad: tensor([-0.0614,  0.3478])\n",
      "Epoch 1265, Loss 3.293517\n",
      "Params: tensor(0.3280)\n",
      "Grad: tensor([-0.0613,  0.3472])\n",
      "Epoch 1266, Loss 3.292276\n",
      "Params: tensor(0.3280)\n",
      "Grad: tensor([-0.0612,  0.3466])\n",
      "Epoch 1267, Loss 3.291036\n",
      "Params: tensor(0.3280)\n",
      "Grad: tensor([-0.0611,  0.3460])\n",
      "Epoch 1268, Loss 3.289804\n",
      "Params: tensor(0.3279)\n",
      "Grad: tensor([-0.0610,  0.3454])\n",
      "Epoch 1269, Loss 3.288573\n",
      "Params: tensor(0.3279)\n",
      "Grad: tensor([-0.0609,  0.3448])\n",
      "Epoch 1270, Loss 3.287347\n",
      "Params: tensor(0.3279)\n",
      "Grad: tensor([-0.0608,  0.3443])\n",
      "Epoch 1271, Loss 3.286129\n",
      "Params: tensor(0.3278)\n",
      "Grad: tensor([-0.0607,  0.3437])\n",
      "Epoch 1272, Loss 3.284911\n",
      "Params: tensor(0.3278)\n",
      "Grad: tensor([-0.0606,  0.3431])\n",
      "Epoch 1273, Loss 3.283698\n",
      "Params: tensor(0.3278)\n",
      "Grad: tensor([-0.0605,  0.3425])\n",
      "Epoch 1274, Loss 3.282488\n",
      "Params: tensor(0.3277)\n",
      "Grad: tensor([-0.0604,  0.3419])\n",
      "Epoch 1275, Loss 3.281284\n",
      "Params: tensor(0.3277)\n",
      "Grad: tensor([-0.0603,  0.3413])\n",
      "Epoch 1276, Loss 3.280085\n",
      "Params: tensor(0.3277)\n",
      "Grad: tensor([-0.0602,  0.3408])\n",
      "Epoch 1277, Loss 3.278888\n",
      "Params: tensor(0.3276)\n",
      "Grad: tensor([-0.0601,  0.3402])\n",
      "Epoch 1278, Loss 3.277696\n",
      "Params: tensor(0.3276)\n",
      "Grad: tensor([-0.0600,  0.3396])\n",
      "Epoch 1279, Loss 3.276506\n",
      "Params: tensor(0.3276)\n",
      "Grad: tensor([-0.0599,  0.3390])\n",
      "Epoch 1280, Loss 3.275322\n",
      "Params: tensor(0.3275)\n",
      "Grad: tensor([-0.0598,  0.3384])\n",
      "Epoch 1281, Loss 3.274142\n",
      "Params: tensor(0.3275)\n",
      "Grad: tensor([-0.0597,  0.3379])\n",
      "Epoch 1282, Loss 3.272968\n",
      "Params: tensor(0.3275)\n",
      "Grad: tensor([-0.0596,  0.3373])\n",
      "Epoch 1283, Loss 3.271793\n",
      "Params: tensor(0.3274)\n",
      "Grad: tensor([-0.0595,  0.3367])\n",
      "Epoch 1284, Loss 3.270625\n",
      "Params: tensor(0.3274)\n",
      "Grad: tensor([-0.0594,  0.3362])\n",
      "Epoch 1285, Loss 3.269460\n",
      "Params: tensor(0.3274)\n",
      "Grad: tensor([-0.0593,  0.3356])\n",
      "Epoch 1286, Loss 3.268301\n",
      "Params: tensor(0.3273)\n",
      "Grad: tensor([-0.0592,  0.3350])\n",
      "Epoch 1287, Loss 3.267143\n",
      "Params: tensor(0.3273)\n",
      "Grad: tensor([-0.0591,  0.3344])\n",
      "Epoch 1288, Loss 3.265991\n",
      "Params: tensor(0.3273)\n",
      "Grad: tensor([-0.0590,  0.3339])\n",
      "Epoch 1289, Loss 3.264842\n",
      "Params: tensor(0.3272)\n",
      "Grad: tensor([-0.0589,  0.3333])\n",
      "Epoch 1290, Loss 3.263700\n",
      "Params: tensor(0.3272)\n",
      "Grad: tensor([-0.0588,  0.3327])\n",
      "Epoch 1291, Loss 3.262556\n",
      "Params: tensor(0.3272)\n",
      "Grad: tensor([-0.0587,  0.3322])\n",
      "Epoch 1292, Loss 3.261421\n",
      "Params: tensor(0.3271)\n",
      "Grad: tensor([-0.0586,  0.3316])\n",
      "Epoch 1293, Loss 3.260287\n",
      "Params: tensor(0.3271)\n",
      "Grad: tensor([-0.0585,  0.3311])\n",
      "Epoch 1294, Loss 3.259160\n",
      "Params: tensor(0.3271)\n",
      "Grad: tensor([-0.0584,  0.3305])\n",
      "Epoch 1295, Loss 3.258033\n",
      "Params: tensor(0.3270)\n",
      "Grad: tensor([-0.0583,  0.3299])\n",
      "Epoch 1296, Loss 3.256912\n",
      "Params: tensor(0.3270)\n",
      "Grad: tensor([-0.0582,  0.3294])\n",
      "Epoch 1297, Loss 3.255795\n",
      "Params: tensor(0.3270)\n",
      "Grad: tensor([-0.0581,  0.3288])\n",
      "Epoch 1298, Loss 3.254681\n",
      "Params: tensor(0.3269)\n",
      "Grad: tensor([-0.0580,  0.3282])\n",
      "Epoch 1299, Loss 3.253569\n",
      "Params: tensor(0.3269)\n",
      "Grad: tensor([-0.0579,  0.3277])\n",
      "Epoch 1300, Loss 3.252462\n",
      "Params: tensor(0.3269)\n",
      "Grad: tensor([-0.0578,  0.3271])\n",
      "Epoch 1301, Loss 3.251362\n",
      "Params: tensor(0.3268)\n",
      "Grad: tensor([-0.0577,  0.3266])\n",
      "Epoch 1302, Loss 3.250263\n",
      "Params: tensor(0.3268)\n",
      "Grad: tensor([-0.0576,  0.3260])\n",
      "Epoch 1303, Loss 3.249168\n",
      "Params: tensor(0.3268)\n",
      "Grad: tensor([-0.0575,  0.3255])\n",
      "Epoch 1304, Loss 3.248077\n",
      "Params: tensor(0.3267)\n",
      "Grad: tensor([-0.0574,  0.3249])\n",
      "Epoch 1305, Loss 3.246988\n",
      "Params: tensor(0.3267)\n",
      "Grad: tensor([-0.0573,  0.3244])\n",
      "Epoch 1306, Loss 3.245904\n",
      "Params: tensor(0.3267)\n",
      "Grad: tensor([-0.0572,  0.3238])\n",
      "Epoch 1307, Loss 3.244824\n",
      "Params: tensor(0.3267)\n",
      "Grad: tensor([-0.0571,  0.3233])\n",
      "Epoch 1308, Loss 3.243747\n",
      "Params: tensor(0.3266)\n",
      "Grad: tensor([-0.0570,  0.3227])\n",
      "Epoch 1309, Loss 3.242674\n",
      "Params: tensor(0.3266)\n",
      "Grad: tensor([-0.0569,  0.3222])\n",
      "Epoch 1310, Loss 3.241606\n",
      "Params: tensor(0.3266)\n",
      "Grad: tensor([-0.0568,  0.3216])\n",
      "Epoch 1311, Loss 3.240538\n",
      "Params: tensor(0.3265)\n",
      "Grad: tensor([-0.0567,  0.3211])\n",
      "Epoch 1312, Loss 3.239475\n",
      "Params: tensor(0.3265)\n",
      "Grad: tensor([-0.0566,  0.3205])\n",
      "Epoch 1313, Loss 3.238419\n",
      "Params: tensor(0.3265)\n",
      "Grad: tensor([-0.0565,  0.3200])\n",
      "Epoch 1314, Loss 3.237363\n",
      "Params: tensor(0.3264)\n",
      "Grad: tensor([-0.0564,  0.3194])\n",
      "Epoch 1315, Loss 3.236314\n",
      "Params: tensor(0.3264)\n",
      "Grad: tensor([-0.0563,  0.3189])\n",
      "Epoch 1316, Loss 3.235265\n",
      "Params: tensor(0.3264)\n",
      "Grad: tensor([-0.0562,  0.3184])\n",
      "Epoch 1317, Loss 3.234218\n",
      "Params: tensor(0.3263)\n",
      "Grad: tensor([-0.0561,  0.3178])\n",
      "Epoch 1318, Loss 3.233179\n",
      "Params: tensor(0.3263)\n",
      "Grad: tensor([-0.0561,  0.3173])\n",
      "Epoch 1319, Loss 3.232143\n",
      "Params: tensor(0.3263)\n",
      "Grad: tensor([-0.0560,  0.3167])\n",
      "Epoch 1320, Loss 3.231109\n",
      "Params: tensor(0.3263)\n",
      "Grad: tensor([-0.0558,  0.3162])\n",
      "Epoch 1321, Loss 3.230078\n",
      "Params: tensor(0.3262)\n",
      "Grad: tensor([-0.0558,  0.3157])\n",
      "Epoch 1322, Loss 3.229051\n",
      "Params: tensor(0.3262)\n",
      "Grad: tensor([-0.0557,  0.3151])\n",
      "Epoch 1323, Loss 3.228027\n",
      "Params: tensor(0.3262)\n",
      "Grad: tensor([-0.0556,  0.3146])\n",
      "Epoch 1324, Loss 3.227010\n",
      "Params: tensor(0.3261)\n",
      "Grad: tensor([-0.0555,  0.3141])\n",
      "Epoch 1325, Loss 3.225992\n",
      "Params: tensor(0.3261)\n",
      "Grad: tensor([-0.0554,  0.3135])\n",
      "Epoch 1326, Loss 3.224979\n",
      "Params: tensor(0.3261)\n",
      "Grad: tensor([-0.0553,  0.3130])\n",
      "Epoch 1327, Loss 3.223971\n",
      "Params: tensor(0.3260)\n",
      "Grad: tensor([-0.0552,  0.3125])\n",
      "Epoch 1328, Loss 3.222965\n",
      "Params: tensor(0.3260)\n",
      "Grad: tensor([-0.0551,  0.3119])\n",
      "Epoch 1329, Loss 3.221960\n",
      "Params: tensor(0.3260)\n",
      "Grad: tensor([-0.0550,  0.3114])\n",
      "Epoch 1330, Loss 3.220962\n",
      "Params: tensor(0.3259)\n",
      "Grad: tensor([-0.0549,  0.3109])\n",
      "Epoch 1331, Loss 3.219967\n",
      "Params: tensor(0.3259)\n",
      "Grad: tensor([-0.0548,  0.3103])\n",
      "Epoch 1332, Loss 3.218975\n",
      "Params: tensor(0.3259)\n",
      "Grad: tensor([-0.0547,  0.3098])\n",
      "Epoch 1333, Loss 3.217986\n",
      "Params: tensor(0.3259)\n",
      "Grad: tensor([-0.0546,  0.3093])\n",
      "Epoch 1334, Loss 3.217000\n",
      "Params: tensor(0.3258)\n",
      "Grad: tensor([-0.0545,  0.3088])\n",
      "Epoch 1335, Loss 3.216017\n",
      "Params: tensor(0.3258)\n",
      "Grad: tensor([-0.0544,  0.3082])\n",
      "Epoch 1336, Loss 3.215038\n",
      "Params: tensor(0.3258)\n",
      "Grad: tensor([-0.0543,  0.3077])\n",
      "Epoch 1337, Loss 3.214062\n",
      "Params: tensor(0.3257)\n",
      "Grad: tensor([-0.0543,  0.3072])\n",
      "Epoch 1338, Loss 3.213092\n",
      "Params: tensor(0.3257)\n",
      "Grad: tensor([-0.0542,  0.3067])\n",
      "Epoch 1339, Loss 3.212122\n",
      "Params: tensor(0.3257)\n",
      "Grad: tensor([-0.0541,  0.3061])\n",
      "Epoch 1340, Loss 3.211157\n",
      "Params: tensor(0.3257)\n",
      "Grad: tensor([-0.0540,  0.3056])\n",
      "Epoch 1341, Loss 3.210192\n",
      "Params: tensor(0.3256)\n",
      "Grad: tensor([-0.0539,  0.3051])\n",
      "Epoch 1342, Loss 3.209235\n",
      "Params: tensor(0.3256)\n",
      "Grad: tensor([-0.0538,  0.3046])\n",
      "Epoch 1343, Loss 3.208279\n",
      "Params: tensor(0.3256)\n",
      "Grad: tensor([-0.0537,  0.3041])\n",
      "Epoch 1344, Loss 3.207326\n",
      "Params: tensor(0.3255)\n",
      "Grad: tensor([-0.0536,  0.3036])\n",
      "Epoch 1345, Loss 3.206376\n",
      "Params: tensor(0.3255)\n",
      "Grad: tensor([-0.0535,  0.3030])\n",
      "Epoch 1346, Loss 3.205430\n",
      "Params: tensor(0.3255)\n",
      "Grad: tensor([-0.0534,  0.3025])\n",
      "Epoch 1347, Loss 3.204488\n",
      "Params: tensor(0.3254)\n",
      "Grad: tensor([-0.0533,  0.3020])\n",
      "Epoch 1348, Loss 3.203547\n",
      "Params: tensor(0.3254)\n",
      "Grad: tensor([-0.0532,  0.3015])\n",
      "Epoch 1349, Loss 3.202610\n",
      "Params: tensor(0.3254)\n",
      "Grad: tensor([-0.0532,  0.3010])\n",
      "Epoch 1350, Loss 3.201678\n",
      "Params: tensor(0.3254)\n",
      "Grad: tensor([-0.0531,  0.3005])\n",
      "Epoch 1351, Loss 3.200747\n",
      "Params: tensor(0.3253)\n",
      "Grad: tensor([-0.0530,  0.3000])\n",
      "Epoch 1352, Loss 3.199820\n",
      "Params: tensor(0.3253)\n",
      "Grad: tensor([-0.0529,  0.2995])\n",
      "Epoch 1353, Loss 3.198897\n",
      "Params: tensor(0.3253)\n",
      "Grad: tensor([-0.0528,  0.2989])\n",
      "Epoch 1354, Loss 3.197976\n",
      "Params: tensor(0.3252)\n",
      "Grad: tensor([-0.0527,  0.2984])\n",
      "Epoch 1355, Loss 3.197060\n",
      "Params: tensor(0.3252)\n",
      "Grad: tensor([-0.0526,  0.2979])\n",
      "Epoch 1356, Loss 3.196143\n",
      "Params: tensor(0.3252)\n",
      "Grad: tensor([-0.0525,  0.2974])\n",
      "Epoch 1357, Loss 3.195231\n",
      "Params: tensor(0.3252)\n",
      "Grad: tensor([-0.0524,  0.2969])\n",
      "Epoch 1358, Loss 3.194324\n",
      "Params: tensor(0.3251)\n",
      "Grad: tensor([-0.0524,  0.2964])\n",
      "Epoch 1359, Loss 3.193420\n",
      "Params: tensor(0.3251)\n",
      "Grad: tensor([-0.0523,  0.2959])\n",
      "Epoch 1360, Loss 3.192517\n",
      "Params: tensor(0.3251)\n",
      "Grad: tensor([-0.0522,  0.2954])\n",
      "Epoch 1361, Loss 3.191616\n",
      "Params: tensor(0.3250)\n",
      "Grad: tensor([-0.0521,  0.2949])\n",
      "Epoch 1362, Loss 3.190720\n",
      "Params: tensor(0.3250)\n",
      "Grad: tensor([-0.0520,  0.2944])\n",
      "Epoch 1363, Loss 3.189829\n",
      "Params: tensor(0.3250)\n",
      "Grad: tensor([-0.0519,  0.2939])\n",
      "Epoch 1364, Loss 3.188938\n",
      "Params: tensor(0.3250)\n",
      "Grad: tensor([-0.0518,  0.2934])\n",
      "Epoch 1365, Loss 3.188051\n",
      "Params: tensor(0.3249)\n",
      "Grad: tensor([-0.0517,  0.2929])\n",
      "Epoch 1366, Loss 3.187166\n",
      "Params: tensor(0.3249)\n",
      "Grad: tensor([-0.0516,  0.2924])\n",
      "Epoch 1367, Loss 3.186287\n",
      "Params: tensor(0.3249)\n",
      "Grad: tensor([-0.0516,  0.2919])\n",
      "Epoch 1368, Loss 3.185409\n",
      "Params: tensor(0.3249)\n",
      "Grad: tensor([-0.0515,  0.2914])\n",
      "Epoch 1369, Loss 3.184534\n",
      "Params: tensor(0.3248)\n",
      "Grad: tensor([-0.0514,  0.2909])\n",
      "Epoch 1370, Loss 3.183662\n",
      "Params: tensor(0.3248)\n",
      "Grad: tensor([-0.0513,  0.2904])\n",
      "Epoch 1371, Loss 3.182792\n",
      "Params: tensor(0.3248)\n",
      "Grad: tensor([-0.0512,  0.2899])\n",
      "Epoch 1372, Loss 3.181925\n",
      "Params: tensor(0.3247)\n",
      "Grad: tensor([-0.0511,  0.2894])\n",
      "Epoch 1373, Loss 3.181063\n",
      "Params: tensor(0.3247)\n",
      "Grad: tensor([-0.0510,  0.2890])\n",
      "Epoch 1374, Loss 3.180201\n",
      "Params: tensor(0.3247)\n",
      "Grad: tensor([-0.0509,  0.2885])\n",
      "Epoch 1375, Loss 3.179347\n",
      "Params: tensor(0.3247)\n",
      "Grad: tensor([-0.0509,  0.2880])\n",
      "Epoch 1376, Loss 3.178490\n",
      "Params: tensor(0.3246)\n",
      "Grad: tensor([-0.0508,  0.2875])\n",
      "Epoch 1377, Loss 3.177638\n",
      "Params: tensor(0.3246)\n",
      "Grad: tensor([-0.0507,  0.2870])\n",
      "Epoch 1378, Loss 3.176789\n",
      "Params: tensor(0.3246)\n",
      "Grad: tensor([-0.0506,  0.2865])\n",
      "Epoch 1379, Loss 3.175945\n",
      "Params: tensor(0.3246)\n",
      "Grad: tensor([-0.0505,  0.2860])\n",
      "Epoch 1380, Loss 3.175101\n",
      "Params: tensor(0.3245)\n",
      "Grad: tensor([-0.0504,  0.2855])\n",
      "Epoch 1381, Loss 3.174262\n",
      "Params: tensor(0.3245)\n",
      "Grad: tensor([-0.0504,  0.2850])\n",
      "Epoch 1382, Loss 3.173425\n",
      "Params: tensor(0.3245)\n",
      "Grad: tensor([-0.0503,  0.2846])\n",
      "Epoch 1383, Loss 3.172590\n",
      "Params: tensor(0.3244)\n",
      "Grad: tensor([-0.0502,  0.2841])\n",
      "Epoch 1384, Loss 3.171759\n",
      "Params: tensor(0.3244)\n",
      "Grad: tensor([-0.0501,  0.2836])\n",
      "Epoch 1385, Loss 3.170929\n",
      "Params: tensor(0.3244)\n",
      "Grad: tensor([-0.0500,  0.2831])\n",
      "Epoch 1386, Loss 3.170103\n",
      "Params: tensor(0.3244)\n",
      "Grad: tensor([-0.0499,  0.2826])\n",
      "Epoch 1387, Loss 3.169280\n",
      "Params: tensor(0.3243)\n",
      "Grad: tensor([-0.0498,  0.2822])\n",
      "Epoch 1388, Loss 3.168462\n",
      "Params: tensor(0.3243)\n",
      "Grad: tensor([-0.0498,  0.2817])\n",
      "Epoch 1389, Loss 3.167644\n",
      "Params: tensor(0.3243)\n",
      "Grad: tensor([-0.0497,  0.2812])\n",
      "Epoch 1390, Loss 3.166827\n",
      "Params: tensor(0.3243)\n",
      "Grad: tensor([-0.0496,  0.2807])\n",
      "Epoch 1391, Loss 3.166017\n",
      "Params: tensor(0.3242)\n",
      "Grad: tensor([-0.0495,  0.2802])\n",
      "Epoch 1392, Loss 3.165207\n",
      "Params: tensor(0.3242)\n",
      "Grad: tensor([-0.0494,  0.2798])\n",
      "Epoch 1393, Loss 3.164401\n",
      "Params: tensor(0.3242)\n",
      "Grad: tensor([-0.0493,  0.2793])\n",
      "Epoch 1394, Loss 3.163594\n",
      "Params: tensor(0.3242)\n",
      "Grad: tensor([-0.0492,  0.2788])\n",
      "Epoch 1395, Loss 3.162795\n",
      "Params: tensor(0.3241)\n",
      "Grad: tensor([-0.0492,  0.2783])\n",
      "Epoch 1396, Loss 3.161996\n",
      "Params: tensor(0.3241)\n",
      "Grad: tensor([-0.0491,  0.2779])\n",
      "Epoch 1397, Loss 3.161201\n",
      "Params: tensor(0.3241)\n",
      "Grad: tensor([-0.0490,  0.2774])\n",
      "Epoch 1398, Loss 3.160410\n",
      "Params: tensor(0.3240)\n",
      "Grad: tensor([-0.0489,  0.2769])\n",
      "Epoch 1399, Loss 3.159618\n",
      "Params: tensor(0.3240)\n",
      "Grad: tensor([-0.0488,  0.2765])\n",
      "Epoch 1400, Loss 3.158830\n",
      "Params: tensor(0.3240)\n",
      "Grad: tensor([-0.0488,  0.2760])\n",
      "Epoch 1401, Loss 3.158046\n",
      "Params: tensor(0.3240)\n",
      "Grad: tensor([-0.0487,  0.2755])\n",
      "Epoch 1402, Loss 3.157263\n",
      "Params: tensor(0.3239)\n",
      "Grad: tensor([-0.0486,  0.2751])\n",
      "Epoch 1403, Loss 3.156484\n",
      "Params: tensor(0.3239)\n",
      "Grad: tensor([-0.0485,  0.2746])\n",
      "Epoch 1404, Loss 3.155708\n",
      "Params: tensor(0.3239)\n",
      "Grad: tensor([-0.0484,  0.2741])\n",
      "Epoch 1405, Loss 3.154933\n",
      "Params: tensor(0.3239)\n",
      "Grad: tensor([-0.0483,  0.2736])\n",
      "Epoch 1406, Loss 3.154162\n",
      "Params: tensor(0.3238)\n",
      "Grad: tensor([-0.0483,  0.2732])\n",
      "Epoch 1407, Loss 3.153393\n",
      "Params: tensor(0.3238)\n",
      "Grad: tensor([-0.0482,  0.2727])\n",
      "Epoch 1408, Loss 3.152628\n",
      "Params: tensor(0.3238)\n",
      "Grad: tensor([-0.0481,  0.2723])\n",
      "Epoch 1409, Loss 3.151865\n",
      "Params: tensor(0.3238)\n",
      "Grad: tensor([-0.0480,  0.2718])\n",
      "Epoch 1410, Loss 3.151101\n",
      "Params: tensor(0.3237)\n",
      "Grad: tensor([-0.0479,  0.2713])\n",
      "Epoch 1411, Loss 3.150343\n",
      "Params: tensor(0.3237)\n",
      "Grad: tensor([-0.0479,  0.2709])\n",
      "Epoch 1412, Loss 3.149587\n",
      "Params: tensor(0.3237)\n",
      "Grad: tensor([-0.0478,  0.2704])\n",
      "Epoch 1413, Loss 3.148833\n",
      "Params: tensor(0.3237)\n",
      "Grad: tensor([-0.0477,  0.2700])\n",
      "Epoch 1414, Loss 3.148083\n",
      "Params: tensor(0.3236)\n",
      "Grad: tensor([-0.0476,  0.2695])\n",
      "Epoch 1415, Loss 3.147335\n",
      "Params: tensor(0.3236)\n",
      "Grad: tensor([-0.0475,  0.2690])\n",
      "Epoch 1416, Loss 3.146588\n",
      "Params: tensor(0.3236)\n",
      "Grad: tensor([-0.0474,  0.2686])\n",
      "Epoch 1417, Loss 3.145845\n",
      "Params: tensor(0.3236)\n",
      "Grad: tensor([-0.0474,  0.2681])\n",
      "Epoch 1418, Loss 3.145105\n",
      "Params: tensor(0.3235)\n",
      "Grad: tensor([-0.0473,  0.2677])\n",
      "Epoch 1419, Loss 3.144367\n",
      "Params: tensor(0.3235)\n",
      "Grad: tensor([-0.0472,  0.2672])\n",
      "Epoch 1420, Loss 3.143630\n",
      "Params: tensor(0.3235)\n",
      "Grad: tensor([-0.0471,  0.2668])\n",
      "Epoch 1421, Loss 3.142899\n",
      "Params: tensor(0.3235)\n",
      "Grad: tensor([-0.0470,  0.2663])\n",
      "Epoch 1422, Loss 3.142166\n",
      "Params: tensor(0.3234)\n",
      "Grad: tensor([-0.0469,  0.2659])\n",
      "Epoch 1423, Loss 3.141439\n",
      "Params: tensor(0.3234)\n",
      "Grad: tensor([-0.0469,  0.2654])\n",
      "Epoch 1424, Loss 3.140712\n",
      "Params: tensor(0.3234)\n",
      "Grad: tensor([-0.0468,  0.2649])\n",
      "Epoch 1425, Loss 3.139989\n",
      "Params: tensor(0.3234)\n",
      "Grad: tensor([-0.0467,  0.2645])\n",
      "Epoch 1426, Loss 3.139271\n",
      "Params: tensor(0.3233)\n",
      "Grad: tensor([-0.0466,  0.2641])\n",
      "Epoch 1427, Loss 3.138551\n",
      "Params: tensor(0.3233)\n",
      "Grad: tensor([-0.0466,  0.2636])\n",
      "Epoch 1428, Loss 3.137835\n",
      "Params: tensor(0.3233)\n",
      "Grad: tensor([-0.0465,  0.2632])\n",
      "Epoch 1429, Loss 3.137121\n",
      "Params: tensor(0.3233)\n",
      "Grad: tensor([-0.0464,  0.2627])\n",
      "Epoch 1430, Loss 3.136409\n",
      "Params: tensor(0.3232)\n",
      "Grad: tensor([-0.0463,  0.2623])\n",
      "Epoch 1431, Loss 3.135702\n",
      "Params: tensor(0.3232)\n",
      "Grad: tensor([-0.0462,  0.2618])\n",
      "Epoch 1432, Loss 3.134994\n",
      "Params: tensor(0.3232)\n",
      "Grad: tensor([-0.0461,  0.2614])\n",
      "Epoch 1433, Loss 3.134292\n",
      "Params: tensor(0.3232)\n",
      "Grad: tensor([-0.0461,  0.2609])\n",
      "Epoch 1434, Loss 3.133590\n",
      "Params: tensor(0.3231)\n",
      "Grad: tensor([-0.0460,  0.2605])\n",
      "Epoch 1435, Loss 3.132889\n",
      "Params: tensor(0.3231)\n",
      "Grad: tensor([-0.0459,  0.2600])\n",
      "Epoch 1436, Loss 3.132194\n",
      "Params: tensor(0.3231)\n",
      "Grad: tensor([-0.0459,  0.2596])\n",
      "Epoch 1437, Loss 3.131500\n",
      "Params: tensor(0.3231)\n",
      "Grad: tensor([-0.0458,  0.2592])\n",
      "Epoch 1438, Loss 3.130810\n",
      "Params: tensor(0.3230)\n",
      "Grad: tensor([-0.0457,  0.2587])\n",
      "Epoch 1439, Loss 3.130119\n",
      "Params: tensor(0.3230)\n",
      "Grad: tensor([-0.0456,  0.2583])\n",
      "Epoch 1440, Loss 3.129432\n",
      "Params: tensor(0.3230)\n",
      "Grad: tensor([-0.0455,  0.2578])\n",
      "Epoch 1441, Loss 3.128746\n",
      "Params: tensor(0.3230)\n",
      "Grad: tensor([-0.0455,  0.2574])\n",
      "Epoch 1442, Loss 3.128064\n",
      "Params: tensor(0.3230)\n",
      "Grad: tensor([-0.0454,  0.2570])\n",
      "Epoch 1443, Loss 3.127382\n",
      "Params: tensor(0.3229)\n",
      "Grad: tensor([-0.0453,  0.2565])\n",
      "Epoch 1444, Loss 3.126705\n",
      "Params: tensor(0.3229)\n",
      "Grad: tensor([-0.0453,  0.2561])\n",
      "Epoch 1445, Loss 3.126030\n",
      "Params: tensor(0.3229)\n",
      "Grad: tensor([-0.0452,  0.2557])\n",
      "Epoch 1446, Loss 3.125356\n",
      "Params: tensor(0.3229)\n",
      "Grad: tensor([-0.0451,  0.2552])\n",
      "Epoch 1447, Loss 3.124683\n",
      "Params: tensor(0.3228)\n",
      "Grad: tensor([-0.0450,  0.2548])\n",
      "Epoch 1448, Loss 3.124016\n",
      "Params: tensor(0.3228)\n",
      "Grad: tensor([-0.0449,  0.2544])\n",
      "Epoch 1449, Loss 3.123349\n",
      "Params: tensor(0.3228)\n",
      "Grad: tensor([-0.0449,  0.2539])\n",
      "Epoch 1450, Loss 3.122686\n",
      "Params: tensor(0.3228)\n",
      "Grad: tensor([-0.0448,  0.2535])\n",
      "Epoch 1451, Loss 3.122022\n",
      "Params: tensor(0.3227)\n",
      "Grad: tensor([-0.0447,  0.2531])\n",
      "Epoch 1452, Loss 3.121362\n",
      "Params: tensor(0.3227)\n",
      "Grad: tensor([-0.0446,  0.2526])\n",
      "Epoch 1453, Loss 3.120707\n",
      "Params: tensor(0.3227)\n",
      "Grad: tensor([-0.0445,  0.2522])\n",
      "Epoch 1454, Loss 3.120049\n",
      "Params: tensor(0.3227)\n",
      "Grad: tensor([-0.0445,  0.2518])\n",
      "Epoch 1455, Loss 3.119397\n",
      "Params: tensor(0.3226)\n",
      "Grad: tensor([-0.0444,  0.2513])\n",
      "Epoch 1456, Loss 3.118746\n",
      "Params: tensor(0.3226)\n",
      "Grad: tensor([-0.0443,  0.2509])\n",
      "Epoch 1457, Loss 3.118098\n",
      "Params: tensor(0.3226)\n",
      "Grad: tensor([-0.0442,  0.2505])\n",
      "Epoch 1458, Loss 3.117451\n",
      "Params: tensor(0.3226)\n",
      "Grad: tensor([-0.0442,  0.2501])\n",
      "Epoch 1459, Loss 3.116805\n",
      "Params: tensor(0.3226)\n",
      "Grad: tensor([-0.0441,  0.2496])\n",
      "Epoch 1460, Loss 3.116164\n",
      "Params: tensor(0.3225)\n",
      "Grad: tensor([-0.0440,  0.2492])\n",
      "Epoch 1461, Loss 3.115525\n",
      "Params: tensor(0.3225)\n",
      "Grad: tensor([-0.0439,  0.2488])\n",
      "Epoch 1462, Loss 3.114886\n",
      "Params: tensor(0.3225)\n",
      "Grad: tensor([-0.0439,  0.2484])\n",
      "Epoch 1463, Loss 3.114251\n",
      "Params: tensor(0.3225)\n",
      "Grad: tensor([-0.0438,  0.2480])\n",
      "Epoch 1464, Loss 3.113617\n",
      "Params: tensor(0.3224)\n",
      "Grad: tensor([-0.0437,  0.2475])\n",
      "Epoch 1465, Loss 3.112985\n",
      "Params: tensor(0.3224)\n",
      "Grad: tensor([-0.0437,  0.2471])\n",
      "Epoch 1466, Loss 3.112358\n",
      "Params: tensor(0.3224)\n",
      "Grad: tensor([-0.0436,  0.2467])\n",
      "Epoch 1467, Loss 3.111731\n",
      "Params: tensor(0.3224)\n",
      "Grad: tensor([-0.0435,  0.2463])\n",
      "Epoch 1468, Loss 3.111103\n",
      "Params: tensor(0.3224)\n",
      "Grad: tensor([-0.0434,  0.2459])\n",
      "Epoch 1469, Loss 3.110484\n",
      "Params: tensor(0.3223)\n",
      "Grad: tensor([-0.0433,  0.2454])\n",
      "Epoch 1470, Loss 3.109859\n",
      "Params: tensor(0.3223)\n",
      "Grad: tensor([-0.0433,  0.2450])\n",
      "Epoch 1471, Loss 3.109243\n",
      "Params: tensor(0.3223)\n",
      "Grad: tensor([-0.0432,  0.2446])\n",
      "Epoch 1472, Loss 3.108627\n",
      "Params: tensor(0.3223)\n",
      "Grad: tensor([-0.0431,  0.2442])\n",
      "Epoch 1473, Loss 3.108011\n",
      "Params: tensor(0.3222)\n",
      "Grad: tensor([-0.0430,  0.2438])\n",
      "Epoch 1474, Loss 3.107401\n",
      "Params: tensor(0.3222)\n",
      "Grad: tensor([-0.0430,  0.2434])\n",
      "Epoch 1475, Loss 3.106791\n",
      "Params: tensor(0.3222)\n",
      "Grad: tensor([-0.0429,  0.2429])\n",
      "Epoch 1476, Loss 3.106180\n",
      "Params: tensor(0.3222)\n",
      "Grad: tensor([-0.0428,  0.2425])\n",
      "Epoch 1477, Loss 3.105575\n",
      "Params: tensor(0.3222)\n",
      "Grad: tensor([-0.0428,  0.2421])\n",
      "Epoch 1478, Loss 3.104972\n",
      "Params: tensor(0.3221)\n",
      "Grad: tensor([-0.0427,  0.2417])\n",
      "Epoch 1479, Loss 3.104370\n",
      "Params: tensor(0.3221)\n",
      "Grad: tensor([-0.0426,  0.2413])\n",
      "Epoch 1480, Loss 3.103770\n",
      "Params: tensor(0.3221)\n",
      "Grad: tensor([-0.0425,  0.2409])\n",
      "Epoch 1481, Loss 3.103172\n",
      "Params: tensor(0.3221)\n",
      "Grad: tensor([-0.0425,  0.2405])\n",
      "Epoch 1482, Loss 3.102576\n",
      "Params: tensor(0.3220)\n",
      "Grad: tensor([-0.0424,  0.2401])\n",
      "Epoch 1483, Loss 3.101982\n",
      "Params: tensor(0.3220)\n",
      "Grad: tensor([-0.0423,  0.2397])\n",
      "Epoch 1484, Loss 3.101390\n",
      "Params: tensor(0.3220)\n",
      "Grad: tensor([-0.0423,  0.2393])\n",
      "Epoch 1485, Loss 3.100802\n",
      "Params: tensor(0.3220)\n",
      "Grad: tensor([-0.0422,  0.2388])\n",
      "Epoch 1486, Loss 3.100213\n",
      "Params: tensor(0.3220)\n",
      "Grad: tensor([-0.0421,  0.2384])\n",
      "Epoch 1487, Loss 3.099627\n",
      "Params: tensor(0.3219)\n",
      "Grad: tensor([-0.0421,  0.2380])\n",
      "Epoch 1488, Loss 3.099044\n",
      "Params: tensor(0.3219)\n",
      "Grad: tensor([-0.0420,  0.2376])\n",
      "Epoch 1489, Loss 3.098463\n",
      "Params: tensor(0.3219)\n",
      "Grad: tensor([-0.0419,  0.2372])\n",
      "Epoch 1490, Loss 3.097883\n",
      "Params: tensor(0.3219)\n",
      "Grad: tensor([-0.0418,  0.2368])\n",
      "Epoch 1491, Loss 3.097302\n",
      "Params: tensor(0.3218)\n",
      "Grad: tensor([-0.0418,  0.2364])\n",
      "Epoch 1492, Loss 3.096727\n",
      "Params: tensor(0.3218)\n",
      "Grad: tensor([-0.0417,  0.2360])\n",
      "Epoch 1493, Loss 3.096153\n",
      "Params: tensor(0.3218)\n",
      "Grad: tensor([-0.0416,  0.2356])\n",
      "Epoch 1494, Loss 3.095583\n",
      "Params: tensor(0.3218)\n",
      "Grad: tensor([-0.0416,  0.2352])\n",
      "Epoch 1495, Loss 3.095011\n",
      "Params: tensor(0.3218)\n",
      "Grad: tensor([-0.0415,  0.2348])\n",
      "Epoch 1496, Loss 3.094444\n",
      "Params: tensor(0.3217)\n",
      "Grad: tensor([-0.0414,  0.2344])\n",
      "Epoch 1497, Loss 3.093877\n",
      "Params: tensor(0.3217)\n",
      "Grad: tensor([-0.0413,  0.2340])\n",
      "Epoch 1498, Loss 3.093314\n",
      "Params: tensor(0.3217)\n",
      "Grad: tensor([-0.0413,  0.2336])\n",
      "Epoch 1499, Loss 3.092751\n",
      "Params: tensor(0.3217)\n",
      "Grad: tensor([-0.0412,  0.2332])\n",
      "Epoch 1500, Loss 3.092191\n",
      "Params: tensor(0.3217)\n",
      "Grad: tensor([-0.0411,  0.2328])\n",
      "Epoch 1501, Loss 3.091630\n",
      "Params: tensor(0.3216)\n",
      "Grad: tensor([-0.0411,  0.2324])\n",
      "Epoch 1502, Loss 3.091074\n",
      "Params: tensor(0.3216)\n",
      "Grad: tensor([-0.0410,  0.2320])\n",
      "Epoch 1503, Loss 3.090520\n",
      "Params: tensor(0.3216)\n",
      "Grad: tensor([-0.0409,  0.2317])\n",
      "Epoch 1504, Loss 3.089969\n",
      "Params: tensor(0.3216)\n",
      "Grad: tensor([-0.0408,  0.2313])\n",
      "Epoch 1505, Loss 3.089417\n",
      "Params: tensor(0.3215)\n",
      "Grad: tensor([-0.0408,  0.2309])\n",
      "Epoch 1506, Loss 3.088867\n",
      "Params: tensor(0.3215)\n",
      "Grad: tensor([-0.0407,  0.2305])\n",
      "Epoch 1507, Loss 3.088320\n",
      "Params: tensor(0.3215)\n",
      "Grad: tensor([-0.0406,  0.2301])\n",
      "Epoch 1508, Loss 3.087775\n",
      "Params: tensor(0.3215)\n",
      "Grad: tensor([-0.0406,  0.2297])\n",
      "Epoch 1509, Loss 3.087232\n",
      "Params: tensor(0.3215)\n",
      "Grad: tensor([-0.0405,  0.2293])\n",
      "Epoch 1510, Loss 3.086690\n",
      "Params: tensor(0.3214)\n",
      "Grad: tensor([-0.0404,  0.2289])\n",
      "Epoch 1511, Loss 3.086150\n",
      "Params: tensor(0.3214)\n",
      "Grad: tensor([-0.0404,  0.2285])\n",
      "Epoch 1512, Loss 3.085612\n",
      "Params: tensor(0.3214)\n",
      "Grad: tensor([-0.0403,  0.2281])\n",
      "Epoch 1513, Loss 3.085075\n",
      "Params: tensor(0.3214)\n",
      "Grad: tensor([-0.0402,  0.2277])\n",
      "Epoch 1514, Loss 3.084542\n",
      "Params: tensor(0.3214)\n",
      "Grad: tensor([-0.0402,  0.2274])\n",
      "Epoch 1515, Loss 3.084009\n",
      "Params: tensor(0.3213)\n",
      "Grad: tensor([-0.0401,  0.2270])\n",
      "Epoch 1516, Loss 3.083478\n",
      "Params: tensor(0.3213)\n",
      "Grad: tensor([-0.0400,  0.2266])\n",
      "Epoch 1517, Loss 3.082948\n",
      "Params: tensor(0.3213)\n",
      "Grad: tensor([-0.0400,  0.2262])\n",
      "Epoch 1518, Loss 3.082422\n",
      "Params: tensor(0.3213)\n",
      "Grad: tensor([-0.0399,  0.2258])\n",
      "Epoch 1519, Loss 3.081897\n",
      "Params: tensor(0.3213)\n",
      "Grad: tensor([-0.0398,  0.2254])\n",
      "Epoch 1520, Loss 3.081373\n",
      "Params: tensor(0.3212)\n",
      "Grad: tensor([-0.0398,  0.2250])\n",
      "Epoch 1521, Loss 3.080850\n",
      "Params: tensor(0.3212)\n",
      "Grad: tensor([-0.0397,  0.2247])\n",
      "Epoch 1522, Loss 3.080331\n",
      "Params: tensor(0.3212)\n",
      "Grad: tensor([-0.0396,  0.2243])\n",
      "Epoch 1523, Loss 3.079811\n",
      "Params: tensor(0.3212)\n",
      "Grad: tensor([-0.0396,  0.2239])\n",
      "Epoch 1524, Loss 3.079296\n",
      "Params: tensor(0.3212)\n",
      "Grad: tensor([-0.0395,  0.2235])\n",
      "Epoch 1525, Loss 3.078781\n",
      "Params: tensor(0.3211)\n",
      "Grad: tensor([-0.0394,  0.2231])\n",
      "Epoch 1526, Loss 3.078268\n",
      "Params: tensor(0.3211)\n",
      "Grad: tensor([-0.0394,  0.2228])\n",
      "Epoch 1527, Loss 3.077758\n",
      "Params: tensor(0.3211)\n",
      "Grad: tensor([-0.0393,  0.2224])\n",
      "Epoch 1528, Loss 3.077248\n",
      "Params: tensor(0.3211)\n",
      "Grad: tensor([-0.0392,  0.2220])\n",
      "Epoch 1529, Loss 3.076739\n",
      "Params: tensor(0.3211)\n",
      "Grad: tensor([-0.0391,  0.2216])\n",
      "Epoch 1530, Loss 3.076232\n",
      "Params: tensor(0.3210)\n",
      "Grad: tensor([-0.0391,  0.2213])\n",
      "Epoch 1531, Loss 3.075729\n",
      "Params: tensor(0.3210)\n",
      "Grad: tensor([-0.0390,  0.2209])\n",
      "Epoch 1532, Loss 3.075225\n",
      "Params: tensor(0.3210)\n",
      "Grad: tensor([-0.0390,  0.2205])\n",
      "Epoch 1533, Loss 3.074724\n",
      "Params: tensor(0.3210)\n",
      "Grad: tensor([-0.0389,  0.2201])\n",
      "Epoch 1534, Loss 3.074227\n",
      "Params: tensor(0.3210)\n",
      "Grad: tensor([-0.0388,  0.2198])\n",
      "Epoch 1535, Loss 3.073726\n",
      "Params: tensor(0.3209)\n",
      "Grad: tensor([-0.0387,  0.2194])\n",
      "Epoch 1536, Loss 3.073232\n",
      "Params: tensor(0.3209)\n",
      "Grad: tensor([-0.0387,  0.2190])\n",
      "Epoch 1537, Loss 3.072739\n",
      "Params: tensor(0.3209)\n",
      "Grad: tensor([-0.0386,  0.2186])\n",
      "Epoch 1538, Loss 3.072245\n",
      "Params: tensor(0.3209)\n",
      "Grad: tensor([-0.0385,  0.2183])\n",
      "Epoch 1539, Loss 3.071753\n",
      "Params: tensor(0.3209)\n",
      "Grad: tensor([-0.0385,  0.2179])\n",
      "Epoch 1540, Loss 3.071265\n",
      "Params: tensor(0.3208)\n",
      "Grad: tensor([-0.0384,  0.2175])\n",
      "Epoch 1541, Loss 3.070778\n",
      "Params: tensor(0.3208)\n",
      "Grad: tensor([-0.0383,  0.2172])\n",
      "Epoch 1542, Loss 3.070293\n",
      "Params: tensor(0.3208)\n",
      "Grad: tensor([-0.0383,  0.2168])\n",
      "Epoch 1543, Loss 3.069808\n",
      "Params: tensor(0.3208)\n",
      "Grad: tensor([-0.0382,  0.2164])\n",
      "Epoch 1544, Loss 3.069326\n",
      "Params: tensor(0.3208)\n",
      "Grad: tensor([-0.0382,  0.2161])\n",
      "Epoch 1545, Loss 3.068845\n",
      "Params: tensor(0.3207)\n",
      "Grad: tensor([-0.0381,  0.2157])\n",
      "Epoch 1546, Loss 3.068366\n",
      "Params: tensor(0.3207)\n",
      "Grad: tensor([-0.0380,  0.2153])\n",
      "Epoch 1547, Loss 3.067887\n",
      "Params: tensor(0.3207)\n",
      "Grad: tensor([-0.0380,  0.2150])\n",
      "Epoch 1548, Loss 3.067412\n",
      "Params: tensor(0.3207)\n",
      "Grad: tensor([-0.0379,  0.2146])\n",
      "Epoch 1549, Loss 3.066937\n",
      "Params: tensor(0.3207)\n",
      "Grad: tensor([-0.0378,  0.2142])\n",
      "Epoch 1550, Loss 3.066463\n",
      "Params: tensor(0.3206)\n",
      "Grad: tensor([-0.0378,  0.2139])\n",
      "Epoch 1551, Loss 3.065993\n",
      "Params: tensor(0.3206)\n",
      "Grad: tensor([-0.0377,  0.2135])\n",
      "Epoch 1552, Loss 3.065524\n",
      "Params: tensor(0.3206)\n",
      "Grad: tensor([-0.0376,  0.2131])\n",
      "Epoch 1553, Loss 3.065055\n",
      "Params: tensor(0.3206)\n",
      "Grad: tensor([-0.0376,  0.2128])\n",
      "Epoch 1554, Loss 3.064588\n",
      "Params: tensor(0.3206)\n",
      "Grad: tensor([-0.0375,  0.2124])\n",
      "Epoch 1555, Loss 3.064123\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0375,  0.2120])\n",
      "Epoch 1556, Loss 3.063660\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0374,  0.2117])\n",
      "Epoch 1557, Loss 3.063199\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0373,  0.2113])\n",
      "Epoch 1558, Loss 3.062738\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0373,  0.2110])\n",
      "Epoch 1559, Loss 3.062280\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0372,  0.2106])\n",
      "Epoch 1560, Loss 3.061822\n",
      "Params: tensor(0.3205)\n",
      "Grad: tensor([-0.0371,  0.2103])\n",
      "Epoch 1561, Loss 3.061367\n",
      "Params: tensor(0.3204)\n",
      "Grad: tensor([-0.0371,  0.2099])\n",
      "Epoch 1562, Loss 3.060913\n",
      "Params: tensor(0.3204)\n",
      "Grad: tensor([-0.0370,  0.2095])\n",
      "Epoch 1563, Loss 3.060462\n",
      "Params: tensor(0.3204)\n",
      "Grad: tensor([-0.0370,  0.2092])\n",
      "Epoch 1564, Loss 3.060011\n",
      "Params: tensor(0.3204)\n",
      "Grad: tensor([-0.0369,  0.2088])\n",
      "Epoch 1565, Loss 3.059561\n",
      "Params: tensor(0.3204)\n",
      "Grad: tensor([-0.0368,  0.2085])\n",
      "Epoch 1566, Loss 3.059114\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0368,  0.2081])\n",
      "Epoch 1567, Loss 3.058668\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0367,  0.2078])\n",
      "Epoch 1568, Loss 3.058221\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0366,  0.2074])\n",
      "Epoch 1569, Loss 3.057781\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0366,  0.2071])\n",
      "Epoch 1570, Loss 3.057338\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0365,  0.2067])\n",
      "Epoch 1571, Loss 3.056898\n",
      "Params: tensor(0.3203)\n",
      "Grad: tensor([-0.0364,  0.2064])\n",
      "Epoch 1572, Loss 3.056458\n",
      "Params: tensor(0.3202)\n",
      "Grad: tensor([-0.0364,  0.2060])\n",
      "Epoch 1573, Loss 3.056019\n",
      "Params: tensor(0.3202)\n",
      "Grad: tensor([-0.0363,  0.2057])\n",
      "Epoch 1574, Loss 3.055585\n",
      "Params: tensor(0.3202)\n",
      "Grad: tensor([-0.0363,  0.2053])\n",
      "Epoch 1575, Loss 3.055151\n",
      "Params: tensor(0.3202)\n",
      "Grad: tensor([-0.0362,  0.2050])\n",
      "Epoch 1576, Loss 3.054717\n",
      "Params: tensor(0.3202)\n",
      "Grad: tensor([-0.0361,  0.2046])\n",
      "Epoch 1577, Loss 3.054286\n",
      "Params: tensor(0.3201)\n",
      "Grad: tensor([-0.0361,  0.2043])\n",
      "Epoch 1578, Loss 3.053857\n",
      "Params: tensor(0.3201)\n",
      "Grad: tensor([-0.0360,  0.2039])\n",
      "Epoch 1579, Loss 3.053427\n",
      "Params: tensor(0.3201)\n",
      "Grad: tensor([-0.0360,  0.2036])\n",
      "Epoch 1580, Loss 3.053000\n",
      "Params: tensor(0.3201)\n",
      "Grad: tensor([-0.0359,  0.2032])\n",
      "Epoch 1581, Loss 3.052576\n",
      "Params: tensor(0.3201)\n",
      "Grad: tensor([-0.0358,  0.2029])\n",
      "Epoch 1582, Loss 3.052152\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0358,  0.2025])\n",
      "Epoch 1583, Loss 3.051730\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0357,  0.2022])\n",
      "Epoch 1584, Loss 3.051306\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0357,  0.2018])\n",
      "Epoch 1585, Loss 3.050888\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0356,  0.2015])\n",
      "Epoch 1586, Loss 3.050471\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0355,  0.2012])\n",
      "Epoch 1587, Loss 3.050052\n",
      "Params: tensor(0.3200)\n",
      "Grad: tensor([-0.0355,  0.2008])\n",
      "Epoch 1588, Loss 3.049639\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0354,  0.2005])\n",
      "Epoch 1589, Loss 3.049223\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0354,  0.2001])\n",
      "Epoch 1590, Loss 3.048811\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0353,  0.1998])\n",
      "Epoch 1591, Loss 3.048398\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0353,  0.1995])\n",
      "Epoch 1592, Loss 3.047991\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0352,  0.1991])\n",
      "Epoch 1593, Loss 3.047581\n",
      "Params: tensor(0.3199)\n",
      "Grad: tensor([-0.0351,  0.1988])\n",
      "Epoch 1594, Loss 3.047173\n",
      "Params: tensor(0.3198)\n",
      "Grad: tensor([-0.0351,  0.1984])\n",
      "Epoch 1595, Loss 3.046768\n",
      "Params: tensor(0.3198)\n",
      "Grad: tensor([-0.0350,  0.1981])\n",
      "Epoch 1596, Loss 3.046362\n",
      "Params: tensor(0.3198)\n",
      "Grad: tensor([-0.0349,  0.1978])\n",
      "Epoch 1597, Loss 3.045960\n",
      "Params: tensor(0.3198)\n",
      "Grad: tensor([-0.0349,  0.1974])\n",
      "Epoch 1598, Loss 3.045559\n",
      "Params: tensor(0.3198)\n",
      "Grad: tensor([-0.0348,  0.1971])\n",
      "Epoch 1599, Loss 3.045160\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0348,  0.1968])\n",
      "Epoch 1600, Loss 3.044759\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0347,  0.1964])\n",
      "Epoch 1601, Loss 3.044361\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0346,  0.1961])\n",
      "Epoch 1602, Loss 3.043966\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0346,  0.1958])\n",
      "Epoch 1603, Loss 3.043571\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0345,  0.1954])\n",
      "Epoch 1604, Loss 3.043176\n",
      "Params: tensor(0.3197)\n",
      "Grad: tensor([-0.0345,  0.1951])\n",
      "Epoch 1605, Loss 3.042785\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0344,  0.1948])\n",
      "Epoch 1606, Loss 3.042395\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0343,  0.1944])\n",
      "Epoch 1607, Loss 3.042005\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0343,  0.1941])\n",
      "Epoch 1608, Loss 3.041615\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0342,  0.1938])\n",
      "Epoch 1609, Loss 3.041230\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0342,  0.1934])\n",
      "Epoch 1610, Loss 3.040844\n",
      "Params: tensor(0.3196)\n",
      "Grad: tensor([-0.0341,  0.1931])\n",
      "Epoch 1611, Loss 3.040461\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0341,  0.1928])\n",
      "Epoch 1612, Loss 3.040077\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0340,  0.1925])\n",
      "Epoch 1613, Loss 3.039695\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0339,  0.1921])\n",
      "Epoch 1614, Loss 3.039314\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0339,  0.1918])\n",
      "Epoch 1615, Loss 3.038934\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0338,  0.1915])\n",
      "Epoch 1616, Loss 3.038557\n",
      "Params: tensor(0.3195)\n",
      "Grad: tensor([-0.0338,  0.1912])\n",
      "Epoch 1617, Loss 3.038181\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0337,  0.1908])\n",
      "Epoch 1618, Loss 3.037805\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0337,  0.1905])\n",
      "Epoch 1619, Loss 3.037432\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0336,  0.1902])\n",
      "Epoch 1620, Loss 3.037059\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0335,  0.1899])\n",
      "Epoch 1621, Loss 3.036689\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0335,  0.1895])\n",
      "Epoch 1622, Loss 3.036319\n",
      "Params: tensor(0.3194)\n",
      "Grad: tensor([-0.0334,  0.1892])\n",
      "Epoch 1623, Loss 3.035949\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0334,  0.1889])\n",
      "Epoch 1624, Loss 3.035583\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0333,  0.1886])\n",
      "Epoch 1625, Loss 3.035216\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0333,  0.1883])\n",
      "Epoch 1626, Loss 3.034849\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0332,  0.1879])\n",
      "Epoch 1627, Loss 3.034485\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0331,  0.1876])\n",
      "Epoch 1628, Loss 3.034123\n",
      "Params: tensor(0.3193)\n",
      "Grad: tensor([-0.0331,  0.1873])\n",
      "Epoch 1629, Loss 3.033762\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0330,  0.1870])\n",
      "Epoch 1630, Loss 3.033402\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0330,  0.1867])\n",
      "Epoch 1631, Loss 3.033041\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0329,  0.1863])\n",
      "Epoch 1632, Loss 3.032685\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0329,  0.1860])\n",
      "Epoch 1633, Loss 3.032329\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0328,  0.1857])\n",
      "Epoch 1634, Loss 3.031973\n",
      "Params: tensor(0.3192)\n",
      "Grad: tensor([-0.0327,  0.1854])\n",
      "Epoch 1635, Loss 3.031619\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0327,  0.1851])\n",
      "Epoch 1636, Loss 3.031265\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0326,  0.1848])\n",
      "Epoch 1637, Loss 3.030913\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0326,  0.1845])\n",
      "Epoch 1638, Loss 3.030564\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0325,  0.1841])\n",
      "Epoch 1639, Loss 3.030215\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0325,  0.1838])\n",
      "Epoch 1640, Loss 3.029867\n",
      "Params: tensor(0.3191)\n",
      "Grad: tensor([-0.0324,  0.1835])\n",
      "Epoch 1641, Loss 3.029518\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0324,  0.1832])\n",
      "Epoch 1642, Loss 3.029173\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0323,  0.1829])\n",
      "Epoch 1643, Loss 3.028828\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0323,  0.1826])\n",
      "Epoch 1644, Loss 3.028486\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0322,  0.1823])\n",
      "Epoch 1645, Loss 3.028142\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0321,  0.1820])\n",
      "Epoch 1646, Loss 3.027802\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0321,  0.1817])\n",
      "Epoch 1647, Loss 3.027463\n",
      "Params: tensor(0.3190)\n",
      "Grad: tensor([-0.0320,  0.1813])\n",
      "Epoch 1648, Loss 3.027122\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0320,  0.1810])\n",
      "Epoch 1649, Loss 3.026784\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0319,  0.1807])\n",
      "Epoch 1650, Loss 3.026447\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0319,  0.1804])\n",
      "Epoch 1651, Loss 3.026111\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0318,  0.1801])\n",
      "Epoch 1652, Loss 3.025780\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0318,  0.1798])\n",
      "Epoch 1653, Loss 3.025447\n",
      "Params: tensor(0.3189)\n",
      "Grad: tensor([-0.0317,  0.1795])\n",
      "Epoch 1654, Loss 3.025114\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0317,  0.1792])\n",
      "Epoch 1655, Loss 3.024782\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0316,  0.1789])\n",
      "Epoch 1656, Loss 3.024452\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0316,  0.1786])\n",
      "Epoch 1657, Loss 3.024125\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0315,  0.1783])\n",
      "Epoch 1658, Loss 3.023796\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0315,  0.1780])\n",
      "Epoch 1659, Loss 3.023471\n",
      "Params: tensor(0.3188)\n",
      "Grad: tensor([-0.0314,  0.1777])\n",
      "Epoch 1660, Loss 3.023145\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0313,  0.1774])\n",
      "Epoch 1661, Loss 3.022820\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0313,  0.1771])\n",
      "Epoch 1662, Loss 3.022498\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0312,  0.1768])\n",
      "Epoch 1663, Loss 3.022177\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0312,  0.1765])\n",
      "Epoch 1664, Loss 3.021855\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0311,  0.1762])\n",
      "Epoch 1665, Loss 3.021534\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0311,  0.1759])\n",
      "Epoch 1666, Loss 3.021217\n",
      "Params: tensor(0.3187)\n",
      "Grad: tensor([-0.0310,  0.1756])\n",
      "Epoch 1667, Loss 3.020898\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0310,  0.1753])\n",
      "Epoch 1668, Loss 3.020582\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0309,  0.1750])\n",
      "Epoch 1669, Loss 3.020265\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0309,  0.1747])\n",
      "Epoch 1670, Loss 3.019952\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0308,  0.1744])\n",
      "Epoch 1671, Loss 3.019639\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0308,  0.1741])\n",
      "Epoch 1672, Loss 3.019325\n",
      "Params: tensor(0.3186)\n",
      "Grad: tensor([-0.0307,  0.1738])\n",
      "Epoch 1673, Loss 3.019016\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0307,  0.1735])\n",
      "Epoch 1674, Loss 3.018706\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0306,  0.1732])\n",
      "Epoch 1675, Loss 3.018395\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0305,  0.1729])\n",
      "Epoch 1676, Loss 3.018089\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0305,  0.1726])\n",
      "Epoch 1677, Loss 3.017780\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0304,  0.1723])\n",
      "Epoch 1678, Loss 3.017475\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0304,  0.1720])\n",
      "Epoch 1679, Loss 3.017170\n",
      "Params: tensor(0.3185)\n",
      "Grad: tensor([-0.0303,  0.1717])\n",
      "Epoch 1680, Loss 3.016867\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0303,  0.1715])\n",
      "Epoch 1681, Loss 3.016564\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0302,  0.1712])\n",
      "Epoch 1682, Loss 3.016262\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0302,  0.1709])\n",
      "Epoch 1683, Loss 3.015959\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0301,  0.1706])\n",
      "Epoch 1684, Loss 3.015662\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0301,  0.1703])\n",
      "Epoch 1685, Loss 3.015361\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0300,  0.1700])\n",
      "Epoch 1686, Loss 3.015064\n",
      "Params: tensor(0.3184)\n",
      "Grad: tensor([-0.0300,  0.1697])\n",
      "Epoch 1687, Loss 3.014768\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0299,  0.1694])\n",
      "Epoch 1688, Loss 3.014472\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0299,  0.1691])\n",
      "Epoch 1689, Loss 3.014179\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0298,  0.1688])\n",
      "Epoch 1690, Loss 3.013884\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0298,  0.1686])\n",
      "Epoch 1691, Loss 3.013591\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0297,  0.1683])\n",
      "Epoch 1692, Loss 3.013299\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0297,  0.1680])\n",
      "Epoch 1693, Loss 3.013008\n",
      "Params: tensor(0.3183)\n",
      "Grad: tensor([-0.0296,  0.1677])\n",
      "Epoch 1694, Loss 3.012719\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0296,  0.1674])\n",
      "Epoch 1695, Loss 3.012431\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0295,  0.1671])\n",
      "Epoch 1696, Loss 3.012141\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0295,  0.1668])\n",
      "Epoch 1697, Loss 3.011855\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0294,  0.1666])\n",
      "Epoch 1698, Loss 3.011570\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0294,  0.1663])\n",
      "Epoch 1699, Loss 3.011284\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0293,  0.1660])\n",
      "Epoch 1700, Loss 3.011001\n",
      "Params: tensor(0.3182)\n",
      "Grad: tensor([-0.0293,  0.1657])\n",
      "Epoch 1701, Loss 3.010718\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0292,  0.1654])\n",
      "Epoch 1702, Loss 3.010436\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0292,  0.1652])\n",
      "Epoch 1703, Loss 3.010156\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0291,  0.1649])\n",
      "Epoch 1704, Loss 3.009876\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0291,  0.1646])\n",
      "Epoch 1705, Loss 3.009595\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0290,  0.1643])\n",
      "Epoch 1706, Loss 3.009319\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0290,  0.1640])\n",
      "Epoch 1707, Loss 3.009040\n",
      "Params: tensor(0.3181)\n",
      "Grad: tensor([-0.0289,  0.1638])\n",
      "Epoch 1708, Loss 3.008763\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0289,  0.1635])\n",
      "Epoch 1709, Loss 3.008487\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0288,  0.1632])\n",
      "Epoch 1710, Loss 3.008215\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0288,  0.1629])\n",
      "Epoch 1711, Loss 3.007941\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0287,  0.1626])\n",
      "Epoch 1712, Loss 3.007668\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0287,  0.1624])\n",
      "Epoch 1713, Loss 3.007397\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0286,  0.1621])\n",
      "Epoch 1714, Loss 3.007126\n",
      "Params: tensor(0.3180)\n",
      "Grad: tensor([-0.0286,  0.1618])\n",
      "Epoch 1715, Loss 3.006857\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0285,  0.1615])\n",
      "Epoch 1716, Loss 3.006586\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0285,  0.1613])\n",
      "Epoch 1717, Loss 3.006318\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0284,  0.1610])\n",
      "Epoch 1718, Loss 3.006052\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0284,  0.1607])\n",
      "Epoch 1719, Loss 3.005785\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0284,  0.1604])\n",
      "Epoch 1720, Loss 3.005521\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0283,  0.1602])\n",
      "Epoch 1721, Loss 3.005256\n",
      "Params: tensor(0.3179)\n",
      "Grad: tensor([-0.0283,  0.1599])\n",
      "Epoch 1722, Loss 3.004993\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0282,  0.1596])\n",
      "Epoch 1723, Loss 3.004729\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0281,  0.1594])\n",
      "Epoch 1724, Loss 3.004467\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0281,  0.1591])\n",
      "Epoch 1725, Loss 3.004207\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0280,  0.1588])\n",
      "Epoch 1726, Loss 3.003947\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0280,  0.1586])\n",
      "Epoch 1727, Loss 3.003690\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0280,  0.1583])\n",
      "Epoch 1728, Loss 3.003431\n",
      "Params: tensor(0.3178)\n",
      "Grad: tensor([-0.0279,  0.1580])\n",
      "Epoch 1729, Loss 3.003174\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0279,  0.1577])\n",
      "Epoch 1730, Loss 3.002918\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0278,  0.1575])\n",
      "Epoch 1731, Loss 3.002661\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0278,  0.1572])\n",
      "Epoch 1732, Loss 3.002406\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0277,  0.1569])\n",
      "Epoch 1733, Loss 3.002152\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0277,  0.1567])\n",
      "Epoch 1734, Loss 3.001901\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0276,  0.1564])\n",
      "Epoch 1735, Loss 3.001649\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0276,  0.1561])\n",
      "Epoch 1736, Loss 3.001395\n",
      "Params: tensor(0.3177)\n",
      "Grad: tensor([-0.0275,  0.1559])\n",
      "Epoch 1737, Loss 3.001145\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0275,  0.1556])\n",
      "Epoch 1738, Loss 3.000898\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0274,  0.1553])\n",
      "Epoch 1739, Loss 3.000648\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0274,  0.1551])\n",
      "Epoch 1740, Loss 3.000400\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0273,  0.1548])\n",
      "Epoch 1741, Loss 3.000154\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0273,  0.1546])\n",
      "Epoch 1742, Loss 2.999907\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0273,  0.1543])\n",
      "Epoch 1743, Loss 2.999662\n",
      "Params: tensor(0.3176)\n",
      "Grad: tensor([-0.0272,  0.1540])\n",
      "Epoch 1744, Loss 2.999417\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0272,  0.1538])\n",
      "Epoch 1745, Loss 2.999174\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0271,  0.1535])\n",
      "Epoch 1746, Loss 2.998930\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0271,  0.1533])\n",
      "Epoch 1747, Loss 2.998688\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0270,  0.1530])\n",
      "Epoch 1748, Loss 2.998448\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0270,  0.1527])\n",
      "Epoch 1749, Loss 2.998208\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0269,  0.1525])\n",
      "Epoch 1750, Loss 2.997968\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0269,  0.1522])\n",
      "Epoch 1751, Loss 2.997730\n",
      "Params: tensor(0.3175)\n",
      "Grad: tensor([-0.0268,  0.1520])\n",
      "Epoch 1752, Loss 2.997490\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0268,  0.1517])\n",
      "Epoch 1753, Loss 2.997254\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0267,  0.1514])\n",
      "Epoch 1754, Loss 2.997018\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0267,  0.1512])\n",
      "Epoch 1755, Loss 2.996782\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0266,  0.1509])\n",
      "Epoch 1756, Loss 2.996548\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0266,  0.1507])\n",
      "Epoch 1757, Loss 2.996313\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0266,  0.1504])\n",
      "Epoch 1758, Loss 2.996081\n",
      "Params: tensor(0.3174)\n",
      "Grad: tensor([-0.0265,  0.1502])\n",
      "Epoch 1759, Loss 2.995847\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0265,  0.1499])\n",
      "Epoch 1760, Loss 2.995615\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0264,  0.1496])\n",
      "Epoch 1761, Loss 2.995387\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0264,  0.1494])\n",
      "Epoch 1762, Loss 2.995156\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0263,  0.1491])\n",
      "Epoch 1763, Loss 2.994928\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0263,  0.1489])\n",
      "Epoch 1764, Loss 2.994699\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0263,  0.1486])\n",
      "Epoch 1765, Loss 2.994471\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0262,  0.1484])\n",
      "Epoch 1766, Loss 2.994245\n",
      "Params: tensor(0.3173)\n",
      "Grad: tensor([-0.0262,  0.1481])\n",
      "Epoch 1767, Loss 2.994019\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0261,  0.1479])\n",
      "Epoch 1768, Loss 2.993794\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0261,  0.1476])\n",
      "Epoch 1769, Loss 2.993569\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0260,  0.1474])\n",
      "Epoch 1770, Loss 2.993344\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0260,  0.1471])\n",
      "Epoch 1771, Loss 2.993121\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0260,  0.1469])\n",
      "Epoch 1772, Loss 2.992900\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0259,  0.1466])\n",
      "Epoch 1773, Loss 2.992678\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0259,  0.1464])\n",
      "Epoch 1774, Loss 2.992457\n",
      "Params: tensor(0.3172)\n",
      "Grad: tensor([-0.0258,  0.1461])\n",
      "Epoch 1775, Loss 2.992237\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0258,  0.1459])\n",
      "Epoch 1776, Loss 2.992017\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0257,  0.1456])\n",
      "Epoch 1777, Loss 2.991798\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0257,  0.1454])\n",
      "Epoch 1778, Loss 2.991582\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0256,  0.1451])\n",
      "Epoch 1779, Loss 2.991366\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0256,  0.1449])\n",
      "Epoch 1780, Loss 2.991146\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0256,  0.1446])\n",
      "Epoch 1781, Loss 2.990932\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0255,  0.1444])\n",
      "Epoch 1782, Loss 2.990719\n",
      "Params: tensor(0.3171)\n",
      "Grad: tensor([-0.0255,  0.1442])\n",
      "Epoch 1783, Loss 2.990503\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0254,  0.1439])\n",
      "Epoch 1784, Loss 2.990288\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0254,  0.1437])\n",
      "Epoch 1785, Loss 2.990078\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0253,  0.1434])\n",
      "Epoch 1786, Loss 2.989866\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0253,  0.1432])\n",
      "Epoch 1787, Loss 2.989655\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0252,  0.1429])\n",
      "Epoch 1788, Loss 2.989443\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0252,  0.1427])\n",
      "Epoch 1789, Loss 2.989233\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0252,  0.1424])\n",
      "Epoch 1790, Loss 2.989025\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0251,  0.1422])\n",
      "Epoch 1791, Loss 2.988817\n",
      "Params: tensor(0.3170)\n",
      "Grad: tensor([-0.0251,  0.1420])\n",
      "Epoch 1792, Loss 2.988609\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0250,  0.1417])\n",
      "Epoch 1793, Loss 2.988401\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0250,  0.1415])\n",
      "Epoch 1794, Loss 2.988195\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0249,  0.1412])\n",
      "Epoch 1795, Loss 2.987989\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0249,  0.1410])\n",
      "Epoch 1796, Loss 2.987785\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0249,  0.1408])\n",
      "Epoch 1797, Loss 2.987582\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0248,  0.1405])\n",
      "Epoch 1798, Loss 2.987377\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0248,  0.1403])\n",
      "Epoch 1799, Loss 2.987174\n",
      "Params: tensor(0.3169)\n",
      "Grad: tensor([-0.0247,  0.1400])\n",
      "Epoch 1800, Loss 2.986974\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0247,  0.1398])\n",
      "Epoch 1801, Loss 2.986771\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0246,  0.1396])\n",
      "Epoch 1802, Loss 2.986570\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0246,  0.1393])\n",
      "Epoch 1803, Loss 2.986371\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0246,  0.1391])\n",
      "Epoch 1804, Loss 2.986171\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0245,  0.1389])\n",
      "Epoch 1805, Loss 2.985972\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0245,  0.1386])\n",
      "Epoch 1806, Loss 2.985774\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0245,  0.1384])\n",
      "Epoch 1807, Loss 2.985578\n",
      "Params: tensor(0.3168)\n",
      "Grad: tensor([-0.0244,  0.1382])\n",
      "Epoch 1808, Loss 2.985381\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0244,  0.1379])\n",
      "Epoch 1809, Loss 2.985184\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0243,  0.1377])\n",
      "Epoch 1810, Loss 2.984989\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0243,  0.1374])\n",
      "Epoch 1811, Loss 2.984793\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0243,  0.1372])\n",
      "Epoch 1812, Loss 2.984601\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0242,  0.1370])\n",
      "Epoch 1813, Loss 2.984407\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0242,  0.1368])\n",
      "Epoch 1814, Loss 2.984215\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0241,  0.1365])\n",
      "Epoch 1815, Loss 2.984022\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0241,  0.1363])\n",
      "Epoch 1816, Loss 2.983831\n",
      "Params: tensor(0.3167)\n",
      "Grad: tensor([-0.0240,  0.1361])\n",
      "Epoch 1817, Loss 2.983639\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0240,  0.1358])\n",
      "Epoch 1818, Loss 2.983449\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0239,  0.1356])\n",
      "Epoch 1819, Loss 2.983259\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0239,  0.1354])\n",
      "Epoch 1820, Loss 2.983073\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0239,  0.1351])\n",
      "Epoch 1821, Loss 2.982884\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0238,  0.1349])\n",
      "Epoch 1822, Loss 2.982697\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0238,  0.1347])\n",
      "Epoch 1823, Loss 2.982510\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0237,  0.1344])\n",
      "Epoch 1824, Loss 2.982322\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0237,  0.1342])\n",
      "Epoch 1825, Loss 2.982137\n",
      "Params: tensor(0.3166)\n",
      "Grad: tensor([-0.0237,  0.1340])\n",
      "Epoch 1826, Loss 2.981953\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0236,  0.1338])\n",
      "Epoch 1827, Loss 2.981769\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0236,  0.1335])\n",
      "Epoch 1828, Loss 2.981586\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0236,  0.1333])\n",
      "Epoch 1829, Loss 2.981402\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0235,  0.1331])\n",
      "Epoch 1830, Loss 2.981219\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0235,  0.1329])\n",
      "Epoch 1831, Loss 2.981037\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0235,  0.1326])\n",
      "Epoch 1832, Loss 2.980856\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0234,  0.1324])\n",
      "Epoch 1833, Loss 2.980675\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0234,  0.1322])\n",
      "Epoch 1834, Loss 2.980495\n",
      "Params: tensor(0.3165)\n",
      "Grad: tensor([-0.0233,  0.1320])\n",
      "Epoch 1835, Loss 2.980315\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0233,  0.1317])\n",
      "Epoch 1836, Loss 2.980137\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0232,  0.1315])\n",
      "Epoch 1837, Loss 2.979958\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0232,  0.1313])\n",
      "Epoch 1838, Loss 2.979782\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0232,  0.1311])\n",
      "Epoch 1839, Loss 2.979604\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0231,  0.1308])\n",
      "Epoch 1840, Loss 2.979428\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0231,  0.1306])\n",
      "Epoch 1841, Loss 2.979253\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0230,  0.1304])\n",
      "Epoch 1842, Loss 2.979078\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0230,  0.1302])\n",
      "Epoch 1843, Loss 2.978902\n",
      "Params: tensor(0.3164)\n",
      "Grad: tensor([-0.0229,  0.1300])\n",
      "Epoch 1844, Loss 2.978729\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0229,  0.1297])\n",
      "Epoch 1845, Loss 2.978556\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0229,  0.1295])\n",
      "Epoch 1846, Loss 2.978382\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0228,  0.1293])\n",
      "Epoch 1847, Loss 2.978211\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0228,  0.1291])\n",
      "Epoch 1848, Loss 2.978039\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0228,  0.1288])\n",
      "Epoch 1849, Loss 2.977867\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0227,  0.1286])\n",
      "Epoch 1850, Loss 2.977696\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0227,  0.1284])\n",
      "Epoch 1851, Loss 2.977527\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0227,  0.1282])\n",
      "Epoch 1852, Loss 2.977357\n",
      "Params: tensor(0.3163)\n",
      "Grad: tensor([-0.0226,  0.1280])\n",
      "Epoch 1853, Loss 2.977188\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0226,  0.1278])\n",
      "Epoch 1854, Loss 2.977021\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0225,  0.1275])\n",
      "Epoch 1855, Loss 2.976853\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0225,  0.1273])\n",
      "Epoch 1856, Loss 2.976687\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0225,  0.1271])\n",
      "Epoch 1857, Loss 2.976520\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0224,  0.1269])\n",
      "Epoch 1858, Loss 2.976354\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0224,  0.1267])\n",
      "Epoch 1859, Loss 2.976189\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0223,  0.1265])\n",
      "Epoch 1860, Loss 2.976023\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0223,  0.1263])\n",
      "Epoch 1861, Loss 2.975860\n",
      "Params: tensor(0.3162)\n",
      "Grad: tensor([-0.0223,  0.1260])\n",
      "Epoch 1862, Loss 2.975697\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0222,  0.1258])\n",
      "Epoch 1863, Loss 2.975533\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0222,  0.1256])\n",
      "Epoch 1864, Loss 2.975369\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0222,  0.1254])\n",
      "Epoch 1865, Loss 2.975208\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0221,  0.1252])\n",
      "Epoch 1866, Loss 2.975046\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0221,  0.1250])\n",
      "Epoch 1867, Loss 2.974886\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0220,  0.1248])\n",
      "Epoch 1868, Loss 2.974725\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0220,  0.1245])\n",
      "Epoch 1869, Loss 2.974565\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0220,  0.1243])\n",
      "Epoch 1870, Loss 2.974406\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0219,  0.1241])\n",
      "Epoch 1871, Loss 2.974248\n",
      "Params: tensor(0.3161)\n",
      "Grad: tensor([-0.0219,  0.1239])\n",
      "Epoch 1872, Loss 2.974088\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0219,  0.1237])\n",
      "Epoch 1873, Loss 2.973930\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0218,  0.1235])\n",
      "Epoch 1874, Loss 2.973776\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0218,  0.1233])\n",
      "Epoch 1875, Loss 2.973618\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0217,  0.1231])\n",
      "Epoch 1876, Loss 2.973463\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0217,  0.1229])\n",
      "Epoch 1877, Loss 2.973307\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0217,  0.1227])\n",
      "Epoch 1878, Loss 2.973151\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0216,  0.1224])\n",
      "Epoch 1879, Loss 2.972996\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0216,  0.1222])\n",
      "Epoch 1880, Loss 2.972843\n",
      "Params: tensor(0.3160)\n",
      "Grad: tensor([-0.0215,  0.1220])\n",
      "Epoch 1881, Loss 2.972690\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0215,  0.1218])\n",
      "Epoch 1882, Loss 2.972536\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0215,  0.1216])\n",
      "Epoch 1883, Loss 2.972383\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0214,  0.1214])\n",
      "Epoch 1884, Loss 2.972232\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0214,  0.1212])\n",
      "Epoch 1885, Loss 2.972081\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0214,  0.1210])\n",
      "Epoch 1886, Loss 2.971931\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0213,  0.1208])\n",
      "Epoch 1887, Loss 2.971780\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0213,  0.1206])\n",
      "Epoch 1888, Loss 2.971630\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0213,  0.1204])\n",
      "Epoch 1889, Loss 2.971481\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0212,  0.1202])\n",
      "Epoch 1890, Loss 2.971332\n",
      "Params: tensor(0.3159)\n",
      "Grad: tensor([-0.0212,  0.1200])\n",
      "Epoch 1891, Loss 2.971184\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0212,  0.1198])\n",
      "Epoch 1892, Loss 2.971035\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0211,  0.1196])\n",
      "Epoch 1893, Loss 2.970888\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0211,  0.1194])\n",
      "Epoch 1894, Loss 2.970741\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0211,  0.1192])\n",
      "Epoch 1895, Loss 2.970596\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0210,  0.1190])\n",
      "Epoch 1896, Loss 2.970449\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0210,  0.1188])\n",
      "Epoch 1897, Loss 2.970304\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0209,  0.1186])\n",
      "Epoch 1898, Loss 2.970159\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0209,  0.1183])\n",
      "Epoch 1899, Loss 2.970016\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0209,  0.1182])\n",
      "Epoch 1900, Loss 2.969871\n",
      "Params: tensor(0.3158)\n",
      "Grad: tensor([-0.0208,  0.1180])\n",
      "Epoch 1901, Loss 2.969727\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0208,  0.1178])\n",
      "Epoch 1902, Loss 2.969586\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0208,  0.1175])\n",
      "Epoch 1903, Loss 2.969443\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0207,  0.1173])\n",
      "Epoch 1904, Loss 2.969302\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0207,  0.1172])\n",
      "Epoch 1905, Loss 2.969160\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0206,  0.1170])\n",
      "Epoch 1906, Loss 2.969017\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0206,  0.1168])\n",
      "Epoch 1907, Loss 2.968879\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0206,  0.1166])\n",
      "Epoch 1908, Loss 2.968739\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0205,  0.1164])\n",
      "Epoch 1909, Loss 2.968599\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0205,  0.1162])\n",
      "Epoch 1910, Loss 2.968460\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0205,  0.1160])\n",
      "Epoch 1911, Loss 2.968321\n",
      "Params: tensor(0.3157)\n",
      "Grad: tensor([-0.0204,  0.1158])\n",
      "Epoch 1912, Loss 2.968183\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0204,  0.1156])\n",
      "Epoch 1913, Loss 2.968046\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0204,  0.1154])\n",
      "Epoch 1914, Loss 2.967908\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0204,  0.1152])\n",
      "Epoch 1915, Loss 2.967772\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0203,  0.1150])\n",
      "Epoch 1916, Loss 2.967636\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0203,  0.1148])\n",
      "Epoch 1917, Loss 2.967499\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0202,  0.1146])\n",
      "Epoch 1918, Loss 2.967365\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0202,  0.1144])\n",
      "Epoch 1919, Loss 2.967230\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0202,  0.1142])\n",
      "Epoch 1920, Loss 2.967095\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0202,  0.1140])\n",
      "Epoch 1921, Loss 2.966961\n",
      "Params: tensor(0.3156)\n",
      "Grad: tensor([-0.0201,  0.1138])\n",
      "Epoch 1922, Loss 2.966828\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0201,  0.1136])\n",
      "Epoch 1923, Loss 2.966693\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0200,  0.1134])\n",
      "Epoch 1924, Loss 2.966561\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0200,  0.1132])\n",
      "Epoch 1925, Loss 2.966429\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0200,  0.1130])\n",
      "Epoch 1926, Loss 2.966297\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0199,  0.1128])\n",
      "Epoch 1927, Loss 2.966168\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0199,  0.1127])\n",
      "Epoch 1928, Loss 2.966036\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0199,  0.1125])\n",
      "Epoch 1929, Loss 2.965904\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0198,  0.1123])\n",
      "Epoch 1930, Loss 2.965777\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0198,  0.1121])\n",
      "Epoch 1931, Loss 2.965647\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0198,  0.1119])\n",
      "Epoch 1932, Loss 2.965516\n",
      "Params: tensor(0.3155)\n",
      "Grad: tensor([-0.0197,  0.1117])\n",
      "Epoch 1933, Loss 2.965388\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0197,  0.1115])\n",
      "Epoch 1934, Loss 2.965261\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0197,  0.1113])\n",
      "Epoch 1935, Loss 2.965131\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0196,  0.1111])\n",
      "Epoch 1936, Loss 2.965006\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0196,  0.1109])\n",
      "Epoch 1937, Loss 2.964877\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0196,  0.1108])\n",
      "Epoch 1938, Loss 2.964751\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0195,  0.1106])\n",
      "Epoch 1939, Loss 2.964625\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0195,  0.1104])\n",
      "Epoch 1940, Loss 2.964500\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0195,  0.1102])\n",
      "Epoch 1941, Loss 2.964375\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0195,  0.1100])\n",
      "Epoch 1942, Loss 2.964250\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0194,  0.1098])\n",
      "Epoch 1943, Loss 2.964126\n",
      "Params: tensor(0.3154)\n",
      "Grad: tensor([-0.0194,  0.1096])\n",
      "Epoch 1944, Loss 2.964001\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0194,  0.1094])\n",
      "Epoch 1945, Loss 2.963879\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0193,  0.1093])\n",
      "Epoch 1946, Loss 2.963756\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0193,  0.1091])\n",
      "Epoch 1947, Loss 2.963632\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0192,  0.1089])\n",
      "Epoch 1948, Loss 2.963511\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0192,  0.1087])\n",
      "Epoch 1949, Loss 2.963388\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0192,  0.1085])\n",
      "Epoch 1950, Loss 2.963266\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0191,  0.1083])\n",
      "Epoch 1951, Loss 2.963149\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0191,  0.1081])\n",
      "Epoch 1952, Loss 2.963026\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0191,  0.1080])\n",
      "Epoch 1953, Loss 2.962907\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0190,  0.1078])\n",
      "Epoch 1954, Loss 2.962788\n",
      "Params: tensor(0.3153)\n",
      "Grad: tensor([-0.0190,  0.1076])\n",
      "Epoch 1955, Loss 2.962667\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0190,  0.1074])\n",
      "Epoch 1956, Loss 2.962547\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0189,  0.1072])\n",
      "Epoch 1957, Loss 2.962429\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0189,  0.1071])\n",
      "Epoch 1958, Loss 2.962312\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0189,  0.1069])\n",
      "Epoch 1959, Loss 2.962195\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0188,  0.1067])\n",
      "Epoch 1960, Loss 2.962078\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0188,  0.1065])\n",
      "Epoch 1961, Loss 2.961959\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0188,  0.1063])\n",
      "Epoch 1962, Loss 2.961843\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0187,  0.1062])\n",
      "Epoch 1963, Loss 2.961728\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0187,  0.1060])\n",
      "Epoch 1964, Loss 2.961611\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0187,  0.1058])\n",
      "Epoch 1965, Loss 2.961496\n",
      "Params: tensor(0.3152)\n",
      "Grad: tensor([-0.0187,  0.1056])\n",
      "Epoch 1966, Loss 2.961382\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0186,  0.1054])\n",
      "Epoch 1967, Loss 2.961267\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0186,  0.1052])\n",
      "Epoch 1968, Loss 2.961153\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0186,  0.1051])\n",
      "Epoch 1969, Loss 2.961038\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0185,  0.1049])\n",
      "Epoch 1970, Loss 2.960926\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0185,  0.1047])\n",
      "Epoch 1971, Loss 2.960813\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0185,  0.1045])\n",
      "Epoch 1972, Loss 2.960700\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0184,  0.1044])\n",
      "Epoch 1973, Loss 2.960587\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0184,  0.1042])\n",
      "Epoch 1974, Loss 2.960475\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0184,  0.1040])\n",
      "Epoch 1975, Loss 2.960365\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0183,  0.1038])\n",
      "Epoch 1976, Loss 2.960255\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0183,  0.1037])\n",
      "Epoch 1977, Loss 2.960143\n",
      "Params: tensor(0.3151)\n",
      "Grad: tensor([-0.0183,  0.1035])\n",
      "Epoch 1978, Loss 2.960033\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0182,  0.1033])\n",
      "Epoch 1979, Loss 2.959923\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0182,  0.1031])\n",
      "Epoch 1980, Loss 2.959812\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0182,  0.1029])\n",
      "Epoch 1981, Loss 2.959703\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0182,  0.1028])\n",
      "Epoch 1982, Loss 2.959594\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0181,  0.1026])\n",
      "Epoch 1983, Loss 2.959486\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0181,  0.1024])\n",
      "Epoch 1984, Loss 2.959378\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0181,  0.1022])\n",
      "Epoch 1985, Loss 2.959271\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0180,  0.1021])\n",
      "Epoch 1986, Loss 2.959162\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0180,  0.1019])\n",
      "Epoch 1987, Loss 2.959055\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0180,  0.1017])\n",
      "Epoch 1988, Loss 2.958950\n",
      "Params: tensor(0.3150)\n",
      "Grad: tensor([-0.0179,  0.1016])\n",
      "Epoch 1989, Loss 2.958842\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0179,  0.1014])\n",
      "Epoch 1990, Loss 2.958738\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0179,  0.1012])\n",
      "Epoch 1991, Loss 2.958632\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0179,  0.1010])\n",
      "Epoch 1992, Loss 2.958526\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0178,  0.1009])\n",
      "Epoch 1993, Loss 2.958422\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0178,  0.1007])\n",
      "Epoch 1994, Loss 2.958317\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0178,  0.1005])\n",
      "Epoch 1995, Loss 2.958212\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0177,  0.1004])\n",
      "Epoch 1996, Loss 2.958109\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0177,  0.1002])\n",
      "Epoch 1997, Loss 2.958006\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0176,  0.1000])\n",
      "Epoch 1998, Loss 2.957904\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0176,  0.0998])\n",
      "Epoch 1999, Loss 2.957801\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0176,  0.0997])\n",
      "Epoch 2000, Loss 2.957698\n",
      "Params: tensor(0.3149)\n",
      "Grad: tensor([-0.0176,  0.0995])\n",
      "Epoch 2001, Loss 2.957596\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0176,  0.0993])\n",
      "Epoch 2002, Loss 2.957494\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0175,  0.0992])\n",
      "Epoch 2003, Loss 2.957393\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0175,  0.0990])\n",
      "Epoch 2004, Loss 2.957292\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0174,  0.0988])\n",
      "Epoch 2005, Loss 2.957193\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0174,  0.0987])\n",
      "Epoch 2006, Loss 2.957091\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0174,  0.0985])\n",
      "Epoch 2007, Loss 2.956992\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0174,  0.0983])\n",
      "Epoch 2008, Loss 2.956892\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0173,  0.0982])\n",
      "Epoch 2009, Loss 2.956792\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0173,  0.0980])\n",
      "Epoch 2010, Loss 2.956694\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0173,  0.0978])\n",
      "Epoch 2011, Loss 2.956595\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0172,  0.0977])\n",
      "Epoch 2012, Loss 2.956496\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0172,  0.0975])\n",
      "Epoch 2013, Loss 2.956397\n",
      "Params: tensor(0.3148)\n",
      "Grad: tensor([-0.0172,  0.0973])\n",
      "Epoch 2014, Loss 2.956300\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0172,  0.0972])\n",
      "Epoch 2015, Loss 2.956204\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0171,  0.0970])\n",
      "Epoch 2016, Loss 2.956108\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0171,  0.0968])\n",
      "Epoch 2017, Loss 2.956010\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0171,  0.0967])\n",
      "Epoch 2018, Loss 2.955914\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0171,  0.0965])\n",
      "Epoch 2019, Loss 2.955817\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0170,  0.0963])\n",
      "Epoch 2020, Loss 2.955722\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0170,  0.0962])\n",
      "Epoch 2021, Loss 2.955627\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0170,  0.0960])\n",
      "Epoch 2022, Loss 2.955533\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0169,  0.0959])\n",
      "Epoch 2023, Loss 2.955436\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0169,  0.0957])\n",
      "Epoch 2024, Loss 2.955343\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0169,  0.0955])\n",
      "Epoch 2025, Loss 2.955250\n",
      "Params: tensor(0.3147)\n",
      "Grad: tensor([-0.0169,  0.0954])\n",
      "Epoch 2026, Loss 2.955154\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0168,  0.0952])\n",
      "Epoch 2027, Loss 2.955062\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0168,  0.0950])\n",
      "Epoch 2028, Loss 2.954969\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0168,  0.0949])\n",
      "Epoch 2029, Loss 2.954875\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0167,  0.0947])\n",
      "Epoch 2030, Loss 2.954783\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0167,  0.0946])\n",
      "Epoch 2031, Loss 2.954691\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0167,  0.0944])\n",
      "Epoch 2032, Loss 2.954600\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0167,  0.0942])\n",
      "Epoch 2033, Loss 2.954507\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0166,  0.0941])\n",
      "Epoch 2034, Loss 2.954417\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0166,  0.0939])\n",
      "Epoch 2035, Loss 2.954326\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0165,  0.0938])\n",
      "Epoch 2036, Loss 2.954235\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0165,  0.0936])\n",
      "Epoch 2037, Loss 2.954145\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0165,  0.0934])\n",
      "Epoch 2038, Loss 2.954055\n",
      "Params: tensor(0.3146)\n",
      "Grad: tensor([-0.0165,  0.0933])\n",
      "Epoch 2039, Loss 2.953966\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0164,  0.0931])\n",
      "Epoch 2040, Loss 2.953876\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0164,  0.0930])\n",
      "Epoch 2041, Loss 2.953787\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0164,  0.0928])\n",
      "Epoch 2042, Loss 2.953698\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0164,  0.0926])\n",
      "Epoch 2043, Loss 2.953610\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0163,  0.0925])\n",
      "Epoch 2044, Loss 2.953521\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0163,  0.0923])\n",
      "Epoch 2045, Loss 2.953434\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0163,  0.0922])\n",
      "Epoch 2046, Loss 2.953346\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0163,  0.0920])\n",
      "Epoch 2047, Loss 2.953259\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0162,  0.0919])\n",
      "Epoch 2048, Loss 2.953171\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0162,  0.0917])\n",
      "Epoch 2049, Loss 2.953085\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0162,  0.0915])\n",
      "Epoch 2050, Loss 2.953000\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0162,  0.0914])\n",
      "Epoch 2051, Loss 2.952913\n",
      "Params: tensor(0.3145)\n",
      "Grad: tensor([-0.0161,  0.0912])\n",
      "Epoch 2052, Loss 2.952828\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0161,  0.0911])\n",
      "Epoch 2053, Loss 2.952742\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0161,  0.0909])\n",
      "Epoch 2054, Loss 2.952657\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0160,  0.0908])\n",
      "Epoch 2055, Loss 2.952571\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0160,  0.0906])\n",
      "Epoch 2056, Loss 2.952487\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0160,  0.0905])\n",
      "Epoch 2057, Loss 2.952403\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0160,  0.0903])\n",
      "Epoch 2058, Loss 2.952318\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0159,  0.0902])\n",
      "Epoch 2059, Loss 2.952235\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0159,  0.0900])\n",
      "Epoch 2060, Loss 2.952152\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0159,  0.0899])\n",
      "Epoch 2061, Loss 2.952068\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0158,  0.0897])\n",
      "Epoch 2062, Loss 2.951985\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0158,  0.0895])\n",
      "Epoch 2063, Loss 2.951902\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0158,  0.0894])\n",
      "Epoch 2064, Loss 2.951820\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0158,  0.0892])\n",
      "Epoch 2065, Loss 2.951738\n",
      "Params: tensor(0.3144)\n",
      "Grad: tensor([-0.0157,  0.0891])\n",
      "Epoch 2066, Loss 2.951656\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0157,  0.0889])\n",
      "Epoch 2067, Loss 2.951576\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0157,  0.0888])\n",
      "Epoch 2068, Loss 2.951494\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0157,  0.0886])\n",
      "Epoch 2069, Loss 2.951413\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0157,  0.0885])\n",
      "Epoch 2070, Loss 2.951333\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0156,  0.0883])\n",
      "Epoch 2071, Loss 2.951252\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0156,  0.0882])\n",
      "Epoch 2072, Loss 2.951171\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0155,  0.0880])\n",
      "Epoch 2073, Loss 2.951093\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0155,  0.0879])\n",
      "Epoch 2074, Loss 2.951012\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0155,  0.0877])\n",
      "Epoch 2075, Loss 2.950932\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0155,  0.0876])\n",
      "Epoch 2076, Loss 2.950853\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0154,  0.0874])\n",
      "Epoch 2077, Loss 2.950774\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0154,  0.0873])\n",
      "Epoch 2078, Loss 2.950697\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0154,  0.0871])\n",
      "Epoch 2079, Loss 2.950618\n",
      "Params: tensor(0.3143)\n",
      "Grad: tensor([-0.0154,  0.0870])\n",
      "Epoch 2080, Loss 2.950540\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0154,  0.0868])\n",
      "Epoch 2081, Loss 2.950463\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0153,  0.0867])\n",
      "Epoch 2082, Loss 2.950385\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0153,  0.0866])\n",
      "Epoch 2083, Loss 2.950308\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0153,  0.0864])\n",
      "Epoch 2084, Loss 2.950231\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0152,  0.0863])\n",
      "Epoch 2085, Loss 2.950154\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0152,  0.0861])\n",
      "Epoch 2086, Loss 2.950078\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0152,  0.0860])\n",
      "Epoch 2087, Loss 2.950003\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0152,  0.0858])\n",
      "Epoch 2088, Loss 2.949925\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0152,  0.0857])\n",
      "Epoch 2089, Loss 2.949850\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0151,  0.0855])\n",
      "Epoch 2090, Loss 2.949776\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0151,  0.0854])\n",
      "Epoch 2091, Loss 2.949699\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0151,  0.0852])\n",
      "Epoch 2092, Loss 2.949626\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0150,  0.0851])\n",
      "Epoch 2093, Loss 2.949550\n",
      "Params: tensor(0.3142)\n",
      "Grad: tensor([-0.0150,  0.0850])\n",
      "Epoch 2094, Loss 2.949476\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0150,  0.0848])\n",
      "Epoch 2095, Loss 2.949401\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0149,  0.0847])\n",
      "Epoch 2096, Loss 2.949328\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0150,  0.0845])\n",
      "Epoch 2097, Loss 2.949254\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0149,  0.0844])\n",
      "Epoch 2098, Loss 2.949182\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0149,  0.0842])\n",
      "Epoch 2099, Loss 2.949108\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0149,  0.0841])\n",
      "Epoch 2100, Loss 2.949035\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0148,  0.0839])\n",
      "Epoch 2101, Loss 2.948962\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0148,  0.0838])\n",
      "Epoch 2102, Loss 2.948890\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0148,  0.0837])\n",
      "Epoch 2103, Loss 2.948818\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0148,  0.0835])\n",
      "Epoch 2104, Loss 2.948745\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0148,  0.0834])\n",
      "Epoch 2105, Loss 2.948675\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0147,  0.0832])\n",
      "Epoch 2106, Loss 2.948602\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0147,  0.0831])\n",
      "Epoch 2107, Loss 2.948532\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0146,  0.0830])\n",
      "Epoch 2108, Loss 2.948462\n",
      "Params: tensor(0.3141)\n",
      "Grad: tensor([-0.0146,  0.0828])\n",
      "Epoch 2109, Loss 2.948391\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0146,  0.0827])\n",
      "Epoch 2110, Loss 2.948321\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0146,  0.0825])\n",
      "Epoch 2111, Loss 2.948250\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0145,  0.0824])\n",
      "Epoch 2112, Loss 2.948181\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0145,  0.0823])\n",
      "Epoch 2113, Loss 2.948109\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0145,  0.0821])\n",
      "Epoch 2114, Loss 2.948041\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0145,  0.0820])\n",
      "Epoch 2115, Loss 2.947971\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0144,  0.0818])\n",
      "Epoch 2116, Loss 2.947902\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0144,  0.0817])\n",
      "Epoch 2117, Loss 2.947833\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0144,  0.0816])\n",
      "Epoch 2118, Loss 2.947765\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0144,  0.0814])\n",
      "Epoch 2119, Loss 2.947696\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0144,  0.0813])\n",
      "Epoch 2120, Loss 2.947628\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0143,  0.0811])\n",
      "Epoch 2121, Loss 2.947560\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0143,  0.0810])\n",
      "Epoch 2122, Loss 2.947494\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0143,  0.0809])\n",
      "Epoch 2123, Loss 2.947426\n",
      "Params: tensor(0.3140)\n",
      "Grad: tensor([-0.0143,  0.0807])\n",
      "Epoch 2124, Loss 2.947357\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0142,  0.0806])\n",
      "Epoch 2125, Loss 2.947293\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0142,  0.0805])\n",
      "Epoch 2126, Loss 2.947225\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0142,  0.0803])\n",
      "Epoch 2127, Loss 2.947158\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0142,  0.0802])\n",
      "Epoch 2128, Loss 2.947092\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0141,  0.0800])\n",
      "Epoch 2129, Loss 2.947026\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0141,  0.0799])\n",
      "Epoch 2130, Loss 2.946960\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0141,  0.0798])\n",
      "Epoch 2131, Loss 2.946895\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0141,  0.0796])\n",
      "Epoch 2132, Loss 2.946830\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0141,  0.0795])\n",
      "Epoch 2133, Loss 2.946764\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0140,  0.0794])\n",
      "Epoch 2134, Loss 2.946700\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0140,  0.0792])\n",
      "Epoch 2135, Loss 2.946635\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0140,  0.0791])\n",
      "Epoch 2136, Loss 2.946571\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0139,  0.0790])\n",
      "Epoch 2137, Loss 2.946507\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0139,  0.0788])\n",
      "Epoch 2138, Loss 2.946442\n",
      "Params: tensor(0.3139)\n",
      "Grad: tensor([-0.0139,  0.0787])\n",
      "Epoch 2139, Loss 2.946378\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0139,  0.0786])\n",
      "Epoch 2140, Loss 2.946314\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0138,  0.0784])\n",
      "Epoch 2141, Loss 2.946251\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0138,  0.0783])\n",
      "Epoch 2142, Loss 2.946189\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0138,  0.0782])\n",
      "Epoch 2143, Loss 2.946126\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0138,  0.0780])\n",
      "Epoch 2144, Loss 2.946063\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0138,  0.0779])\n",
      "Epoch 2145, Loss 2.946001\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0137,  0.0778])\n",
      "Epoch 2146, Loss 2.945937\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0137,  0.0776])\n",
      "Epoch 2147, Loss 2.945876\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0137,  0.0775])\n",
      "Epoch 2148, Loss 2.945815\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0137,  0.0774])\n",
      "Epoch 2149, Loss 2.945753\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0136,  0.0772])\n",
      "Epoch 2150, Loss 2.945690\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0136,  0.0771])\n",
      "Epoch 2151, Loss 2.945630\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0136,  0.0770])\n",
      "Epoch 2152, Loss 2.945567\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0136,  0.0768])\n",
      "Epoch 2153, Loss 2.945508\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0135,  0.0767])\n",
      "Epoch 2154, Loss 2.945447\n",
      "Params: tensor(0.3138)\n",
      "Grad: tensor([-0.0135,  0.0766])\n",
      "Epoch 2155, Loss 2.945385\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0135,  0.0765])\n",
      "Epoch 2156, Loss 2.945325\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0135,  0.0763])\n",
      "Epoch 2157, Loss 2.945267\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0135,  0.0762])\n",
      "Epoch 2158, Loss 2.945206\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0134,  0.0761])\n",
      "Epoch 2159, Loss 2.945146\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0134,  0.0759])\n",
      "Epoch 2160, Loss 2.945088\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0134,  0.0758])\n",
      "Epoch 2161, Loss 2.945028\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0134,  0.0757])\n",
      "Epoch 2162, Loss 2.944969\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0133,  0.0755])\n",
      "Epoch 2163, Loss 2.944911\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0133,  0.0754])\n",
      "Epoch 2164, Loss 2.944852\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0133,  0.0753])\n",
      "Epoch 2165, Loss 2.944792\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0133,  0.0752])\n",
      "Epoch 2166, Loss 2.944736\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0133,  0.0750])\n",
      "Epoch 2167, Loss 2.944678\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0132,  0.0749])\n",
      "Epoch 2168, Loss 2.944619\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0132,  0.0748])\n",
      "Epoch 2169, Loss 2.944562\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0132,  0.0747])\n",
      "Epoch 2170, Loss 2.944504\n",
      "Params: tensor(0.3137)\n",
      "Grad: tensor([-0.0132,  0.0745])\n",
      "Epoch 2171, Loss 2.944447\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0132,  0.0744])\n",
      "Epoch 2172, Loss 2.944391\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0131,  0.0743])\n",
      "Epoch 2173, Loss 2.944332\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0131,  0.0742])\n",
      "Epoch 2174, Loss 2.944276\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0131,  0.0740])\n",
      "Epoch 2175, Loss 2.944220\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0131,  0.0739])\n",
      "Epoch 2176, Loss 2.944164\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0130,  0.0738])\n",
      "Epoch 2177, Loss 2.944108\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0130,  0.0736])\n",
      "Epoch 2178, Loss 2.944053\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0130,  0.0735])\n",
      "Epoch 2179, Loss 2.943996\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0130,  0.0734])\n",
      "Epoch 2180, Loss 2.943941\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0129,  0.0733])\n",
      "Epoch 2181, Loss 2.943887\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0129,  0.0731])\n",
      "Epoch 2182, Loss 2.943831\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0129,  0.0730])\n",
      "Epoch 2183, Loss 2.943776\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0129,  0.0729])\n",
      "Epoch 2184, Loss 2.943721\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0129,  0.0728])\n",
      "Epoch 2185, Loss 2.943666\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0128,  0.0727])\n",
      "Epoch 2186, Loss 2.943613\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0128,  0.0725])\n",
      "Epoch 2187, Loss 2.943558\n",
      "Params: tensor(0.3136)\n",
      "Grad: tensor([-0.0128,  0.0724])\n",
      "Epoch 2188, Loss 2.943503\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0128,  0.0723])\n",
      "Epoch 2189, Loss 2.943451\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0127,  0.0722])\n",
      "Epoch 2190, Loss 2.943395\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0127,  0.0720])\n",
      "Epoch 2191, Loss 2.943343\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0127,  0.0719])\n",
      "Epoch 2192, Loss 2.943290\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0127,  0.0718])\n",
      "Epoch 2193, Loss 2.943235\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0127,  0.0717])\n",
      "Epoch 2194, Loss 2.943183\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0126,  0.0715])\n",
      "Epoch 2195, Loss 2.943130\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0126,  0.0714])\n",
      "Epoch 2196, Loss 2.943079\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0126,  0.0713])\n",
      "Epoch 2197, Loss 2.943027\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0126,  0.0712])\n",
      "Epoch 2198, Loss 2.942973\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0126,  0.0711])\n",
      "Epoch 2199, Loss 2.942922\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0125,  0.0709])\n",
      "Epoch 2200, Loss 2.942870\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0125,  0.0708])\n",
      "Epoch 2201, Loss 2.942818\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0125,  0.0707])\n",
      "Epoch 2202, Loss 2.942766\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0125,  0.0706])\n",
      "Epoch 2203, Loss 2.942714\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0124,  0.0705])\n",
      "Epoch 2204, Loss 2.942665\n",
      "Params: tensor(0.3135)\n",
      "Grad: tensor([-0.0124,  0.0703])\n",
      "Epoch 2205, Loss 2.942612\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0124,  0.0702])\n",
      "Epoch 2206, Loss 2.942564\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0124,  0.0701])\n",
      "Epoch 2207, Loss 2.942510\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0124,  0.0700])\n",
      "Epoch 2208, Loss 2.942461\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0123,  0.0699])\n",
      "Epoch 2209, Loss 2.942411\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0123,  0.0697])\n",
      "Epoch 2210, Loss 2.942361\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0123,  0.0696])\n",
      "Epoch 2211, Loss 2.942310\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0123,  0.0695])\n",
      "Epoch 2212, Loss 2.942261\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0694])\n",
      "Epoch 2213, Loss 2.942211\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0693])\n",
      "Epoch 2214, Loss 2.942162\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0692])\n",
      "Epoch 2215, Loss 2.942112\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0690])\n",
      "Epoch 2216, Loss 2.942062\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0689])\n",
      "Epoch 2217, Loss 2.942014\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0122,  0.0688])\n",
      "Epoch 2218, Loss 2.941965\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0121,  0.0687])\n",
      "Epoch 2219, Loss 2.941918\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0121,  0.0686])\n",
      "Epoch 2220, Loss 2.941868\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0121,  0.0685])\n",
      "Epoch 2221, Loss 2.941821\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0121,  0.0683])\n",
      "Epoch 2222, Loss 2.941773\n",
      "Params: tensor(0.3134)\n",
      "Grad: tensor([-0.0120,  0.0682])\n",
      "Epoch 2223, Loss 2.941724\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0120,  0.0681])\n",
      "Epoch 2224, Loss 2.941677\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0120,  0.0680])\n",
      "Epoch 2225, Loss 2.941629\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0120,  0.0679])\n",
      "Epoch 2226, Loss 2.941582\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0120,  0.0678])\n",
      "Epoch 2227, Loss 2.941534\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0119,  0.0676])\n",
      "Epoch 2228, Loss 2.941488\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0119,  0.0675])\n",
      "Epoch 2229, Loss 2.941440\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0119,  0.0674])\n",
      "Epoch 2230, Loss 2.941393\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0119,  0.0673])\n",
      "Epoch 2231, Loss 2.941346\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0119,  0.0672])\n",
      "Epoch 2232, Loss 2.941299\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0118,  0.0671])\n",
      "Epoch 2233, Loss 2.941253\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0118,  0.0670])\n",
      "Epoch 2234, Loss 2.941206\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0118,  0.0668])\n",
      "Epoch 2235, Loss 2.941163\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0118,  0.0667])\n",
      "Epoch 2236, Loss 2.941116\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0118,  0.0666])\n",
      "Epoch 2237, Loss 2.941070\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0117,  0.0665])\n",
      "Epoch 2238, Loss 2.941025\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0117,  0.0664])\n",
      "Epoch 2239, Loss 2.940979\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0117,  0.0663])\n",
      "Epoch 2240, Loss 2.940933\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0117,  0.0662])\n",
      "Epoch 2241, Loss 2.940890\n",
      "Params: tensor(0.3133)\n",
      "Grad: tensor([-0.0117,  0.0661])\n",
      "Epoch 2242, Loss 2.940844\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0117,  0.0659])\n",
      "Epoch 2243, Loss 2.940798\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0116,  0.0658])\n",
      "Epoch 2244, Loss 2.940753\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0116,  0.0657])\n",
      "Epoch 2245, Loss 2.940711\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0116,  0.0656])\n",
      "Epoch 2246, Loss 2.940666\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0116,  0.0655])\n",
      "Epoch 2247, Loss 2.940621\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0115,  0.0654])\n",
      "Epoch 2248, Loss 2.940576\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0115,  0.0653])\n",
      "Epoch 2249, Loss 2.940533\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0115,  0.0652])\n",
      "Epoch 2250, Loss 2.940489\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0115,  0.0650])\n",
      "Epoch 2251, Loss 2.940446\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0115,  0.0649])\n",
      "Epoch 2252, Loss 2.940403\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0648])\n",
      "Epoch 2253, Loss 2.940358\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0647])\n",
      "Epoch 2254, Loss 2.940316\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0646])\n",
      "Epoch 2255, Loss 2.940274\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0645])\n",
      "Epoch 2256, Loss 2.940229\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0644])\n",
      "Epoch 2257, Loss 2.940188\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0643])\n",
      "Epoch 2258, Loss 2.940144\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0114,  0.0642])\n",
      "Epoch 2259, Loss 2.940102\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0113,  0.0641])\n",
      "Epoch 2260, Loss 2.940060\n",
      "Params: tensor(0.3132)\n",
      "Grad: tensor([-0.0113,  0.0640])\n",
      "Epoch 2261, Loss 2.940018\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0113,  0.0638])\n",
      "Epoch 2262, Loss 2.939977\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0113,  0.0637])\n",
      "Epoch 2263, Loss 2.939934\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0112,  0.0636])\n",
      "Epoch 2264, Loss 2.939891\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0112,  0.0635])\n",
      "Epoch 2265, Loss 2.939851\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0112,  0.0634])\n",
      "Epoch 2266, Loss 2.939809\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0112,  0.0633])\n",
      "Epoch 2267, Loss 2.939770\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0112,  0.0632])\n",
      "Epoch 2268, Loss 2.939727\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0631])\n",
      "Epoch 2269, Loss 2.939686\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0630])\n",
      "Epoch 2270, Loss 2.939646\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0629])\n",
      "Epoch 2271, Loss 2.939605\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0628])\n",
      "Epoch 2272, Loss 2.939566\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0627])\n",
      "Epoch 2273, Loss 2.939522\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0111,  0.0626])\n",
      "Epoch 2274, Loss 2.939483\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0110,  0.0624])\n",
      "Epoch 2275, Loss 2.939443\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0110,  0.0623])\n",
      "Epoch 2276, Loss 2.939403\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0110,  0.0622])\n",
      "Epoch 2277, Loss 2.939361\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0110,  0.0621])\n",
      "Epoch 2278, Loss 2.939323\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0110,  0.0620])\n",
      "Epoch 2279, Loss 2.939282\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0109,  0.0619])\n",
      "Epoch 2280, Loss 2.939243\n",
      "Params: tensor(0.3131)\n",
      "Grad: tensor([-0.0109,  0.0618])\n",
      "Epoch 2281, Loss 2.939205\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0109,  0.0617])\n",
      "Epoch 2282, Loss 2.939165\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0109,  0.0616])\n",
      "Epoch 2283, Loss 2.939127\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0109,  0.0615])\n",
      "Epoch 2284, Loss 2.939087\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0614])\n",
      "Epoch 2285, Loss 2.939049\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0613])\n",
      "Epoch 2286, Loss 2.939011\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0612])\n",
      "Epoch 2287, Loss 2.938971\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0611])\n",
      "Epoch 2288, Loss 2.938933\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0610])\n",
      "Epoch 2289, Loss 2.938893\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0108,  0.0609])\n",
      "Epoch 2290, Loss 2.938857\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0107,  0.0608])\n",
      "Epoch 2291, Loss 2.938820\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0107,  0.0607])\n",
      "Epoch 2292, Loss 2.938779\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0107,  0.0606])\n",
      "Epoch 2293, Loss 2.938743\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0107,  0.0605])\n",
      "Epoch 2294, Loss 2.938705\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0107,  0.0604])\n",
      "Epoch 2295, Loss 2.938667\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0603])\n",
      "Epoch 2296, Loss 2.938629\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0602])\n",
      "Epoch 2297, Loss 2.938593\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0601])\n",
      "Epoch 2298, Loss 2.938555\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0600])\n",
      "Epoch 2299, Loss 2.938519\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0598])\n",
      "Epoch 2300, Loss 2.938481\n",
      "Params: tensor(0.3130)\n",
      "Grad: tensor([-0.0106,  0.0597])\n",
      "Epoch 2301, Loss 2.938444\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0596])\n",
      "Epoch 2302, Loss 2.938408\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0595])\n",
      "Epoch 2303, Loss 2.938371\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0594])\n",
      "Epoch 2304, Loss 2.938335\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0593])\n",
      "Epoch 2305, Loss 2.938299\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0592])\n",
      "Epoch 2306, Loss 2.938263\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0105,  0.0591])\n",
      "Epoch 2307, Loss 2.938227\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0104,  0.0590])\n",
      "Epoch 2308, Loss 2.938190\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0104,  0.0589])\n",
      "Epoch 2309, Loss 2.938155\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0104,  0.0588])\n",
      "Epoch 2310, Loss 2.938118\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0104,  0.0587])\n",
      "Epoch 2311, Loss 2.938084\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0104,  0.0586])\n",
      "Epoch 2312, Loss 2.938049\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0585])\n",
      "Epoch 2313, Loss 2.938014\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0584])\n",
      "Epoch 2314, Loss 2.937977\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0583])\n",
      "Epoch 2315, Loss 2.937943\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0582])\n",
      "Epoch 2316, Loss 2.937908\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0581])\n",
      "Epoch 2317, Loss 2.937872\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0103,  0.0580])\n",
      "Epoch 2318, Loss 2.937839\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0102,  0.0580])\n",
      "Epoch 2319, Loss 2.937804\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch 2320, Loss 2.937769\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch 2321, Loss 2.937734\n",
      "Params: tensor(0.3129)\n",
      "Grad: tensor([-0.0102,  0.0577])\n",
      "Epoch 2322, Loss 2.937700\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0102,  0.0576])\n",
      "Epoch 2323, Loss 2.937665\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0102,  0.0575])\n",
      "Epoch 2324, Loss 2.937632\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0574])\n",
      "Epoch 2325, Loss 2.937598\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0573])\n",
      "Epoch 2326, Loss 2.937565\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0572])\n",
      "Epoch 2327, Loss 2.937531\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0571])\n",
      "Epoch 2328, Loss 2.937499\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0570])\n",
      "Epoch 2329, Loss 2.937465\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0101,  0.0569])\n",
      "Epoch 2330, Loss 2.937430\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0568])\n",
      "Epoch 2331, Loss 2.937398\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0567])\n",
      "Epoch 2332, Loss 2.937364\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0566])\n",
      "Epoch 2333, Loss 2.937332\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0565])\n",
      "Epoch 2334, Loss 2.937299\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0564])\n",
      "Epoch 2335, Loss 2.937265\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0100,  0.0563])\n",
      "Epoch 2336, Loss 2.937232\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0099,  0.0562])\n",
      "Epoch 2337, Loss 2.937201\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0099,  0.0561])\n",
      "Epoch 2338, Loss 2.937167\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0099,  0.0560])\n",
      "Epoch 2339, Loss 2.937134\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0099,  0.0559])\n",
      "Epoch 2340, Loss 2.937104\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0099,  0.0558])\n",
      "Epoch 2341, Loss 2.937071\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0098,  0.0557])\n",
      "Epoch 2342, Loss 2.937039\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0098,  0.0556])\n",
      "Epoch 2343, Loss 2.937008\n",
      "Params: tensor(0.3128)\n",
      "Grad: tensor([-0.0098,  0.0555])\n",
      "Epoch 2344, Loss 2.936976\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0098,  0.0554])\n",
      "Epoch 2345, Loss 2.936945\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch 2346, Loss 2.936912\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch 2347, Loss 2.936883\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0552])\n",
      "Epoch 2348, Loss 2.936851\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0551])\n",
      "Epoch 2349, Loss 2.936819\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0550])\n",
      "Epoch 2350, Loss 2.936788\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0549])\n",
      "Epoch 2351, Loss 2.936757\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0548])\n",
      "Epoch 2352, Loss 2.936725\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0097,  0.0547])\n",
      "Epoch 2353, Loss 2.936694\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0546])\n",
      "Epoch 2354, Loss 2.936665\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0545])\n",
      "Epoch 2355, Loss 2.936633\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0544])\n",
      "Epoch 2356, Loss 2.936602\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0543])\n",
      "Epoch 2357, Loss 2.936572\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0542])\n",
      "Epoch 2358, Loss 2.936542\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0095,  0.0541])\n",
      "Epoch 2359, Loss 2.936511\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0096,  0.0540])\n",
      "Epoch 2360, Loss 2.936481\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0095,  0.0540])\n",
      "Epoch 2361, Loss 2.936451\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0095,  0.0539])\n",
      "Epoch 2362, Loss 2.936421\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0095,  0.0538])\n",
      "Epoch 2363, Loss 2.936392\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0095,  0.0537])\n",
      "Epoch 2364, Loss 2.936362\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0094,  0.0536])\n",
      "Epoch 2365, Loss 2.936332\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0094,  0.0535])\n",
      "Epoch 2366, Loss 2.936304\n",
      "Params: tensor(0.3127)\n",
      "Grad: tensor([-0.0094,  0.0534])\n",
      "Epoch 2367, Loss 2.936274\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0094,  0.0533])\n",
      "Epoch 2368, Loss 2.936244\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0094,  0.0532])\n",
      "Epoch 2369, Loss 2.936216\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0094,  0.0531])\n",
      "Epoch 2370, Loss 2.936188\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch 2371, Loss 2.936156\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch 2372, Loss 2.936128\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0529])\n",
      "Epoch 2373, Loss 2.936100\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0528])\n",
      "Epoch 2374, Loss 2.936072\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0527])\n",
      "Epoch 2375, Loss 2.936042\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0526])\n",
      "Epoch 2376, Loss 2.936014\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0525])\n",
      "Epoch 2377, Loss 2.935986\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0524])\n",
      "Epoch 2378, Loss 2.935957\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0093,  0.0523])\n",
      "Epoch 2379, Loss 2.935928\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch 2380, Loss 2.935901\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch 2381, Loss 2.935873\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0521])\n",
      "Epoch 2382, Loss 2.935845\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0520])\n",
      "Epoch 2383, Loss 2.935817\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0519])\n",
      "Epoch 2384, Loss 2.935789\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0518])\n",
      "Epoch 2385, Loss 2.935762\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0092,  0.0517])\n",
      "Epoch 2386, Loss 2.935734\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0091,  0.0516])\n",
      "Epoch 2387, Loss 2.935707\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0091,  0.0515])\n",
      "Epoch 2388, Loss 2.935679\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch 2389, Loss 2.935650\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch 2390, Loss 2.935626\n",
      "Params: tensor(0.3126)\n",
      "Grad: tensor([-0.0090,  0.0513])\n",
      "Epoch 2391, Loss 2.935596\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0512])\n",
      "Epoch 2392, Loss 2.935571\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0511])\n",
      "Epoch 2393, Loss 2.935544\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0510])\n",
      "Epoch 2394, Loss 2.935516\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0509])\n",
      "Epoch 2395, Loss 2.935489\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0508])\n",
      "Epoch 2396, Loss 2.935465\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch 2397, Loss 2.935436\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch 2398, Loss 2.935411\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0089,  0.0506])\n",
      "Epoch 2399, Loss 2.935385\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0089,  0.0505])\n",
      "Epoch 2400, Loss 2.935356\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0089,  0.0504])\n",
      "Epoch 2401, Loss 2.935332\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0089,  0.0503])\n",
      "Epoch 2402, Loss 2.935304\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0089,  0.0502])\n",
      "Epoch 2403, Loss 2.935281\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0502])\n",
      "Epoch 2404, Loss 2.935252\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0501])\n",
      "Epoch 2405, Loss 2.935228\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0500])\n",
      "Epoch 2406, Loss 2.935203\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0499])\n",
      "Epoch 2407, Loss 2.935177\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0498])\n",
      "Epoch 2408, Loss 2.935152\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0497])\n",
      "Epoch 2409, Loss 2.935126\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch 2410, Loss 2.935100\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch 2411, Loss 2.935075\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0087,  0.0495])\n",
      "Epoch 2412, Loss 2.935049\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0087,  0.0494])\n",
      "Epoch 2413, Loss 2.935024\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0087,  0.0493])\n",
      "Epoch 2414, Loss 2.935001\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0087,  0.0492])\n",
      "Epoch 2415, Loss 2.934973\n",
      "Params: tensor(0.3125)\n",
      "Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch 2416, Loss 2.934949\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch 2417, Loss 2.934925\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0490])\n",
      "Epoch 2418, Loss 2.934899\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0489])\n",
      "Epoch 2419, Loss 2.934876\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0488])\n",
      "Epoch 2420, Loss 2.934853\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0487])\n",
      "Epoch 2421, Loss 2.934826\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch 2422, Loss 2.934802\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch 2423, Loss 2.934777\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0485])\n",
      "Epoch 2424, Loss 2.934753\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0484])\n",
      "Epoch 2425, Loss 2.934730\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0086,  0.0483])\n",
      "Epoch 2426, Loss 2.934705\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0085,  0.0482])\n",
      "Epoch 2427, Loss 2.934681\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch 2428, Loss 2.934658\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch 2429, Loss 2.934635\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0085,  0.0480])\n",
      "Epoch 2430, Loss 2.934609\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0085,  0.0479])\n",
      "Epoch 2431, Loss 2.934585\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0478])\n",
      "Epoch 2432, Loss 2.934563\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch 2433, Loss 2.934541\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch 2434, Loss 2.934516\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0476])\n",
      "Epoch 2435, Loss 2.934493\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0475])\n",
      "Epoch 2436, Loss 2.934469\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0474])\n",
      "Epoch 2437, Loss 2.934446\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0084,  0.0473])\n",
      "Epoch 2438, Loss 2.934423\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0083,  0.0473])\n",
      "Epoch 2439, Loss 2.934400\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0083,  0.0472])\n",
      "Epoch 2440, Loss 2.934377\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0083,  0.0471])\n",
      "Epoch 2441, Loss 2.934355\n",
      "Params: tensor(0.3124)\n",
      "Grad: tensor([-0.0083,  0.0470])\n",
      "Epoch 2442, Loss 2.934331\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch 2443, Loss 2.934309\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch 2444, Loss 2.934287\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0083,  0.0468])\n",
      "Epoch 2445, Loss 2.934264\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0083,  0.0467])\n",
      "Epoch 2446, Loss 2.934242\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0083,  0.0466])\n",
      "Epoch 2447, Loss 2.934219\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch 2448, Loss 2.934198\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch 2449, Loss 2.934175\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0464])\n",
      "Epoch 2450, Loss 2.934151\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0463])\n",
      "Epoch 2451, Loss 2.934129\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0462])\n",
      "Epoch 2452, Loss 2.934108\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0082,  0.0461])\n",
      "Epoch 2453, Loss 2.934084\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0461])\n",
      "Epoch 2454, Loss 2.934065\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0460])\n",
      "Epoch 2455, Loss 2.934043\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0459])\n",
      "Epoch 2456, Loss 2.934020\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0458])\n",
      "Epoch 2457, Loss 2.934000\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch 2458, Loss 2.933978\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch 2459, Loss 2.933956\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0456])\n",
      "Epoch 2460, Loss 2.933935\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0455])\n",
      "Epoch 2461, Loss 2.933914\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch 2462, Loss 2.933893\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch 2463, Loss 2.933871\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0453])\n",
      "Epoch 2464, Loss 2.933849\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0452])\n",
      "Epoch 2465, Loss 2.933828\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch 2466, Loss 2.933807\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch 2467, Loss 2.933787\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0079,  0.0450])\n",
      "Epoch 2468, Loss 2.933767\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0079,  0.0449])\n",
      "Epoch 2469, Loss 2.933746\n",
      "Params: tensor(0.3123)\n",
      "Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch 2470, Loss 2.933723\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch 2471, Loss 2.933704\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0079,  0.0447])\n",
      "Epoch 2472, Loss 2.933682\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0079,  0.0446])\n",
      "Epoch 2473, Loss 2.933662\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0079,  0.0445])\n",
      "Epoch 2474, Loss 2.933643\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0079,  0.0444])\n",
      "Epoch 2475, Loss 2.933622\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0444])\n",
      "Epoch 2476, Loss 2.933602\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0443])\n",
      "Epoch 2477, Loss 2.933583\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0442])\n",
      "Epoch 2478, Loss 2.933561\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch 2479, Loss 2.933541\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch 2480, Loss 2.933521\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0440])\n",
      "Epoch 2481, Loss 2.933501\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0078,  0.0439])\n",
      "Epoch 2482, Loss 2.933480\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch 2483, Loss 2.933463\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch 2484, Loss 2.933442\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0437])\n",
      "Epoch 2485, Loss 2.933422\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch 2486, Loss 2.933403\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch 2487, Loss 2.933382\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0435])\n",
      "Epoch 2488, Loss 2.933365\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0434])\n",
      "Epoch 2489, Loss 2.933345\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0077,  0.0433])\n",
      "Epoch 2490, Loss 2.933325\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0433])\n",
      "Epoch 2491, Loss 2.933306\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0432])\n",
      "Epoch 2492, Loss 2.933287\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0431])\n",
      "Epoch 2493, Loss 2.933266\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch 2494, Loss 2.933249\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch 2495, Loss 2.933229\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0429])\n",
      "Epoch 2496, Loss 2.933209\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0076,  0.0428])\n",
      "Epoch 2497, Loss 2.933190\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch 2498, Loss 2.933172\n",
      "Params: tensor(0.3122)\n",
      "Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch 2499, Loss 2.933154\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0426])\n",
      "Epoch 2500, Loss 2.933134\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch 2501, Loss 2.933116\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch 2502, Loss 2.933097\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0424])\n",
      "Epoch 2503, Loss 2.933079\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0423])\n",
      "Epoch 2504, Loss 2.933060\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0075,  0.0422])\n",
      "Epoch 2505, Loss 2.933043\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0422])\n",
      "Epoch 2506, Loss 2.933025\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0421])\n",
      "Epoch 2507, Loss 2.933007\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch 2508, Loss 2.932988\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch 2509, Loss 2.932970\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0419])\n",
      "Epoch 2510, Loss 2.932953\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0418])\n",
      "Epoch 2511, Loss 2.932932\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0074,  0.0417])\n",
      "Epoch 2512, Loss 2.932915\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0417])\n",
      "Epoch 2513, Loss 2.932898\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0416])\n",
      "Epoch 2514, Loss 2.932880\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch 2515, Loss 2.932862\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch 2516, Loss 2.932846\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0414])\n",
      "Epoch 2517, Loss 2.932826\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0413])\n",
      "Epoch 2518, Loss 2.932810\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch 2519, Loss 2.932790\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch 2520, Loss 2.932774\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0411])\n",
      "Epoch 2521, Loss 2.932758\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch 2522, Loss 2.932739\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch 2523, Loss 2.932723\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0409])\n",
      "Epoch 2524, Loss 2.932706\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch 2525, Loss 2.932689\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch 2526, Loss 2.932671\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0407])\n",
      "Epoch 2527, Loss 2.932654\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0406])\n",
      "Epoch 2528, Loss 2.932637\n",
      "Params: tensor(0.3121)\n",
      "Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch 2529, Loss 2.932619\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch 2530, Loss 2.932603\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0404])\n",
      "Epoch 2531, Loss 2.932585\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch 2532, Loss 2.932569\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch 2533, Loss 2.932553\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0402])\n",
      "Epoch 2534, Loss 2.932535\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch 2535, Loss 2.932520\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch 2536, Loss 2.932502\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0400])\n",
      "Epoch 2537, Loss 2.932487\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0071,  0.0399])\n",
      "Epoch 2538, Loss 2.932469\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0399])\n",
      "Epoch 2539, Loss 2.932455\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0398])\n",
      "Epoch 2540, Loss 2.932438\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch 2541, Loss 2.932421\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch 2542, Loss 2.932404\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0396])\n",
      "Epoch 2543, Loss 2.932387\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch 2544, Loss 2.932371\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch 2545, Loss 2.932358\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0070,  0.0394])\n",
      "Epoch 2546, Loss 2.932340\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch 2547, Loss 2.932324\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch 2548, Loss 2.932310\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0392])\n",
      "Epoch 2549, Loss 2.932293\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch 2550, Loss 2.932277\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch 2551, Loss 2.932261\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0390])\n",
      "Epoch 2552, Loss 2.932246\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch 2553, Loss 2.932229\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch 2554, Loss 2.932215\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0069,  0.0388])\n",
      "Epoch 2555, Loss 2.932198\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch 2556, Loss 2.932183\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch 2557, Loss 2.932167\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0386])\n",
      "Epoch 2558, Loss 2.932153\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch 2559, Loss 2.932137\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch 2560, Loss 2.932122\n",
      "Params: tensor(0.3120)\n",
      "Grad: tensor([-0.0068,  0.0384])\n",
      "Epoch 2561, Loss 2.932107\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch 2562, Loss 2.932092\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch 2563, Loss 2.932076\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0382])\n",
      "Epoch 2564, Loss 2.932061\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch 2565, Loss 2.932047\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch 2566, Loss 2.932031\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0380])\n",
      "Epoch 2567, Loss 2.932017\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch 2568, Loss 2.932002\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch 2569, Loss 2.931986\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch 2570, Loss 2.931972\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch 2571, Loss 2.931957\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0377])\n",
      "Epoch 2572, Loss 2.931941\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0067,  0.0376])\n",
      "Epoch 2573, Loss 2.931929\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0376])\n",
      "Epoch 2574, Loss 2.931914\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0375])\n",
      "Epoch 2575, Loss 2.931900\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch 2576, Loss 2.931885\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch 2577, Loss 2.931870\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0373])\n",
      "Epoch 2578, Loss 2.931855\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch 2579, Loss 2.931842\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch 2580, Loss 2.931828\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0066,  0.0371])\n",
      "Epoch 2581, Loss 2.931813\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0371])\n",
      "Epoch 2582, Loss 2.931799\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0370])\n",
      "Epoch 2583, Loss 2.931786\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch 2584, Loss 2.931771\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch 2585, Loss 2.931759\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0368])\n",
      "Epoch 2586, Loss 2.931742\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch 2587, Loss 2.931729\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch 2588, Loss 2.931717\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch 2589, Loss 2.931701\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch 2590, Loss 2.931687\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0065,  0.0365])\n",
      "Epoch 2591, Loss 2.931674\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch 2592, Loss 2.931660\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch 2593, Loss 2.931648\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0064,  0.0363])\n",
      "Epoch 2594, Loss 2.931632\n",
      "Params: tensor(0.3119)\n",
      "Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch 2595, Loss 2.931619\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch 2596, Loss 2.931606\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch 2597, Loss 2.931593\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch 2598, Loss 2.931580\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0360])\n",
      "Epoch 2599, Loss 2.931566\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch 2600, Loss 2.931554\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch 2601, Loss 2.931538\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch 2602, Loss 2.931526\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch 2603, Loss 2.931512\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0357])\n",
      "Epoch 2604, Loss 2.931499\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch 2605, Loss 2.931488\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch 2606, Loss 2.931474\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0063,  0.0355])\n",
      "Epoch 2607, Loss 2.931462\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0355])\n",
      "Epoch 2608, Loss 2.931448\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0354])\n",
      "Epoch 2609, Loss 2.931436\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch 2610, Loss 2.931423\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch 2611, Loss 2.931411\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch 2612, Loss 2.931397\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch 2613, Loss 2.931384\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0351])\n",
      "Epoch 2614, Loss 2.931371\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch 2615, Loss 2.931358\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch 2616, Loss 2.931346\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch 2617, Loss 2.931335\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch 2618, Loss 2.931322\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0062,  0.0348])\n",
      "Epoch 2619, Loss 2.931308\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch 2620, Loss 2.931296\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch 2621, Loss 2.931282\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch 2622, Loss 2.931272\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch 2623, Loss 2.931258\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0345])\n",
      "Epoch 2624, Loss 2.931245\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch 2625, Loss 2.931234\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch 2626, Loss 2.931222\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0061,  0.0343])\n",
      "Epoch 2627, Loss 2.931211\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0060,  0.0343])\n",
      "Epoch 2628, Loss 2.931196\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch 2629, Loss 2.931185\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch 2630, Loss 2.931173\n",
      "Params: tensor(0.3118)\n",
      "Grad: tensor([-0.0060,  0.0341])\n",
      "Epoch 2631, Loss 2.931162\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch 2632, Loss 2.931149\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch 2633, Loss 2.931138\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch 2634, Loss 2.931126\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch 2635, Loss 2.931114\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0338])\n",
      "Epoch 2636, Loss 2.931101\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0060,  0.0337])\n",
      "Epoch 2637, Loss 2.931090\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0337])\n",
      "Epoch 2638, Loss 2.931079\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch 2639, Loss 2.931067\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch 2640, Loss 2.931054\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch 2641, Loss 2.931044\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch 2642, Loss 2.931034\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0334])\n",
      "Epoch 2643, Loss 2.931021\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch 2644, Loss 2.931010\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch 2645, Loss 2.930999\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch 2646, Loss 2.930987\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch 2647, Loss 2.930976\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch 2648, Loss 2.930964\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch 2649, Loss 2.930953\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch 2650, Loss 2.930941\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch 2651, Loss 2.930932\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0329])\n",
      "Epoch 2652, Loss 2.930921\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch 2653, Loss 2.930908\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch 2654, Loss 2.930899\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch 2655, Loss 2.930885\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch 2656, Loss 2.930876\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0058,  0.0326])\n",
      "Epoch 2657, Loss 2.930863\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0326])\n",
      "Epoch 2658, Loss 2.930854\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch 2659, Loss 2.930841\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch 2660, Loss 2.930833\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0324])\n",
      "Epoch 2661, Loss 2.930821\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch 2662, Loss 2.930811\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch 2663, Loss 2.930801\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch 2664, Loss 2.930788\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch 2665, Loss 2.930778\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch 2666, Loss 2.930767\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch 2667, Loss 2.930757\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0057,  0.0320])\n",
      "Epoch 2668, Loss 2.930746\n",
      "Params: tensor(0.3117)\n",
      "Grad: tensor([-0.0056,  0.0320])\n",
      "Epoch 2669, Loss 2.930736\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch 2670, Loss 2.930724\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch 2671, Loss 2.930715\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0318])\n",
      "Epoch 2672, Loss 2.930704\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch 2673, Loss 2.930694\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch 2674, Loss 2.930685\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch 2675, Loss 2.930674\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch 2676, Loss 2.930663\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch 2677, Loss 2.930654\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch 2678, Loss 2.930644\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch 2679, Loss 2.930631\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch 2680, Loss 2.930621\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch 2681, Loss 2.930613\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch 2682, Loss 2.930603\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch 2683, Loss 2.930593\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch 2684, Loss 2.930582\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0311])\n",
      "Epoch 2685, Loss 2.930571\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch 2686, Loss 2.930562\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch 2687, Loss 2.930552\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch 2688, Loss 2.930543\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch 2689, Loss 2.930534\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0055,  0.0308])\n",
      "Epoch 2690, Loss 2.930523\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0308])\n",
      "Epoch 2691, Loss 2.930514\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch 2692, Loss 2.930502\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch 2693, Loss 2.930493\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch 2694, Loss 2.930482\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch 2695, Loss 2.930474\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch 2696, Loss 2.930464\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch 2697, Loss 2.930454\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch 2698, Loss 2.930445\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch 2699, Loss 2.930436\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch 2700, Loss 2.930426\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch 2701, Loss 2.930416\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0054,  0.0302])\n",
      "Epoch 2702, Loss 2.930408\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0302])\n",
      "Epoch 2703, Loss 2.930398\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch 2704, Loss 2.930388\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch 2705, Loss 2.930380\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch 2706, Loss 2.930370\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch 2707, Loss 2.930360\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch 2708, Loss 2.930353\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch 2709, Loss 2.930342\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch 2710, Loss 2.930335\n",
      "Params: tensor(0.3116)\n",
      "Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch 2711, Loss 2.930325\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch 2712, Loss 2.930315\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch 2713, Loss 2.930306\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch 2714, Loss 2.930298\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch 2715, Loss 2.930288\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch 2716, Loss 2.930279\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch 2717, Loss 2.930270\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch 2718, Loss 2.930262\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch 2719, Loss 2.930254\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0293])\n",
      "Epoch 2720, Loss 2.930244\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0293])\n",
      "Epoch 2721, Loss 2.930235\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch 2722, Loss 2.930226\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch 2723, Loss 2.930218\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch 2724, Loss 2.930209\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch 2725, Loss 2.930201\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch 2726, Loss 2.930190\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch 2727, Loss 2.930183\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch 2728, Loss 2.930173\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch 2729, Loss 2.930166\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch 2730, Loss 2.930156\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch 2731, Loss 2.930149\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch 2732, Loss 2.930139\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch 2733, Loss 2.930131\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch 2734, Loss 2.930123\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch 2735, Loss 2.930113\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0285])\n",
      "Epoch 2736, Loss 2.930107\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0051,  0.0285])\n",
      "Epoch 2737, Loss 2.930099\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch 2738, Loss 2.930090\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch 2739, Loss 2.930081\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch 2740, Loss 2.930073\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch 2741, Loss 2.930064\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch 2742, Loss 2.930056\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch 2743, Loss 2.930048\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch 2744, Loss 2.930041\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch 2745, Loss 2.930032\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch 2746, Loss 2.930022\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch 2747, Loss 2.930016\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2748, Loss 2.930008\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2749, Loss 2.930000\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch 2750, Loss 2.929992\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch 2751, Loss 2.929983\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch 2752, Loss 2.929975\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch 2753, Loss 2.929968\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch 2754, Loss 2.929960\n",
      "Params: tensor(0.3115)\n",
      "Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch 2755, Loss 2.929953\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch 2756, Loss 2.929945\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch 2757, Loss 2.929936\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch 2758, Loss 2.929929\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0049,  0.0274])\n",
      "Epoch 2759, Loss 2.929921\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0274])\n",
      "Epoch 2760, Loss 2.929914\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0049,  0.0273])\n",
      "Epoch 2761, Loss 2.929905\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0273])\n",
      "Epoch 2762, Loss 2.929896\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch 2763, Loss 2.929891\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch 2764, Loss 2.929882\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2765, Loss 2.929875\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2766, Loss 2.929868\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch 2767, Loss 2.929859\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch 2768, Loss 2.929852\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch 2769, Loss 2.929845\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0048,  0.0269])\n",
      "Epoch 2770, Loss 2.929838\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0269])\n",
      "Epoch 2771, Loss 2.929830\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch 2772, Loss 2.929822\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch 2773, Loss 2.929816\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch 2774, Loss 2.929807\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch 2775, Loss 2.929800\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2776, Loss 2.929794\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2777, Loss 2.929786\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch 2778, Loss 2.929778\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch 2779, Loss 2.929771\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch 2780, Loss 2.929765\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch 2781, Loss 2.929757\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch 2782, Loss 2.929750\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch 2783, Loss 2.929743\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch 2784, Loss 2.929735\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch 2785, Loss 2.929729\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0047,  0.0262])\n",
      "Epoch 2786, Loss 2.929722\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch 2787, Loss 2.929714\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch 2788, Loss 2.929707\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch 2789, Loss 2.929701\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch 2790, Loss 2.929692\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch 2791, Loss 2.929685\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch 2792, Loss 2.929681\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch 2793, Loss 2.929672\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch 2794, Loss 2.929666\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch 2795, Loss 2.929659\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0258])\n",
      "Epoch 2796, Loss 2.929653\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch 2797, Loss 2.929646\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch 2798, Loss 2.929638\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch 2799, Loss 2.929632\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch 2800, Loss 2.929626\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch 2801, Loss 2.929620\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch 2802, Loss 2.929611\n",
      "Params: tensor(0.3114)\n",
      "Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2803, Loss 2.929605\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2804, Loss 2.929600\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch 2805, Loss 2.929592\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch 2806, Loss 2.929586\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch 2807, Loss 2.929579\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0045,  0.0252])\n",
      "Epoch 2808, Loss 2.929572\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0252])\n",
      "Epoch 2809, Loss 2.929566\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2810, Loss 2.929559\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2811, Loss 2.929551\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch 2812, Loss 2.929545\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch 2813, Loss 2.929540\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch 2814, Loss 2.929533\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2815, Loss 2.929528\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2816, Loss 2.929521\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch 2817, Loss 2.929513\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0248])\n",
      "Epoch 2818, Loss 2.929507\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0248])\n",
      "Epoch 2819, Loss 2.929501\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch 2820, Loss 2.929496\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch 2821, Loss 2.929489\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0044,  0.0246])\n",
      "Epoch 2822, Loss 2.929482\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch 2823, Loss 2.929476\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch 2824, Loss 2.929471\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch 2825, Loss 2.929463\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch 2826, Loss 2.929458\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch 2827, Loss 2.929452\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch 2828, Loss 2.929445\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2829, Loss 2.929439\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2830, Loss 2.929433\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch 2831, Loss 2.929427\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch 2832, Loss 2.929421\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch 2833, Loss 2.929415\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2834, Loss 2.929409\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2835, Loss 2.929404\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch 2836, Loss 2.929396\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch 2837, Loss 2.929391\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch 2838, Loss 2.929383\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2839, Loss 2.929380\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2840, Loss 2.929373\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch 2841, Loss 2.929368\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch 2842, Loss 2.929361\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch 2843, Loss 2.929356\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2844, Loss 2.929351\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2845, Loss 2.929344\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch 2846, Loss 2.929338\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch 2847, Loss 2.929332\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch 2848, Loss 2.929328\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0042,  0.0235])\n",
      "Epoch 2849, Loss 2.929321\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch 2850, Loss 2.929316\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch 2851, Loss 2.929309\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch 2852, Loss 2.929304\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch 2853, Loss 2.929300\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2854, Loss 2.929293\n",
      "Params: tensor(0.3113)\n",
      "Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2855, Loss 2.929288\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch 2856, Loss 2.929282\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch 2857, Loss 2.929277\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch 2858, Loss 2.929271\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2859, Loss 2.929266\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2860, Loss 2.929260\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch 2861, Loss 2.929255\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch 2862, Loss 2.929250\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch 2863, Loss 2.929244\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2864, Loss 2.929238\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2865, Loss 2.929234\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch 2866, Loss 2.929228\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch 2867, Loss 2.929222\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch 2868, Loss 2.929217\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2869, Loss 2.929211\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2870, Loss 2.929208\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch 2871, Loss 2.929201\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2872, Loss 2.929195\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2873, Loss 2.929191\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch 2874, Loss 2.929185\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch 2875, Loss 2.929180\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch 2876, Loss 2.929175\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2877, Loss 2.929170\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2878, Loss 2.929165\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch 2879, Loss 2.929160\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2880, Loss 2.929155\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2881, Loss 2.929149\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch 2882, Loss 2.929143\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch 2883, Loss 2.929139\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch 2884, Loss 2.929133\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2885, Loss 2.929128\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2886, Loss 2.929122\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch 2887, Loss 2.929119\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2888, Loss 2.929113\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2889, Loss 2.929108\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch 2890, Loss 2.929104\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch 2891, Loss 2.929099\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch 2892, Loss 2.929093\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch 2893, Loss 2.929088\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch 2894, Loss 2.929083\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0218])\n",
      "Epoch 2895, Loss 2.929079\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2896, Loss 2.929074\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2897, Loss 2.929069\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch 2898, Loss 2.929065\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch 2899, Loss 2.929058\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch 2900, Loss 2.929054\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2901, Loss 2.929050\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2902, Loss 2.929044\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch 2903, Loss 2.929041\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2904, Loss 2.929036\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2905, Loss 2.929031\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch 2906, Loss 2.929025\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch 2907, Loss 2.929021\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch 2908, Loss 2.929017\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0037,  0.0213])\n",
      "Epoch 2909, Loss 2.929012\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch 2910, Loss 2.929007\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch 2911, Loss 2.929003\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2912, Loss 2.928999\n",
      "Params: tensor(0.3112)\n",
      "Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2913, Loss 2.928993\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch 2914, Loss 2.928989\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2915, Loss 2.928985\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2916, Loss 2.928980\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch 2917, Loss 2.928976\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2918, Loss 2.928971\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2919, Loss 2.928967\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch 2920, Loss 2.928962\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2921, Loss 2.928958\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2922, Loss 2.928953\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch 2923, Loss 2.928947\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0207])\n",
      "Epoch 2924, Loss 2.928943\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0037,  0.0207])\n",
      "Epoch 2925, Loss 2.928940\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2926, Loss 2.928935\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2927, Loss 2.928932\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch 2928, Loss 2.928926\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2929, Loss 2.928923\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2930, Loss 2.928919\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch 2931, Loss 2.928913\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2932, Loss 2.928909\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2933, Loss 2.928904\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch 2934, Loss 2.928902\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2935, Loss 2.928897\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2936, Loss 2.928893\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch 2937, Loss 2.928887\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch 2938, Loss 2.928883\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0202])\n",
      "Epoch 2939, Loss 2.928880\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch 2940, Loss 2.928878\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0036,  0.0201])\n",
      "Epoch 2941, Loss 2.928871\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch 2942, Loss 2.928867\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch 2943, Loss 2.928864\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2944, Loss 2.928860\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2945, Loss 2.928855\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch 2946, Loss 2.928850\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2947, Loss 2.928845\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2948, Loss 2.928843\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch 2949, Loss 2.928838\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch 2950, Loss 2.928833\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch 2951, Loss 2.928830\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch 2952, Loss 2.928826\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2953, Loss 2.928823\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2954, Loss 2.928818\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch 2955, Loss 2.928816\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch 2956, Loss 2.928811\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch 2957, Loss 2.928805\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0196])\n",
      "Epoch 2958, Loss 2.928802\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0035,  0.0195])\n",
      "Epoch 2959, Loss 2.928799\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch 2960, Loss 2.928795\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch 2961, Loss 2.928789\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2962, Loss 2.928789\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2963, Loss 2.928783\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch 2964, Loss 2.928779\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2965, Loss 2.928775\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2966, Loss 2.928771\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch 2967, Loss 2.928767\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2968, Loss 2.928765\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2969, Loss 2.928761\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch 2970, Loss 2.928758\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2971, Loss 2.928752\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2972, Loss 2.928750\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch 2973, Loss 2.928745\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2974, Loss 2.928741\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2975, Loss 2.928737\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch 2976, Loss 2.928735\n",
      "Params: tensor(0.3111)\n",
      "Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2977, Loss 2.928730\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2978, Loss 2.928727\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch 2979, Loss 2.928723\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2980, Loss 2.928719\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2981, Loss 2.928716\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch 2982, Loss 2.928712\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2983, Loss 2.928708\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2984, Loss 2.928705\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch 2985, Loss 2.928700\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2986, Loss 2.928698\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2987, Loss 2.928695\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2988, Loss 2.928690\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch 2989, Loss 2.928687\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch 2990, Loss 2.928684\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch 2991, Loss 2.928679\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0185])\n",
      "Epoch 2992, Loss 2.928677\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2993, Loss 2.928673\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2994, Loss 2.928669\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch 2995, Loss 2.928666\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2996, Loss 2.928662\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2997, Loss 2.928660\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch 2998, Loss 2.928656\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 2999, Loss 2.928651\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 3000, Loss 2.928648\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch 3001, Loss 2.928646\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3002, Loss 2.928643\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3003, Loss 2.928638\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3004, Loss 2.928635\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch 3005, Loss 2.928632\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3006, Loss 2.928629\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3007, Loss 2.928625\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch 3008, Loss 2.928621\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3009, Loss 2.928617\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3010, Loss 2.928616\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch 3011, Loss 2.928612\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch 3012, Loss 2.928608\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch 3013, Loss 2.928604\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0178])\n",
      "Epoch 3014, Loss 2.928601\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3015, Loss 2.928599\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3016, Loss 2.928596\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3017, Loss 2.928592\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch 3018, Loss 2.928588\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3019, Loss 2.928586\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3020, Loss 2.928583\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch 3021, Loss 2.928580\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3022, Loss 2.928576\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3023, Loss 2.928574\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3024, Loss 2.928570\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch 3025, Loss 2.928567\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch 3026, Loss 2.928564\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch 3027, Loss 2.928561\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0174])\n",
      "Epoch 3028, Loss 2.928557\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3029, Loss 2.928555\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3030, Loss 2.928551\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch 3031, Loss 2.928548\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0031,  0.0172])\n",
      "Epoch 3032, Loss 2.928545\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3033, Loss 2.928543\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3034, Loss 2.928539\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch 3035, Loss 2.928536\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3036, Loss 2.928532\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3037, Loss 2.928531\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch 3038, Loss 2.928528\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3039, Loss 2.928524\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3040, Loss 2.928521\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3041, Loss 2.928519\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch 3042, Loss 2.928514\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3043, Loss 2.928512\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3044, Loss 2.928509\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch 3045, Loss 2.928505\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3046, Loss 2.928503\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3047, Loss 2.928500\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3048, Loss 2.928498\n",
      "Params: tensor(0.3110)\n",
      "Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch 3049, Loss 2.928495\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3050, Loss 2.928491\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3051, Loss 2.928489\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch 3052, Loss 2.928486\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3053, Loss 2.928484\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3054, Loss 2.928481\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch 3055, Loss 2.928477\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3056, Loss 2.928474\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3057, Loss 2.928472\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3058, Loss 2.928469\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch 3059, Loss 2.928468\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3060, Loss 2.928463\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3061, Loss 2.928460\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3062, Loss 2.928458\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch 3063, Loss 2.928456\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3064, Loss 2.928452\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3065, Loss 2.928449\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch 3066, Loss 2.928447\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3067, Loss 2.928443\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3068, Loss 2.928444\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3069, Loss 2.928440\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch 3070, Loss 2.928435\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch 3071, Loss 2.928435\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch 3072, Loss 2.928430\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch 3073, Loss 2.928428\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch 3074, Loss 2.928426\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3075, Loss 2.928423\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3076, Loss 2.928421\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch 3077, Loss 2.928417\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3078, Loss 2.928416\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3079, Loss 2.928411\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3080, Loss 2.928410\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch 3081, Loss 2.928407\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3082, Loss 2.928404\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3083, Loss 2.928402\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3084, Loss 2.928399\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch 3085, Loss 2.928396\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch 3086, Loss 2.928395\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch 3087, Loss 2.928392\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch 3088, Loss 2.928389\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch 3089, Loss 2.928386\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0156])\n",
      "Epoch 3090, Loss 2.928383\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch 3091, Loss 2.928382\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch 3092, Loss 2.928379\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3093, Loss 2.928378\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3094, Loss 2.928375\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3095, Loss 2.928372\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch 3096, Loss 2.928370\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3097, Loss 2.928368\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3098, Loss 2.928364\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3099, Loss 2.928362\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch 3100, Loss 2.928361\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3101, Loss 2.928356\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3102, Loss 2.928355\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3103, Loss 2.928353\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch 3104, Loss 2.928349\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3105, Loss 2.928348\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3106, Loss 2.928345\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3107, Loss 2.928343\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch 3108, Loss 2.928340\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3109, Loss 2.928339\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3110, Loss 2.928337\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3111, Loss 2.928333\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch 3112, Loss 2.928332\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch 3113, Loss 2.928328\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0150])\n",
      "Epoch 3114, Loss 2.928329\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch 3115, Loss 2.928324\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3116, Loss 2.928323\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3117, Loss 2.928320\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3118, Loss 2.928319\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch 3119, Loss 2.928315\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3120, Loss 2.928313\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3121, Loss 2.928310\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3122, Loss 2.928308\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch 3123, Loss 2.928306\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3124, Loss 2.928304\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3125, Loss 2.928303\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3126, Loss 2.928299\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch 3127, Loss 2.928296\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3128, Loss 2.928295\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3129, Loss 2.928293\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3130, Loss 2.928291\n",
      "Params: tensor(0.3109)\n",
      "Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch 3131, Loss 2.928288\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3132, Loss 2.928287\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3133, Loss 2.928285\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0145])\n",
      "Epoch 3134, Loss 2.928282\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch 3135, Loss 2.928280\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch 3136, Loss 2.928276\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3137, Loss 2.928275\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch 3138, Loss 2.928273\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3139, Loss 2.928271\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch 3140, Loss 2.928268\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3141, Loss 2.928267\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3142, Loss 2.928264\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3143, Loss 2.928263\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch 3144, Loss 2.928260\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3145, Loss 2.928259\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3146, Loss 2.928256\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3147, Loss 2.928255\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch 3148, Loss 2.928252\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3149, Loss 2.928250\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3150, Loss 2.928249\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3151, Loss 2.928246\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch 3152, Loss 2.928245\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3153, Loss 2.928242\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3154, Loss 2.928239\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3155, Loss 2.928236\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch 3156, Loss 2.928236\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch 3157, Loss 2.928233\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch 3158, Loss 2.928231\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch 3159, Loss 2.928230\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch 3160, Loss 2.928227\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3161, Loss 2.928226\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0025,  0.0138])\n",
      "Epoch 3162, Loss 2.928225\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3163, Loss 2.928222\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3164, Loss 2.928219\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch 3165, Loss 2.928218\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3166, Loss 2.928216\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3167, Loss 2.928215\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3168, Loss 2.928212\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch 3169, Loss 2.928211\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3170, Loss 2.928209\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3171, Loss 2.928206\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3172, Loss 2.928205\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch 3173, Loss 2.928204\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3174, Loss 2.928202\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3175, Loss 2.928200\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3176, Loss 2.928196\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch 3177, Loss 2.928195\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3178, Loss 2.928195\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3179, Loss 2.928191\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3180, Loss 2.928190\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch 3181, Loss 2.928188\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0134])\n",
      "Epoch 3182, Loss 2.928186\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch 3183, Loss 2.928185\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch 3184, Loss 2.928184\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch 3185, Loss 2.928182\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch 3186, Loss 2.928180\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0024,  0.0132])\n",
      "Epoch 3187, Loss 2.928178\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3188, Loss 2.928175\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3189, Loss 2.928172\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3190, Loss 2.928171\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch 3191, Loss 2.928170\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3192, Loss 2.928169\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3193, Loss 2.928167\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3194, Loss 2.928164\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch 3195, Loss 2.928163\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3196, Loss 2.928162\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3197, Loss 2.928160\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3198, Loss 2.928158\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3199, Loss 2.928157\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch 3200, Loss 2.928154\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3201, Loss 2.928152\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3202, Loss 2.928149\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3203, Loss 2.928150\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch 3204, Loss 2.928147\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0129])\n",
      "Epoch 3205, Loss 2.928146\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3206, Loss 2.928144\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3207, Loss 2.928142\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch 3208, Loss 2.928140\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0128])\n",
      "Epoch 3209, Loss 2.928138\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch 3210, Loss 2.928137\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3211, Loss 2.928135\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3212, Loss 2.928135\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch 3213, Loss 2.928133\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch 3214, Loss 2.928131\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3215, Loss 2.928130\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3216, Loss 2.928126\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3217, Loss 2.928125\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch 3218, Loss 2.928124\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3219, Loss 2.928121\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3220, Loss 2.928121\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3221, Loss 2.928120\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3222, Loss 2.928118\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch 3223, Loss 2.928117\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3224, Loss 2.928115\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3225, Loss 2.928113\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3226, Loss 2.928110\n",
      "Params: tensor(0.3108)\n",
      "Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3227, Loss 2.928109\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch 3228, Loss 2.928108\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3229, Loss 2.928105\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3230, Loss 2.928105\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3231, Loss 2.928104\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch 3232, Loss 2.928102\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0123])\n",
      "Epoch 3233, Loss 2.928101\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3234, Loss 2.928098\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3235, Loss 2.928097\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3236, Loss 2.928095\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch 3237, Loss 2.928094\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3238, Loss 2.928093\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3239, Loss 2.928091\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch 3240, Loss 2.928090\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch 3241, Loss 2.928088\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch 3242, Loss 2.928086\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3243, Loss 2.928085\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3244, Loss 2.928084\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3245, Loss 2.928082\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3246, Loss 2.928080\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch 3247, Loss 2.928079\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3248, Loss 2.928076\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3249, Loss 2.928077\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3250, Loss 2.928075\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3251, Loss 2.928072\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch 3252, Loss 2.928072\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3253, Loss 2.928071\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3254, Loss 2.928068\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3255, Loss 2.928069\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3256, Loss 2.928066\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch 3257, Loss 2.928065\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3258, Loss 2.928064\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3259, Loss 2.928061\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3260, Loss 2.928060\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3261, Loss 2.928057\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch 3262, Loss 2.928058\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3263, Loss 2.928056\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3264, Loss 2.928055\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3265, Loss 2.928052\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3266, Loss 2.928053\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch 3267, Loss 2.928051\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch 3268, Loss 2.928050\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch 3269, Loss 2.928047\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3270, Loss 2.928046\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3271, Loss 2.928046\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3272, Loss 2.928044\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch 3273, Loss 2.928042\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3274, Loss 2.928040\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3275, Loss 2.928040\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3276, Loss 2.928036\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch 3277, Loss 2.928036\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3278, Loss 2.928037\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3279, Loss 2.928034\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3280, Loss 2.928034\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3281, Loss 2.928031\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3282, Loss 2.928032\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch 3283, Loss 2.928028\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3284, Loss 2.928027\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3285, Loss 2.928026\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3286, Loss 2.928025\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3287, Loss 2.928024\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch 3288, Loss 2.928022\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3289, Loss 2.928023\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3290, Loss 2.928021\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3291, Loss 2.928019\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3292, Loss 2.928018\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch 3293, Loss 2.928017\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3294, Loss 2.928015\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3295, Loss 2.928013\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch 3296, Loss 2.928013\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3297, Loss 2.928011\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3298, Loss 2.928009\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch 3299, Loss 2.928008\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3300, Loss 2.928006\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3301, Loss 2.928007\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3302, Loss 2.928007\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3303, Loss 2.928004\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch 3304, Loss 2.928002\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3305, Loss 2.928002\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3306, Loss 2.928000\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3307, Loss 2.928000\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3308, Loss 2.927998\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch 3309, Loss 2.927995\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3310, Loss 2.927995\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3311, Loss 2.927994\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3312, Loss 2.927994\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3313, Loss 2.927991\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3314, Loss 2.927991\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch 3315, Loss 2.927990\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3316, Loss 2.927989\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3317, Loss 2.927988\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3318, Loss 2.927986\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3319, Loss 2.927985\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch 3320, Loss 2.927983\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0106])\n",
      "Epoch 3321, Loss 2.927983\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3322, Loss 2.927981\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3323, Loss 2.927980\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3324, Loss 2.927979\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3325, Loss 2.927979\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch 3326, Loss 2.927977\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3327, Loss 2.927975\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0019,  0.0104])\n",
      "Epoch 3328, Loss 2.927973\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3329, Loss 2.927974\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3330, Loss 2.927974\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3331, Loss 2.927972\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch 3332, Loss 2.927972\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3333, Loss 2.927969\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3334, Loss 2.927969\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3335, Loss 2.927967\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3336, Loss 2.927967\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch 3337, Loss 2.927963\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3338, Loss 2.927963\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3339, Loss 2.927962\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3340, Loss 2.927962\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3341, Loss 2.927960\n",
      "Params: tensor(0.3107)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3342, Loss 2.927960\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch 3343, Loss 2.927959\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3344, Loss 2.927958\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3345, Loss 2.927956\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3346, Loss 2.927956\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3347, Loss 2.927955\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3348, Loss 2.927953\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch 3349, Loss 2.927953\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3350, Loss 2.927951\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3351, Loss 2.927950\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3352, Loss 2.927948\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3353, Loss 2.927947\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0100])\n",
      "Epoch 3354, Loss 2.927948\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch 3355, Loss 2.927945\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3356, Loss 2.927944\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3357, Loss 2.927943\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch 3358, Loss 2.927944\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3359, Loss 2.927942\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch 3360, Loss 2.927941\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch 3361, Loss 2.927940\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3362, Loss 2.927938\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3363, Loss 2.927938\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0018,  0.0098])\n",
      "Epoch 3364, Loss 2.927936\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3365, Loss 2.927936\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3366, Loss 2.927937\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch 3367, Loss 2.927934\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3368, Loss 2.927933\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3369, Loss 2.927932\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3370, Loss 2.927930\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3371, Loss 2.927928\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3372, Loss 2.927931\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch 3373, Loss 2.927929\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3374, Loss 2.927927\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3375, Loss 2.927926\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3376, Loss 2.927925\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3377, Loss 2.927924\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3378, Loss 2.927923\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch 3379, Loss 2.927924\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3380, Loss 2.927922\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3381, Loss 2.927922\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3382, Loss 2.927920\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3383, Loss 2.927918\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3384, Loss 2.927917\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch 3385, Loss 2.927917\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3386, Loss 2.927915\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3387, Loss 2.927915\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3388, Loss 2.927914\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch 3389, Loss 2.927913\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch 3390, Loss 2.927911\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch 3391, Loss 2.927913\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch 3392, Loss 2.927911\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3393, Loss 2.927910\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3394, Loss 2.927909\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3395, Loss 2.927908\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3396, Loss 2.927907\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch 3397, Loss 2.927906\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch 3398, Loss 2.927905\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0017,  0.0092])\n",
      "Epoch 3399, Loss 2.927905\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3400, Loss 2.927904\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3401, Loss 2.927902\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3402, Loss 2.927902\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3403, Loss 2.927902\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch 3404, Loss 2.927899\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3405, Loss 2.927899\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3406, Loss 2.927898\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3407, Loss 2.927899\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3408, Loss 2.927896\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3409, Loss 2.927895\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3410, Loss 2.927896\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch 3411, Loss 2.927894\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3412, Loss 2.927892\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3413, Loss 2.927892\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3414, Loss 2.927891\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3415, Loss 2.927891\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3416, Loss 2.927890\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch 3417, Loss 2.927891\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3418, Loss 2.927888\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3419, Loss 2.927888\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3420, Loss 2.927886\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3421, Loss 2.927887\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3422, Loss 2.927886\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3423, Loss 2.927884\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch 3424, Loss 2.927883\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3425, Loss 2.927881\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3426, Loss 2.927881\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch 3427, Loss 2.927880\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3428, Loss 2.927880\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch 3429, Loss 2.927879\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch 3430, Loss 2.927877\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3431, Loss 2.927876\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3432, Loss 2.927876\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3433, Loss 2.927876\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3434, Loss 2.927876\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3435, Loss 2.927876\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch 3436, Loss 2.927875\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3437, Loss 2.927873\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch 3438, Loss 2.927872\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3439, Loss 2.927871\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3440, Loss 2.927870\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3441, Loss 2.927871\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3442, Loss 2.927869\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3443, Loss 2.927869\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch 3444, Loss 2.927867\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3445, Loss 2.927866\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3446, Loss 2.927866\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3447, Loss 2.927866\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3448, Loss 2.927864\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3449, Loss 2.927863\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3450, Loss 2.927863\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch 3451, Loss 2.927862\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3452, Loss 2.927863\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3453, Loss 2.927860\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3454, Loss 2.927860\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3455, Loss 2.927860\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3456, Loss 2.927859\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3457, Loss 2.927858\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch 3458, Loss 2.927858\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3459, Loss 2.927856\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3460, Loss 2.927857\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3461, Loss 2.927854\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3462, Loss 2.927855\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3463, Loss 2.927854\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3464, Loss 2.927854\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch 3465, Loss 2.927851\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3466, Loss 2.927853\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3467, Loss 2.927852\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3468, Loss 2.927850\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3469, Loss 2.927849\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3470, Loss 2.927849\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch 3471, Loss 2.927848\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch 3472, Loss 2.927848\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3473, Loss 2.927846\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch 3474, Loss 2.927846\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch 3475, Loss 2.927845\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3476, Loss 2.927844\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3477, Loss 2.927844\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3478, Loss 2.927844\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3479, Loss 2.927843\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch 3480, Loss 2.927843\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3481, Loss 2.927842\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3482, Loss 2.927840\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3483, Loss 2.927842\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3484, Loss 2.927839\n",
      "Params: tensor(0.3106)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3485, Loss 2.927838\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3486, Loss 2.927839\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch 3487, Loss 2.927838\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3488, Loss 2.927837\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3489, Loss 2.927835\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3490, Loss 2.927837\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3491, Loss 2.927836\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3492, Loss 2.927835\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3493, Loss 2.927833\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3494, Loss 2.927833\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch 3495, Loss 2.927833\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3496, Loss 2.927832\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3497, Loss 2.927831\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3498, Loss 2.927830\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3499, Loss 2.927830\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3500, Loss 2.927830\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3501, Loss 2.927829\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch 3502, Loss 2.927828\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3503, Loss 2.927828\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3504, Loss 2.927827\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3505, Loss 2.927825\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3506, Loss 2.927827\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3507, Loss 2.927825\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch 3508, Loss 2.927824\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch 3509, Loss 2.927824\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch 3510, Loss 2.927824\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3511, Loss 2.927822\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3512, Loss 2.927822\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3513, Loss 2.927821\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3514, Loss 2.927820\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch 3515, Loss 2.927820\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3516, Loss 2.927821\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch 3517, Loss 2.927819\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch 3518, Loss 2.927819\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch 3519, Loss 2.927819\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3520, Loss 2.927817\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3521, Loss 2.927817\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3522, Loss 2.927816\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3523, Loss 2.927815\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3524, Loss 2.927816\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch 3525, Loss 2.927815\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3526, Loss 2.927814\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3527, Loss 2.927813\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3528, Loss 2.927812\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3529, Loss 2.927811\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3530, Loss 2.927812\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3531, Loss 2.927812\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3532, Loss 2.927810\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch 3533, Loss 2.927809\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3534, Loss 2.927810\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3535, Loss 2.927809\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3536, Loss 2.927808\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3537, Loss 2.927808\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3538, Loss 2.927806\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3539, Loss 2.927806\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3540, Loss 2.927805\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3541, Loss 2.927804\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch 3542, Loss 2.927805\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3543, Loss 2.927804\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3544, Loss 2.927805\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3545, Loss 2.927804\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3546, Loss 2.927804\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3547, Loss 2.927803\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3548, Loss 2.927802\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch 3549, Loss 2.927801\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3550, Loss 2.927801\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3551, Loss 2.927799\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3552, Loss 2.927801\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3553, Loss 2.927798\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3554, Loss 2.927798\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch 3555, Loss 2.927798\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3556, Loss 2.927798\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3557, Loss 2.927798\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch 3558, Loss 2.927796\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3559, Loss 2.927795\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3560, Loss 2.927796\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3561, Loss 2.927794\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3562, Loss 2.927795\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3563, Loss 2.927795\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3564, Loss 2.927793\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch 3565, Loss 2.927795\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0070])\n",
      "Epoch 3566, Loss 2.927791\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3567, Loss 2.927791\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3568, Loss 2.927791\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3569, Loss 2.927790\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3570, Loss 2.927790\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3571, Loss 2.927789\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3572, Loss 2.927790\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3573, Loss 2.927789\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3574, Loss 2.927789\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch 3575, Loss 2.927789\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3576, Loss 2.927787\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3577, Loss 2.927786\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3578, Loss 2.927788\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3579, Loss 2.927785\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3580, Loss 2.927785\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3581, Loss 2.927786\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3582, Loss 2.927785\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch 3583, Loss 2.927784\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3584, Loss 2.927784\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3585, Loss 2.927783\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3586, Loss 2.927783\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3587, Loss 2.927781\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3588, Loss 2.927782\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3589, Loss 2.927781\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3590, Loss 2.927781\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3591, Loss 2.927781\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch 3592, Loss 2.927780\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3593, Loss 2.927780\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3594, Loss 2.927778\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3595, Loss 2.927779\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3596, Loss 2.927778\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3597, Loss 2.927778\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3598, Loss 2.927779\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3599, Loss 2.927777\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3600, Loss 2.927776\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch 3601, Loss 2.927775\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3602, Loss 2.927776\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3603, Loss 2.927773\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3604, Loss 2.927775\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch 3605, Loss 2.927775\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3606, Loss 2.927775\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3607, Loss 2.927773\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3608, Loss 2.927773\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3609, Loss 2.927773\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch 3610, Loss 2.927772\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3611, Loss 2.927772\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3612, Loss 2.927770\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3613, Loss 2.927772\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3614, Loss 2.927771\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3615, Loss 2.927770\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3616, Loss 2.927770\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3617, Loss 2.927769\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3618, Loss 2.927768\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3619, Loss 2.927769\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch 3620, Loss 2.927768\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3621, Loss 2.927767\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3622, Loss 2.927767\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3623, Loss 2.927767\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3624, Loss 2.927765\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3625, Loss 2.927766\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3626, Loss 2.927765\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3627, Loss 2.927765\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3628, Loss 2.927764\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch 3629, Loss 2.927764\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3630, Loss 2.927764\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3631, Loss 2.927762\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3632, Loss 2.927763\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3633, Loss 2.927763\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3634, Loss 2.927762\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3635, Loss 2.927761\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3636, Loss 2.927762\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3637, Loss 2.927759\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch 3638, Loss 2.927761\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3639, Loss 2.927761\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3640, Loss 2.927760\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3641, Loss 2.927759\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3642, Loss 2.927758\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3643, Loss 2.927759\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3644, Loss 2.927757\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3645, Loss 2.927758\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3646, Loss 2.927757\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3647, Loss 2.927757\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch 3648, Loss 2.927757\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3649, Loss 2.927756\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3650, Loss 2.927757\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3651, Loss 2.927756\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch 3652, Loss 2.927756\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3653, Loss 2.927755\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3654, Loss 2.927755\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3655, Loss 2.927754\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3656, Loss 2.927754\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3657, Loss 2.927755\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch 3658, Loss 2.927753\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3659, Loss 2.927752\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3660, Loss 2.927754\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3661, Loss 2.927752\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3662, Loss 2.927751\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3663, Loss 2.927752\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3664, Loss 2.927750\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0011,  0.0059])\n",
      "Epoch 3665, Loss 2.927749\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3666, Loss 2.927751\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch 3667, Loss 2.927750\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3668, Loss 2.927750\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3669, Loss 2.927747\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3670, Loss 2.927749\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3671, Loss 2.927747\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3672, Loss 2.927748\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3673, Loss 2.927748\n",
      "Params: tensor(0.3105)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3674, Loss 2.927747\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3675, Loss 2.927747\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3676, Loss 2.927748\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3677, Loss 2.927747\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch 3678, Loss 2.927747\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3679, Loss 2.927745\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3680, Loss 2.927745\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3681, Loss 2.927746\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3682, Loss 2.927744\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3683, Loss 2.927743\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3684, Loss 2.927743\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3685, Loss 2.927743\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3686, Loss 2.927743\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch 3687, Loss 2.927743\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3688, Loss 2.927744\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3689, Loss 2.927742\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3690, Loss 2.927742\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3691, Loss 2.927742\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3692, Loss 2.927742\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3693, Loss 2.927741\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3694, Loss 2.927741\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3695, Loss 2.927741\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3696, Loss 2.927742\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3697, Loss 2.927741\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3698, Loss 2.927741\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch 3699, Loss 2.927740\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3700, Loss 2.927739\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3701, Loss 2.927738\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3702, Loss 2.927738\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3703, Loss 2.927737\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3704, Loss 2.927737\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3705, Loss 2.927738\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3706, Loss 2.927737\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3707, Loss 2.927736\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3708, Loss 2.927737\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch 3709, Loss 2.927737\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3710, Loss 2.927736\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3711, Loss 2.927734\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3712, Loss 2.927735\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch 3713, Loss 2.927735\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3714, Loss 2.927734\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3715, Loss 2.927734\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3716, Loss 2.927733\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3717, Loss 2.927734\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3718, Loss 2.927733\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3719, Loss 2.927733\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch 3720, Loss 2.927733\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch 3721, Loss 2.927732\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3722, Loss 2.927731\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3723, Loss 2.927731\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3724, Loss 2.927733\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3725, Loss 2.927730\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3726, Loss 2.927730\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3727, Loss 2.927732\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3728, Loss 2.927730\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch 3729, Loss 2.927732\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3730, Loss 2.927731\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch 3731, Loss 2.927730\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3732, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3733, Loss 2.927729\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3734, Loss 2.927729\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3735, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3736, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3737, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3738, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3739, Loss 2.927727\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3740, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3741, Loss 2.927728\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch 3742, Loss 2.927727\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3743, Loss 2.927727\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3744, Loss 2.927726\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3745, Loss 2.927726\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3746, Loss 2.927725\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3747, Loss 2.927725\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3748, Loss 2.927725\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3749, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3750, Loss 2.927724\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3751, Loss 2.927724\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3752, Loss 2.927725\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3753, Loss 2.927724\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch 3754, Loss 2.927724\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3755, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3756, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3757, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3758, Loss 2.927722\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3759, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3760, Loss 2.927722\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3761, Loss 2.927723\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3762, Loss 2.927721\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3763, Loss 2.927722\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3764, Loss 2.927720\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3765, Loss 2.927720\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch 3766, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3767, Loss 2.927721\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3768, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3769, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3770, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3771, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3772, Loss 2.927719\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3773, Loss 2.927720\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3774, Loss 2.927718\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3775, Loss 2.927718\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3776, Loss 2.927717\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch 3777, Loss 2.927718\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0049])\n",
      "Epoch 3778, Loss 2.927717\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3779, Loss 2.927717\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3780, Loss 2.927716\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3781, Loss 2.927716\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3782, Loss 2.927717\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3783, Loss 2.927717\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3784, Loss 2.927716\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0009,  0.0048])\n",
      "Epoch 3785, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3786, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3787, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3788, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3789, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch 3790, Loss 2.927715\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3791, Loss 2.927714\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3792, Loss 2.927714\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3793, Loss 2.927714\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3794, Loss 2.927714\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3795, Loss 2.927713\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3796, Loss 2.927714\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3797, Loss 2.927713\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3798, Loss 2.927712\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3799, Loss 2.927712\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3800, Loss 2.927713\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3801, Loss 2.927711\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3802, Loss 2.927712\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch 3803, Loss 2.927712\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3804, Loss 2.927711\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3805, Loss 2.927712\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3806, Loss 2.927711\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3807, Loss 2.927711\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3808, Loss 2.927711\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3809, Loss 2.927709\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3810, Loss 2.927710\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3811, Loss 2.927710\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3812, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3813, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3814, Loss 2.927709\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch 3815, Loss 2.927709\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3816, Loss 2.927710\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3817, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3818, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3819, Loss 2.927706\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3820, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3821, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3822, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3823, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3824, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3825, Loss 2.927708\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3826, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3827, Loss 2.927706\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch 3828, Loss 2.927707\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3829, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3830, Loss 2.927706\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3831, Loss 2.927706\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3832, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3833, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3834, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3835, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3836, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3837, Loss 2.927705\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3838, Loss 2.927704\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3839, Loss 2.927704\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3840, Loss 2.927704\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3841, Loss 2.927703\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch 3842, Loss 2.927702\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3843, Loss 2.927703\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3844, Loss 2.927703\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3845, Loss 2.927704\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3846, Loss 2.927702\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3847, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3848, Loss 2.927703\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3849, Loss 2.927702\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch 3850, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3851, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3852, Loss 2.927703\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3853, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3854, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3855, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch 3856, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3857, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3858, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3859, Loss 2.927701\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3860, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0008,  0.0042])\n",
      "Epoch 3861, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3862, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3863, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3864, Loss 2.927698\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3865, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3866, Loss 2.927697\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3867, Loss 2.927700\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3868, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3869, Loss 2.927698\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch 3870, Loss 2.927697\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3871, Loss 2.927698\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3872, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3873, Loss 2.927699\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3874, Loss 2.927698\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3875, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3876, Loss 2.927698\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3877, Loss 2.927697\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3878, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3879, Loss 2.927697\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3880, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3881, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3882, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3883, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch 3884, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3885, Loss 2.927695\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3886, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3887, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3888, Loss 2.927695\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3889, Loss 2.927695\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3890, Loss 2.927694\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3891, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3892, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3893, Loss 2.927695\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3894, Loss 2.927695\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3895, Loss 2.927694\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3896, Loss 2.927696\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3897, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch 3898, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3899, Loss 2.927694\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3900, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3901, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3902, Loss 2.927694\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3903, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3904, Loss 2.927693\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3905, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3906, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3907, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3908, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3909, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3910, Loss 2.927692\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3911, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3912, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3913, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch 3914, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3915, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3916, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3917, Loss 2.927691\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3918, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3919, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3920, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3921, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3922, Loss 2.927690\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3923, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3924, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3925, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3926, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3927, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3928, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch 3929, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3930, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3931, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3932, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3933, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3934, Loss 2.927689\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3935, Loss 2.927688\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3936, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3937, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3938, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3939, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3940, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch 3941, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3942, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3943, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3944, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch 3945, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3946, Loss 2.927685\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3947, Loss 2.927685\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3948, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3949, Loss 2.927685\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3950, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3951, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3952, Loss 2.927687\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3953, Loss 2.927685\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3954, Loss 2.927686\n",
      "Params: tensor(0.3104)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3955, Loss 2.927686\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3956, Loss 2.927685\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3957, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3958, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3959, Loss 2.927685\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch 3960, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch 3961, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3962, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3963, Loss 2.927685\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0007,  0.0035])\n",
      "Epoch 3964, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3965, Loss 2.927685\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3966, Loss 2.927685\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3967, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3968, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3969, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3970, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3971, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3972, Loss 2.927684\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3973, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3974, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3975, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3976, Loss 2.927683\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3977, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3978, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch 3979, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3980, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3981, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3982, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3983, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3984, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3985, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3986, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3987, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3988, Loss 2.927682\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3989, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3990, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3991, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3992, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3993, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3994, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch 3995, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3996, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3997, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3998, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 3999, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4000, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4001, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4002, Loss 2.927681\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4003, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4004, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4005, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4006, Loss 2.927680\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4007, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4008, Loss 2.927678\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4009, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4010, Loss 2.927678\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4011, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4012, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch 4013, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4014, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4015, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4016, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4017, Loss 2.927679\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4018, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4019, Loss 2.927678\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4020, Loss 2.927678\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4021, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4022, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4023, Loss 2.927678\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4024, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4025, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4026, Loss 2.927676\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4027, Loss 2.927676\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4028, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4029, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4030, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch 4031, Loss 2.927676\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4032, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4033, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4034, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4035, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4036, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4037, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4038, Loss 2.927677\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4039, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4040, Loss 2.927676\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4041, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4042, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4043, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4044, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4045, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4046, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4047, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4048, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch 4049, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4050, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch 4051, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4052, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4053, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4054, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4055, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4056, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4057, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4058, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4059, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4060, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4061, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4062, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4063, Loss 2.927675\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch 4064, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4065, Loss 2.927674\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4066, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4067, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4068, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4069, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4070, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch 4071, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4072, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4073, Loss 2.927671\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4074, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4075, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4076, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4077, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4078, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4079, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4080, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4081, Loss 2.927671\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4082, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4083, Loss 2.927673\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4084, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4085, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4086, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4087, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4088, Loss 2.927672\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4089, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4090, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch 4091, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4092, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4093, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4094, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4095, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4096, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4097, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4098, Loss 2.927671\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4099, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4100, Loss 2.927671\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4101, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4102, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4103, Loss 2.927671\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4104, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4105, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4106, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4107, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4108, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4109, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4110, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4111, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch 4112, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4113, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4114, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4115, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4116, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4117, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch 4118, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4119, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4120, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4121, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4122, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4123, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4124, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4125, Loss 2.927670\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4126, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4127, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4128, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4129, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4130, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4131, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch 4132, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4133, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch 4134, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4135, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4136, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4137, Loss 2.927669\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4138, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4139, Loss 2.927668\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4140, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4141, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4142, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4143, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4144, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4145, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4146, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4147, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4148, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4149, Loss 2.927667\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4150, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4151, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4152, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4153, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4154, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch 4155, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4156, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch 4157, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4158, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4159, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4160, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4161, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4162, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4163, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4164, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4165, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4166, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4167, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4168, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4169, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4170, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4171, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4172, Loss 2.927666\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4173, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4174, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4175, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4176, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4177, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch 4178, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch 4179, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4180, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4181, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4182, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4183, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4184, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4185, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4186, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4187, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4188, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4189, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4190, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4191, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4192, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch 4193, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4194, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4195, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4196, Loss 2.927665\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4197, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4198, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4199, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4200, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4201, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4202, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4203, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4204, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch 4205, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4206, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4207, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4208, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4209, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4210, Loss 2.927664\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4211, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4212, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4213, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4214, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4215, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4216, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4217, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4218, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4219, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4220, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4221, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4222, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4223, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4224, Loss 2.927663\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4225, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4226, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4227, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4228, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4229, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch 4230, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4231, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4232, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4233, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4234, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4235, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4236, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4237, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4238, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4239, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4240, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4241, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4242, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4243, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4244, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4245, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4246, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4247, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4248, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4249, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4250, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4251, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4252, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4253, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4254, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4255, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4256, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4257, Loss 2.927661\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch 4258, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4259, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4260, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4261, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4262, Loss 2.927662\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0021])\n",
      "Epoch 4263, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4264, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4265, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4266, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4267, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4268, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4269, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4270, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4271, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4272, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4273, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4274, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4275, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4276, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4277, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4278, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4279, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4280, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4281, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4282, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4283, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4284, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch 4285, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4286, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4287, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4288, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4289, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4290, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4291, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4292, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4293, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4294, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4295, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4296, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4297, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4298, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4299, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4300, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4301, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch 4302, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4303, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4304, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4305, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4306, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4307, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4308, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4309, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4310, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4311, Loss 2.927660\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4312, Loss 2.927659\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4313, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch 4314, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4315, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4316, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4317, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4318, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4319, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4320, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4321, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4322, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4323, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4324, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4325, Loss 2.927658\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4326, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4327, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4328, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4329, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4330, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4331, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4332, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4333, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4334, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4335, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4336, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4337, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4338, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4339, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4340, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4341, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch 4342, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4343, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch 4344, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4345, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4346, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch 4347, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4348, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4349, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4350, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4351, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4352, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4353, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4354, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4355, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4356, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4357, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4358, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4359, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4360, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4361, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4362, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4363, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4364, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4365, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4366, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4367, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4368, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4369, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4370, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4371, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4372, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4373, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4374, Loss 2.927657\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4375, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4376, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4377, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4378, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch 4379, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4380, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4381, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4382, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4383, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4384, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4385, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4386, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4387, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4388, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4389, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4390, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4391, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4392, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4393, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4394, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4395, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4396, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4397, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4398, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4399, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4400, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4401, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4402, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4403, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4404, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4405, Loss 2.927656\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4406, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4407, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4408, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4409, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4410, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4411, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch 4412, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4413, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4414, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4415, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4416, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4417, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4418, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4419, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4420, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4421, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4422, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4423, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4424, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4425, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4426, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4427, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4428, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4429, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4430, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4431, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4432, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4433, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4434, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4435, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4436, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4437, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4438, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4439, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4440, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4441, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4442, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4443, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0016])\n",
      "Epoch 4444, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4445, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4446, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4447, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4448, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4449, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch 4450, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4451, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4452, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4453, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4454, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4455, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4456, Loss 2.927655\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4457, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4458, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4459, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4460, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4461, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4462, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4463, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4464, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4465, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4466, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4467, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4468, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4469, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4470, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4471, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4472, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4473, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4474, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4475, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4476, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4477, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4478, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4479, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4480, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4481, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4482, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4483, Loss 2.927654\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4484, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4485, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4486, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4487, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch 4488, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4489, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4490, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4491, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4492, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4493, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4494, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4495, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4496, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch 4497, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4498, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4499, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4500, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4501, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4502, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4503, Loss 2.927653\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4504, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4505, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4506, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4507, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4508, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4509, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4510, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4511, Loss 2.927650\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4512, Loss 2.927651\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4513, Loss 2.927652\n",
      "Params: tensor(0.3103)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4514, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4515, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4516, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4517, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4518, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4519, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4520, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4521, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4522, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4523, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4524, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4525, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4526, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4527, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4528, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4529, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4530, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4531, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch 4532, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4533, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4534, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4535, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4536, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4537, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4538, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4539, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4540, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4541, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4542, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4543, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4544, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4545, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4546, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4547, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4548, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4549, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4550, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4551, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4552, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4553, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4554, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4555, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4556, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4557, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4558, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4559, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4560, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4561, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4562, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4563, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4564, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4565, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4566, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4567, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4568, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4569, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4570, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4571, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4572, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4573, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch 4574, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4575, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4576, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4577, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4578, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4579, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4580, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4581, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4582, Loss 2.927653\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4583, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4584, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4585, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4586, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4587, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4588, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4589, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4590, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4591, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4592, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4593, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4594, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4595, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4596, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4597, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4598, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4599, Loss 2.927652\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4600, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4601, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4602, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4603, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4604, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4605, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4606, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4607, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4608, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4609, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4610, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4611, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4612, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4613, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4614, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4615, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4616, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4617, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4618, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4619, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4620, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4621, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4622, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4623, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4624, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4625, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4626, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch 4627, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4628, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4629, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4630, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4631, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4632, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4633, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4634, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4635, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4636, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4637, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4638, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4639, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4640, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4641, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4642, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4643, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4644, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4645, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4646, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4647, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4648, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4649, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4650, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4651, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4652, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4653, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4654, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4655, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4656, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4657, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4658, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4659, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4660, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4661, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4662, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4663, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4664, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4665, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4666, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4667, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4668, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4669, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4670, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4671, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4672, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4673, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4674, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4675, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch 4676, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4677, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4678, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4679, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4680, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4681, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4682, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4683, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4684, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4685, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4686, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4687, Loss 2.927651\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4688, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4689, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4690, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4691, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4692, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4693, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4694, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4695, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4696, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4697, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4698, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4699, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4700, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4701, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4702, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4703, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4704, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4705, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4706, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4707, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4708, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4709, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4710, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4711, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4712, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4713, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4714, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4715, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4716, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4717, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4718, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4719, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4720, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4721, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4722, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4723, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4724, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4725, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4726, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4727, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4728, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4729, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4730, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4731, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4732, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4733, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4734, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4735, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4736, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4737, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4738, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch 4739, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4740, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4741, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4742, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4743, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4744, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4745, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4746, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4747, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4748, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4749, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4750, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4751, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4752, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4753, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4754, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4755, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4756, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4757, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4758, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4759, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4760, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4761, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4762, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4763, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4764, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4765, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4766, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4767, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4768, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4769, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4770, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4771, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4772, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4773, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4774, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4775, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4776, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4777, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4778, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4779, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4780, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4781, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4782, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4783, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4784, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4785, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4786, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4787, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4788, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4789, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4790, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4791, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4792, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4793, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4794, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4795, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4796, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4797, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch 4798, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4799, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4800, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4801, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4802, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4803, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch 4804, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4805, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4806, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4807, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4808, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4809, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4810, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4811, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4812, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4813, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4814, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4815, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4816, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4817, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4818, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4819, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4820, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4821, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4822, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4823, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4824, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4825, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4826, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4827, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4828, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4829, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4830, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4831, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4832, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4833, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4834, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4835, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4836, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4837, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4838, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4839, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4840, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4841, Loss 2.927650\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4842, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4843, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4844, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4845, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4846, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4847, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4848, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4849, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4850, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4851, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4852, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4853, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4854, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4855, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4856, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4857, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4858, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4859, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4860, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4861, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4862, Loss 2.927645\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4863, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4864, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4865, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4866, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4867, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4868, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4869, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4870, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4871, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4872, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4873, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4874, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4875, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4876, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4877, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4878, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4879, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4880, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4881, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch 4882, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4883, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4884, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4885, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4886, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4887, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4888, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4889, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4890, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4891, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4892, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4893, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4894, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4895, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4896, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4897, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4898, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4899, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4900, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4901, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4902, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4903, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4904, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4905, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4906, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4907, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4908, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4909, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4910, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4911, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4912, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4913, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4914, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4915, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4916, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4917, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4918, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4919, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4920, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4921, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4922, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4923, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4924, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4925, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4926, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4927, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4928, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4929, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4930, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4931, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4932, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4933, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4934, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4935, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4936, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4937, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4938, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4939, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4940, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4941, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4942, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4943, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4944, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4945, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4946, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4947, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4948, Loss 2.927649\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4949, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4950, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch 4951, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch 4952, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch 4953, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4954, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4955, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4956, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4957, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4958, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4959, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch 4960, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4961, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch 4962, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch 4963, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4964, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4965, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4966, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4967, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch 4968, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch 4969, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4970, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4971, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4972, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4973, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4974, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4975, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4976, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4977, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch 4978, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch 4979, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4980, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4981, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4982, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4983, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4984, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4985, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4986, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch 4987, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4988, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4989, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4990, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4991, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4992, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4993, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4994, Loss 2.927646\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch 4995, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch 4996, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4997, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4998, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 4999, Loss 2.927647\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch 5000, Loss 2.927648\n",
      "Params: tensor(0.3102)\n",
      "Grad: tensor([-0.0001,  0.0006])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_un, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_u, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(t_p, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_un, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(t_p, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch %d, Loss %f\" % (epoch, float(loss)))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860120\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer, \n",
    "    params = params, \n",
    "    t_u = t_un,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam -> Data Scale less-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928579\n",
      "Epoch 2000, Loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=2000,\n",
    "    optimizer=optimizer,\n",
    "    params=params,\n",
    "    t_u = t_u,\n",
    "    t_c = t_c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papers",
   "language": "python",
   "name": "papers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
